---
title: "Neural Networks Report"
author: "Anthony Schroeder"
contributor: "Chadd Frasier", "Member", "Member"
date: "04/09/2019"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Neural Networks Report}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}

---
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

<a id="top"></a>

# Project 3 Nueral Networks Vignette
### Anthony Schroeder, Chadd Frasier, <new members>, <new members>
#### NAU April 09, 2019

> [Introduction](#intro)
> [Installation](#install)
> [Neural Networks](#NeuNet)
> [NNetIterations](#nn.iter)
> [NNetEarlyStoppingCV](#nn.escv)
> [Testing With Data Sets](#test)
> [Conclusion](#con)


<a id="intro"></a>
## Introduction
  In Machine Learning, a neural network is a model that attmepts to make decisions in the same way a human brain does. Using serveral interconnected path ways that can make many predictions on multiple types of the same data gives the model much deeper learning capabilities. In this project implimentation we will be developing a single hidden layer Neural Network to make predictions on a large data sample. This will be done using k-fold cross validation and gradient descent on the two loss parameters. By breaking data into train and test sets we can train a learning model that is capable of making preditions on the test data. We will be stepping through the descent using the basic gradient decent algorithm on both parameters like so:
  
  $w^t = w^(t-1) - \alpha\bigtriangledown\mathcal{l(w)}$.
  $v^t = v^(t-1) - \alpha\bigtriangledown\mathcal{l(v)}$
<a id="NeuNet"></a>
## Neural Networks
For this machine learning project we will be writing two functions to handle both regression and binary classification of input matrices to predic output labels of test data features. In order to achieve both of these things we will be implimenting a generic algorithm that uses both versions of the loss function when either is needed. 
Loss Function: $$L(f(x),y_i) = \frac{1}{2}\sum_{i=1}^n[f(x_i)-y_i]$$
Starting from this function we can use the f(x) function to substitute in new variables for the loss function. When we rewrite the loss function in terms of v and w we get the function: $\mathcal{L}(w,v) = \frac{1}{2}\sum_{i=1}^n[w^TS(V^Tx_i) - y_i]$|where$S[a] = \frac{1}{1+exp(-ax)}$ This is the function that our gradient descent will use to minimize the mean loss, and build the resulting weight matrix. This type of gradient descent will only work for regression and if we are expecting to perform binary classification we will need a new loss function. 

Looking at binary classification we will need to adopt the same gradien decent concepts using the Logistic Loss function. Written in term of the input features and labels we observe: $L(f(x),y_i) = \sum_{i=1}^n[log(1+exp(-y_i f(x_i))]$ Following the same idea as before we must exchange the current loss function with variables form the f(x) function and then after deriving the gradient of the loss we observe that we will need to minimize, $L(b_i,y_i) = \frac{-y_i}{1+exp(\tilde{y_i}b_i)}$ throughout the gradient descent. 

<a id="nn.iter"></a>
## NNetIterations
The NNetIterations function will be in charge of performing the gradient descent on the current train/validation split and reporting the resulting V.mat and w.mat that can be used for the predictions. This function accepts a feature matrix of data values (n_observations x n_features). Next the function will need to accept a label vector that corrosponds to the label of each feature in the matrix (n_observations x 1). The function all requires some integers as input. We will need max.iterations (int > 1), step.size(double), n.hidden.units (int > 3), and lastly we will need a second vector that will be called is.train (n_observations x 1). This is.train vector will be used in the NNetIteration function specifically to break the train and test data into their own matrices, TRUE if the observation is in the train set, and FALSE for the validation set. This can be done immediatly like so $X.train <- X.mat[is.train,]$ the feature matrix and $y.train <- Y.vec[is.train]$ for the training label vector. 
One of the key compentents to this function is that fact that we basically perform the same algorithm for both binary classification and regression we just use a different loss function for the delta.W parameter that is calculated in the algorithm. So in order to minimize redundant code we will be implimenting an is.binary value that will run the correct line of code depending on if we need to use the logistic loss or the square loss. After the calulations are finished this function needs to return a list with multiple named elements including the pred.mat(n_observations x max.iterations) that will be a matrix of predictions on each observation and for each iteration of the gradient descent. This function will also return a V.mat(n_features+1 x n.hidden.units) where the first row of the matrix are the intercept terms for each feature. This function will return the last w.vec (n.hidden.units+1) that was calculated by the descent where the first term in the vector is the intercept term, and lastly the function will return a predict(testX.mat) function that will accept an unscaled input matrix and predicts the unknown outputs/labels of the observations in the test matrix.

<a id="nn.escv"></a>
## NNetEarlyStoppingCV
The NNetEarlyStoppingCV will be a function that uses the NNetIterations function to create mean.loss.vectors for both the validation splits and the training set. These are the mean.loss vectors that will go into plotting the mean validation/training lines. This function will use the NNetIterations function to trace the train validation loss means until the vaidation loss starts to grow again. Where the validation loss starts to grow is when we want to stop the processing of the means. In order to get this function started it will need to have a feature matrix, X.mat (n_observations x n_features), the label vector y.vec (n_features x 1),  a fold.vec (n_features x 1) like before in the cross validation sections. and if presented with null data or an incorrect vector size we can fix it by ranomly assigning a vector of the correct size like this `fold.vec=sample(rep(1:n.folds), length(y.vec))`, max.iterations(int), step.size(double), n.hidden.units(int), and lastly we will send the proper amount of folds found in the vector, as n.folds (int).
In order for the NNetIterations function to work properly we will need to scult a is.train vector of boolean arguments to denote which set is the current training set and validation set. Since we will be doing cross validation we will need to do this dynamically becuase each fold with be very different than the one before it. Secondly, in this function we will need to check and see if the current data is for binary classification or regression because in order to calculate the loss vectors we will need either the 01-Loss for binary data and the square loss for regression.
After we run the data on all the folds we will be selecting the ideal step.size using the resulting data and lastly, we will run the NNetIterations function with the new ideal selected step.size that way we can maximize the guessing capabilities while saving ourselves time from processing up to that step.size

<a id= 'test'></a>
## Testing With Data Sets
we are going to run our code on the following data sets for Binary Classification
- ElemStatLearn::spam 2-class [4601, 57] output is last column (spam).
- ElemStatLearn::SAheart 2-class [462, 9] output is last column (chd).
- ElemStatLearn::zip.train: 10-class [7291, 256] output is first column. (ignore classes other than 0 and 1)
And we will be using the following for regression:
- ElemStatLearn::prostate [97 x 8] output is lpsa column, ignore train column.
- ElemStatLearn::ozone [111 x 3] output is first column (ozone).

```{r, results='hide'}
# Spam Data Set
data(spam, package = "ElemStatLearn")

# SAheart Data Set
data(SAheart, package = "ElemStatLearn")

# zip.train Data Set
data(zip.train, package = "ElemStatLearn")

# prostate Data Set
data(prostate, package = "ElemStatLearn")

# OZone Data Set
data(ozone, package = "ElemStatLearn")
```

# How to use the 'ElemStatLearn' Libraries:

How To use this Linear Models Library:
1. Organize your data into:
  a. Training Data:
  Your data should be a matrix with the Column's containing Feature's, and Row's containing the different training Instances

  b.Training Labels:
  Your data should be a single colomn of numerical values,
    Note: if all labels are (0 or 1)  training will be classified as Binary classification, and be trained accordingly. otherwise, training will be Regressive.

## Testing 
# How to use the 'ElemStatLearn' Libraries:
we are going to run our code on the following data sets.

How To use this Linear Models Library:
1. Organize your data into:
  a. Training Data:
  Your data should be a matrix with the Column's containing Feature's, and Row's containing the different training Instances 
    
  b.Training Labels:
  Your data should be a single colomn of numerical values, 
    Note: if all labels are (0 or 1)  training will be classified as Binary classification, and be trained accordingly. otherwise, training will be Regressive.


- We will include the library(LinearModel), to have access to the given functionality.
- The R script Prep_Libraries.R in the tests/testthat folder contains some simple data manipulation to prep the data to be used for the LinearModel Algorithms.

```{r}
library(NeuralNetwork)
#print(getwd())
source("../tests/testthat/Prep_Libraries.R")
source("../R/General.R")
knitr::opts_chunk$set(fig.width=12, fig.height=8) 
#source("tests/testthat/LinearModels_Test_Libraries.R")
```

# Binary Classification:
## Data set 1: spam
```{r}
# Data set 1: spam
  Spam<-Prep_Spam()
```
### Code

```{r}
# Data set 1: spam
Spam<-Prep_Spam()
LossMat.fold.n  <- 4
LossMat.fold.vec<-Random_Folds((Spam$n_Elements),LossMat.fold.n)
Data = cbind(Spam$TrainingData ,Spam$TrainingLabels)
#Randomly shuffle the data
#Data<-Data[sample(nrow(Data)),]
SLI.WholeError.Mat = 0
SLI.TrainError.Mat = 0
SLI.TestError.Mat  = 0

ES_CV_Error.WholeError.Mat = 0
ES_CV_Error.TrainError.Mat = 0
ES_CV_Error.TestError.Mat  = 0

SL_L2.WholeError.Mat = 0
SL_L2.TrainError.Mat = 0
SL_L2.TestError.Mat  = 0

SL_L2_Pen.WholeError.Mat = 0
SL_L2_Pen.TrainError.Mat = 0
SL_L2_Pen.TestError.Mat  = 0

SL_L2CV.WholeError.Mat = 0
SL_L2CV.TrainError.Mat = 0
SL_L2CV.TestError.Mat  = 0

#Perform folds.n fold cross validation
for(i in 1:(LossMat.fold.n))
{
  testIndexes <- which(LossMat.fold.vec==i,arr.ind=TRUE)
  testData    <- as.matrix(Data[testIndexes, ])
  trainData   <- as.matrix(Data[-testIndexes, ])

  print("Please wait this might take some time, Iteration:")
  print(i)
  Scalar.Step = 0.1
  Iterations  = 30
  
  #-------------------------LMSquareLossIterations (SLI_Error)--------------
    LMSquareLossIterations.W.Vec <- LMSquareLossIterations(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],Iterations,Scalar.Step)
    SLI.WholeError.Vec <-Find_Wmatrix_MeanL2Error(Spam$TrainingData, Spam$TrainingLabels,as.matrix(LMSquareLossIterations.W.Vec),Spam$BinaryClassification)
    #print("dim SLI.WHoleError")
    #print(dim(as.matrix(SLI.WholeError.Vec)))
    #SLI.WholeError.Mat <- cbind(SLI.WholeError.Mat,SLI.WholeError.Vec)
    
    SLI.TrainError.Vec <-Find_Wmatrix_MeanL2Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],as.matrix(LMSquareLossIterations.W.Vec),Spam$BinaryClassification)
    SLI.TrainError.Mat <- cbind(SLI.TrainError.Mat,SLI.TrainError.Vec)
  
    SLI.TestError.Vec  <-Find_Wmatrix_MeanL2Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],as.matrix(LMSquareLossIterations.W.Vec),Spam$BinaryClassification)
    SLI.TestError.Mat<- cbind(SLI.TestError.Mat,SLI.TestError.Vec)
  #------------------------------------------------------------------------
  
  #--------------------LMSquareLossEarlyStoppingCV (ES_CV_Error)-----------
    ES_CV_Error.fold.n  <- 4
    ES_CV_Error.fold.vec<-Random_Folds(NROW(trainData),ES_CV_Error.fold.n)
    Scalar.Step = 0.1
    Iterations  = 30
    #ES_CV_Error.W.Vec <- LMSquareLossEarlyStoppingCV(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],Spam$Folds.Vec,Spam$Folds.n,Spam$Iterations)
    ES_CV_Error.List <- LMSquareLossEarlyStoppingCV(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],ES_CV_Error.fold.vec,ES_CV_Error.fold.n,Iterations)
      
    #ES_CV_Error.WholeError.Vec   <-Find_Wmatrix_MeanL2Error(Spam$TrainingData, Spam$TrainingLabels,as.matrix(ES_CV_Error.List$w.mat),Spam$BinaryClassification)
    #ES_CV_Error.WholeError.Mat <- cbind(ES_CV_Error.WholeError.Mat,ES_CV_Error.WholeError.Vec)
  
    ES_CV_Error.TrainError.Vec <-Find_Wmatrix_MeanL2Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],as.matrix(ES_CV_Error.List$w.mat),Spam$BinaryClassification)
    #ES_CV_Error.TrainError.Mat <- cbind(ES_CV_Error.TrainError.Mat,ES_CV_Error.TrainError.Vec)
  
    ES_CV_Error.TestError.Vec  <-Find_Wmatrix_MeanL2Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],as.matrix(ES_CV_Error.List$w.mat),Spam$BinaryClassification)
    #ES_CV_Error.TestError.Mat  <- cbind(ES_CV_Error.TestError.Mat,ES_CV_Error.TestError.Vec)
  #------------------------------------------------------------------------
   
   
   #-------------------------LMSquareLossL2--(SL_L2_Error)------------------
    Normalized_TrainingData_List <- NormalizeMatrix_List(trainData[,1:NCOL(trainData)-1])
    Penalty.Scalar = 1
    opt.thresh = .2

    SL_L2.W.Matrix <-LMSquareLossL2(Normalized_TrainingData_List$NormalizedMatrix, trainData[,NCOL(trainData)], Penalty.Scalar, opt.thresh,Spam$Initial.Vector)
    DeNormalized.W.Matrix <-(t(SL_L2.W.Matrix))/Normalized_TrainingData_List$sd

    #DeNorm.Error <-Find_Wmatrix_MeanL2Error(Spam$TrainingData, Spam$TrainingLabels,t(DeNormalized.W.Matrix),Spam$BinaryClassification)
    SL_L2.WholeError.Vec <-Find_Wmatrix_MeanL1Error(Spam$TrainingData, Spam$TrainingLabels,t(DeNormalized.W.Matrix),Spam$BinaryClassification)
    SL_L2.WholeError.Mat <- rbind(SL_L2.WholeError.Mat,SL_L2.WholeError.Vec)
    
    SL_L2.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],t(DeNormalized.W.Matrix),Spam$BinaryClassification)
    SL_L2.TestError.Mat <- rbind(SL_L2.TestError.Mat,SL_L2.TestError.Vec)
    
    SL_L2.TrainError.Vec <-Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],t(DeNormalized.W.Matrix),Spam$BinaryClassification)
    SL_L2.TrainError.Mat <- rbind(SL_L2.TrainError.Mat,SL_L2.TrainError.Vec)
   #------------------------------------------------------------------------
    
   #-----------------------LMSquareLossL2penalties--(SL_L2_Pen)-------------
    SL_L2_Pen.W.Mat <-LMSquareLossL2penalties(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)], Spam$Penalty.Vector)

    #DeNorm.Error <-Find_Wmatrix_MeanL2Error(Spam$TrainingData, Spam$TrainingLabels,t(DeNormalized.W.Matrix),Spam$BinaryClassification)
    #print("SL_L2_Pen.WholeError.Vec")
    SL_L2_Pen.WholeError.Vec <-Find_Wmatrix_MeanL1Error(Spam$TrainingData, Spam$TrainingLabels,(SL_L2_Pen.W.Mat),Spam$BinaryClassification)
    SL_L2_Pen.WholeError.Mat <- rbind(SL_L2_Pen.WholeError.Mat,SL_L2_Pen.WholeError.Vec)
    
    #print("SL_L2_Pen.TestError.Vec")
    SL_L2_Pen.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],(SL_L2_Pen.W.Mat),Spam$BinaryClassification)
    SL_L2_Pen.TestError.Mat <- rbind(SL_L2_Pen.TestError.Mat,SL_L2_Pen.TestError.Vec)
    
    #print("SL_L2_Pen.TrainError.Vec")
    SL_L2_Pen.TrainError.Vec <- Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],(SL_L2_Pen.W.Mat),Spam$BinaryClassification)
    SL_L2_Pen.TrainError.Mat <- cbind(SL_L2_Pen.TrainError.Mat,SL_L2_Pen.TrainError.Vec)
   #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
    SL_L2CV.fold.n  <- 4
    SL_L2CV.fold.vec<-Random_Folds(NROW(trainData),SL_L2CV.fold.n)
    
    SL_L2CV.List <- LMSquareLossL2CV(as.matrix(trainData[,1:NCOL(trainData)-1]), as.matrix(trainData[,NCOL(trainData)]),as.matrix(t(SL_L2CV.fold.vec)), as.matrix(Spam$Penalty.Vector))
    

    SL_L2CV.WholeError.Vec <- Find_Wmatrix_MeanL1Error(Spam$TrainingData, Spam$TrainingLabels,(SL_L2CV.List$weight.vec),Spam$BinaryClassification)
    SL_L2CV.WholeError.Mat <- rbind(SL_L2CV.WholeError.Mat,SL_L2CV.WholeError.Vec)
    
    
    #SL_L2CV.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],t(SL_L2CV.W.Mat),Spam$BinaryClassification)
    SL_L2CV.TestError.Vec <- SL_L2CV.List$mean.validation.loss
    SL_L2CV.TestError.Mat <- rbind(SL_L2CV.TestError.Mat,SL_L2CV.TestError.Vec)
    #print("SL_L2CV.TestError.Vec")
    #print(dim(as.matrix(SL_L2CV.TestError.Vec)))
    
    #SL_L2CV.TrainError.Vec <- Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],t(SL_L2CV.W.Mat),Spam$BinaryClassification)
    SL_L2CV.TrainError.Vec <- SL_L2CV.List$mean.train.loss.vec
    SL_L2CV.TrainError.Mat <- rbind(SL_L2CV.TrainError.Mat,SL_L2CV.TrainError.Vec)
    #------------------------------------------------------------------------

}
```




### Matrix of loss values
```{r}
#-------------------Matrix of Cross Validation loss values-------------------------------------

#print(SLI.TestError.Mat)

  #Spam_LossMatrix <-rbind(as.double(SLI.TestError.Mat[30,2:5]),ES_CV_Error.TestError.Mat[2,2:5])
  Spam_LossMatrix <-rbind(as.double(SLI.TestError.Mat[30,2:5]),as.double(as.double(t(SL_L2.TestError.Mat[2:5]))))
  Spam_LossMatrix <-rbind(Spam_LossMatrix,as.double(colMeans(SL_L2_Pen.TestError.Mat)[2:5]))
  Spam_LossMatrix <-rbind(Spam_LossMatrix,as.double(SL_L2CV.TrainError.Mat[2:5]))
  print(Spam_LossMatrix)
  

  colnames(Spam_LossMatrix)<-c("Fold1","Fold2","Fold3","Fold4")
  rownames(Spam_LossMatrix)<-c("Linear Squared 30 Iterations","L2 Reg","L2 Reg with Penalty vector","L2 Reg with CV")
  barplot(Spam_LossMatrix, xlab = "Iterations", ylab = "Error",main = "LinearModels_Spam_LossMatrix",legend = (rownames(Spam_LossMatrix)),beside = TRUE)
```





### Train/validation loss plot
```{r}
  #-------------------- Data Set: 1 Spam ----------------------------------
    #-------------------------LMSquareLossIterations (SLI_Error)--------------
    plot(rowMeans(as.matrix(SLI.TrainError.Mat)),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "Spam Loss of each matrix",ylim=c(.1,1))
    lines(rowMeans(as.matrix(SLI.TestError.Mat)),type="o", col = "light blue")
    #------------------------------------------------------------------------
    
    #--------------------LMSquareLossEarlyStoppingCV (ES_CV_Error)-----------
    #lines(rowMeans(as.matrix(ES_CV_Error.TestError.Mat)),type="o", col = "green")
    #lines(rowMeans(as.matrix(ES_CV_Error.TrainError.Mat)),type="o", col = "Light green")
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2--(SL_L2_Error)------------------
    lines(as.matrix((SL_L2.TestError.Mat[2:NROW(SL_L2.TestError.Mat)])),type="o", col = "black")
    lines(as.matrix(SL_L2.TrainError.Mat[2:NROW(SL_L2.TrainError.Mat)]),type="o", col = "gray")
    #------------------------------------------------------------------------
    
    #-----------------------LMSquareLossL2penalties--(SL_L2_Pen)-------------
    lines(as.matrix((SL_L2_Pen.TestError.Mat[2:NROW(SL_L2_Pen.TestError.Mat)])),type="o", col = "red")
    lines(as.matrix((SL_L2_Pen.TrainError.Mat[2:NROW(SL_L2_Pen.TrainError.Mat)])),type="o", col = "pink")
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
    lines(as.matrix((SL_L2CV.TestError.Mat[2:NROW(SL_L2CV.TestError.Mat)])),type="o", col = "orange")
    lines(as.matrix((SL_L2CV.TrainError.Mat[2:NROW(SL_L2CV.TrainError.Mat)])),type="o", col = "gold")
    #------------------------------------------------------------------------
  #------------------------------------------------------------------------
```


### Extra Credit attempt: compaire The LinearModels to the KNN
```{r}
  #--------------------NN1ToKmaxPredict------------------------------------
  Spam<-Prep_Spam()
  Fold.vec = Random_Folds(Spam$n_Elements,4)
  Fold.n   = 4
  KNNLearnCV.List = KNNLearnCV(Spam$TrainingData, Spam$TrainingLabels, 30, Fold.vec, Fold.n)
  #------------------------------------------------------------------------

```


```{r}
  plot(colMeans(as.matrix(KNNLearnCV.List$TestMeanError.mat)),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "",ylim=c(0,1))
  lines(colMeans(as.matrix(KNNLearnCV.List$TrainMeanError.mat)),type="o", col = "gray")
  
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
    lines(as.matrix((SL_L2CV.TestError.Mat[2:NROW(SL_L2CV.TestError.Mat)])),type="o", col = "orange")
    lines(as.matrix((SL_L2CV.TrainError.Mat[2:NROW(SL_L2CV.TrainError.Mat)])),type="o", col = "gold")
    #------------------------------------------------------------------------
dot.x <- KNNLearnCV.List$selected.KNN
dot.y <- KNNLearnCV.List$TestMeanError.Means[dot.x]

matpoints(x = dot.x,
          y = dot.y,
          col = 2,
          pch = 19)
 # legend(25,30,c("decay","growth"),col=c("blue","red"),pch=c(15,19), y.intersp=1800)
```

## Data set 2: SAheart
```{r}
# Data set 2: SAheart
  SAheart<-Prep_SAheart()
```
### Code 
```{r}
  #-------------------- Data set 2: SAheart ----------------------------------
  LossMat.fold.n  <- 4
  LossMat.fold.vec<-Random_Folds((SAheart$n_Elements),LossMat.fold.n)
  Data = cbind(SAheart$TrainingData ,SAheart$TrainingLabels)
  #Randomly shuffle the data
  #Data<-Data[sample(nrow(Data)),]

  SLI.WholeError.Mat = 0
  SLI.TrainError.Mat = 0
  SLI.TestError.Mat  = 0
  
  ES_CV_Error.WholeError.Mat = 0
  ES_CV_Error.TrainError.Mat = 0
  ES_CV_Error.TestError.Mat  = 0
  
  SL_L2.WholeError.Mat = 0
  SL_L2.TrainError.Mat = 0
  SL_L2.TestError.Mat  = 0
  
  SL_L2_Pen.WholeError.Mat = 0
  SL_L2_Pen.TrainError.Mat = 0
  SL_L2_Pen.TestError.Mat  = 0
  
  SL_L2CV.WholeError.Mat = 0
  SL_L2CV.TrainError.Mat = 0
  SL_L2CV.TestError.Mat  = 0
  #Perform folds.n fold cross validation
  for(i in 1:(LossMat.fold.n))
  {
    testIndexes <- which(LossMat.fold.vec==i,arr.ind=TRUE)
    testData    <- as.matrix(Data[testIndexes, ])
    trainData   <- as.matrix(Data[-testIndexes, ])
  
    print("Please wait this might take some time, Iteration:")
    print(i)
    
    Scalar.Step = 0.01
    Iterations  = 30
    #-------------------------LMSquareLossIterations (SLI_Error)--------------
    LMSquareLossIterations.W.Vec <- LMSquareLossIterations(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],Iterations,Scalar.Step)
    SLI.WholeError.Vec <-Find_Wmatrix_MeanL2Error(SAheart$TrainingData, SAheart$TrainingLabels,as.matrix(LMSquareLossIterations.W.Vec),SAheart$BinaryClassification)
    #print("dim SLI.WHoleError")
    #print(dim(as.matrix(SLI.WholeError.Vec)))
    SLI.WholeError.Mat <- cbind(SLI.WholeError.Mat,SLI.WholeError.Vec)

   SLI.TrainError.Vec <-Find_Wmatrix_MeanL2Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],as.matrix(LMSquareLossIterations.W.Vec),SAheart$BinaryClassification)
   SLI.TrainError.Mat <- cbind(SLI.TrainError.Mat,SLI.TrainError.Vec)

    SLI.TestError.Vec  <-Find_Wmatrix_MeanL2Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],as.matrix(LMSquareLossIterations.W.Vec),SAheart$BinaryClassification)
    SLI.TestError.Mat<- cbind(SLI.TestError.Mat,SLI.TestError.Vec)
    #------------------------------------------------------------------------

    #--------------------LMSquareLossEarlyStoppingCV (ES_CV_Error)-----------
    ES_CV_Error.fold.n  <- 4
    ES_CV_Error.fold.vec<-Random_Folds(NROW(trainData),ES_CV_Error.fold.n)

    #ES_CV_Error.W.Vec <- LMSquareLossEarlyStoppingCV(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],SAheart$Folds.Vec,SAheart$Folds.n,SAheart$Iterations)
    ES_CV_Error.List <- LMSquareLossEarlyStoppingCV(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],ES_CV_Error.fold.vec,ES_CV_Error.fold.n,Iterations)
    
    ES_CV_Error.WholeError.Vec   <-Find_Wmatrix_MeanL2Error(SAheart$TrainingData, SAheart$TrainingLabels,as.matrix(ES_CV_Error.List$w.mat),SAheart$BinaryClassification)
    #print("dim ES_CV_Error.WholeError.Vec")
    #print(dim(as.matrix(ES_CV_Error.WholeError.Vec)))
    #ES_CV_Error.WholeError.Mat <- cbind(ES_CV_Error.WholeError.Mat,ES_CV_Error.WholeError.Vec)

    ES_CV_Error.TrainError.Vec <-Find_Wmatrix_MeanL2Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],as.matrix(ES_CV_Error.List$w.mat),SAheart$BinaryClassification)
    #ES_CV_Error.TrainError.Mat <- cbind(ES_CV_Error.TrainError.Mat,ES_CV_Error.TrainError.Vec)

    ES_CV_Error.TestError.Vec  <-Find_Wmatrix_MeanL2Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],as.matrix(ES_CV_Error.List$w.mat),SAheart$BinaryClassification)
    #ES_CV_Error.TestError.Mat  <- cbind(ES_CV_Error.TestError.Mat,ES_CV_Error.TestError.Vec)
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2--(SL_L2_Error)------------------
    Normalized_TrainingData_List <- NormalizeMatrix_List(trainData[,1:NCOL(trainData)-1])
    Penalty.Scalar = 1
    opt.thresh = .2

    SL_L2.W.Matrix <-LMSquareLossL2(Normalized_TrainingData_List$NormalizedMatrix, trainData[,NCOL(trainData)], Penalty.Scalar, opt.thresh,SAheart$Initial.Vector)
    DeNormalized.W.Matrix <-(t(SL_L2.W.Matrix))/Normalized_TrainingData_List$sd

    #DeNorm.Error <-Find_Wmatrix_MeanL2Error(SAheart$TrainingData, SAheart$TrainingLabels,t(DeNormalized.W.Matrix),SAheart$BinaryClassification)
    SL_L2.WholeError.Vec <-Find_Wmatrix_MeanL1Error(SAheart$TrainingData, SAheart$TrainingLabels,t(DeNormalized.W.Matrix),SAheart$BinaryClassification)
    SL_L2.WholeError.Mat <- rbind(SL_L2.WholeError.Mat,SL_L2.WholeError.Vec)
    
    SL_L2.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],t(DeNormalized.W.Matrix),SAheart$BinaryClassification)
    SL_L2.TestError.Mat <- rbind(SL_L2.TestError.Mat,SL_L2.TestError.Vec)
    
    SL_L2.TrainError.Vec <-Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],t(DeNormalized.W.Matrix),SAheart$BinaryClassification)
    SL_L2.TrainError.Mat <- rbind(SL_L2.TrainError.Mat,SL_L2.TrainError.Vec)
    #------------------------------------------------------------------------
    
    #-----------------------LMSquareLossL2penalties--(SL_L2_Pen)-------------
    Penalty.Scalar = 1
    opt.thresh = .2
    SL_L2_Pen.W.Mat <-LMSquareLossL2penalties(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)], SAheart$Penalty.Vector)

    #DeNorm.Error <-Find_Wmatrix_MeanL2Error(SAheart$TrainingData, SAheart$TrainingLabels,t(DeNormalized.W.Matrix),SAheart$BinaryClassification)
    #print("SL_L2_Pen.WholeError.Vec")
    SL_L2_Pen.WholeError.Vec <-Find_Wmatrix_MeanL1Error(SAheart$TrainingData, SAheart$TrainingLabels,(SL_L2_Pen.W.Mat),SAheart$BinaryClassification)
    SL_L2_Pen.WholeError.Mat <- rbind(SL_L2_Pen.WholeError.Mat,SL_L2_Pen.WholeError.Vec)
    
    #print("SL_L2_Pen.TestError.Vec")
    SL_L2_Pen.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],(SL_L2_Pen.W.Mat),SAheart$BinaryClassification)
    SL_L2_Pen.TestError.Mat <- rbind(SL_L2_Pen.TestError.Mat,SL_L2_Pen.TestError.Vec)
    
    #print("SL_L2_Pen.TrainError.Vec")
    SL_L2_Pen.TrainError.Vec <- Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],(SL_L2_Pen.W.Mat),SAheart$BinaryClassification)
    SL_L2_Pen.TrainError.Mat <- rbind(SL_L2_Pen.TrainError.Mat,SL_L2_Pen.TrainError.Vec)
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
    SL_L2CV.fold.n  <- 2
    SL_L2CV.fold.vec<-Random_Folds(NROW(trainData),SL_L2CV.fold.n)
      
    SL_L2CV.List <- LMSquareLossL2CV(as.matrix(trainData[,1:NCOL(trainData)-1]), as.matrix(trainData[,NCOL(trainData)]),as.matrix(t(SL_L2CV.fold.vec)), as.matrix(SAheart$Penalty.Vector))
      

    SL_L2CV.WholeError.Vec <- Find_Wmatrix_MeanL1Error(SAheart$TrainingData, SAheart$TrainingLabels,(SL_L2CV.List$weight.vec),SAheart$BinaryClassification)
    SL_L2CV.WholeError.Mat <- rbind(SL_L2CV.WholeError.Mat,SL_L2CV.WholeError.Vec)
      
      
    #SL_L2CV.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],t(SL_L2CV.W.Mat),SAheart$BinaryClassification)
    SL_L2CV.TestError.Vec <- SL_L2CV.List$mean.validation.loss
    SL_L2CV.TestError.Mat <- rbind(SL_L2CV.TestError.Mat,SL_L2CV.TestError.Vec)
    #print("SL_L2CV.TestError.Vec")
    #print(dim(as.matrix(SL_L2CV.TestError.Vec)))
      
    #SL_L2CV.TrainError.Vec <- Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],t(SL_L2CV.W.Mat),SAheart$BinaryClassification)
    SL_L2CV.TrainError.Vec <- SL_L2CV.List$mean.train.loss.vec
    SL_L2CV.TrainError.Mat <- rbind(SL_L2CV.TrainError.Mat,SL_L2CV.TrainError.Vec)
    #print("SL_L2CV.TrainError.Vec")
    #print(dim(as.matrix(SL_L2CV.TrainError.Vec)))
    #------------------------------------------------------------------------
  }

  #------------------------------------------------------------------------
```
### Matrix of loss values
```{r}
#-------------------Matrix of Cross Validation loss values-------------------------------------

#print(SLI.TestError.Mat)

  #Spam_LossMatrix <-rbind(,ES_CV_Error.TestError.Mat[2,2:5])
  Spam_LossMatrix <-rbind(as.double(SLI.TestError.Mat[30,2:5]),as.double(as.double(t(SL_L2.TestError.Mat[2:5]))))
  Spam_LossMatrix <-rbind(Spam_LossMatrix,as.double(colMeans(SL_L2_Pen.TestError.Mat)[2:5]))
  Spam_LossMatrix <-rbind(Spam_LossMatrix,as.double(SL_L2CV.TrainError.Mat[2:5]))
  print(Spam_LossMatrix)
  

  colnames(Spam_LossMatrix)<-c("Fold1","Fold2","Fold3","Fold4")
  rownames(Spam_LossMatrix)<-c("Linear Squared 30 Iterations","L2 Reg","L2 Reg with Penalty vector","L2 Reg with CV")
  barplot(Spam_LossMatrix, xlab = "Iterations", ylab = "Error",main = "SAheart_LossMatrix",legend = (rownames(Spam_LossMatrix)),beside = TRUE)
```

### Train/validation loss plot


```{r}
  #-------------------- Graph Data Set: 2 SAheart -----------------------------
    #-------------------------LMSquareLossIterations (SLI_Error)--------------
    plot(rowMeans(as.matrix(SLI.TrainError.Mat)),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "SAheart Loss of each matrix",ylim=c(.1,1))
    lines(rowMeans(as.matrix(SLI.TestError.Mat)),type="o", col = "light blue")
    #------------------------------------------------------------------------
    
    #--------------------LMSquareLossEarlyStoppingCV (ES_CV_Error)-----------
    lines(rowMeans(as.matrix(ES_CV_Error.TestError.Vec)),type="o", col = "green")
    lines(rowMeans(as.matrix(ES_CV_Error.TrainError.Vec)),type="o", col = "Light green")
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2--(SL_L2_Error)------------------
    lines(as.matrix((SL_L2.TestError.Mat[2:NROW(SL_L2.TestError.Mat)])),type="o", col = "black")
    lines(as.matrix(SL_L2.TrainError.Mat[2:NROW(SL_L2.TrainError.Mat)]),type="o", col = "gray")
    #------------------------------------------------------------------------
    
    #-----------------------LMSquareLossL2penalties--(SL_L2_Pen)-------------
    lines(as.matrix((SL_L2_Pen.TestError.Mat[2:NROW(SL_L2_Pen.TestError.Mat)])),type="o", col = "red")
    lines(as.matrix((SL_L2_Pen.TrainError.Mat[2:NROW(SL_L2_Pen.TrainError.Mat)])),type="o", col = "pink")
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
    lines(as.matrix((SL_L2CV.TestError.Mat[2:NROW(SL_L2CV.TestError.Mat)])),type="o", col = "orange")
    lines(as.matrix((SL_L2CV.TrainError.Mat[2:NROW(SL_L2CV.TrainError.Mat)])),type="o", col = "gold")
    #------------------------------------------------------------------------
  #------------------------------------------------------------------------
```
### Extra Credit attempt: compaire The LinearModels to the KNN
```{r}
  #--------------------NN1ToKmaxPredict------------------------------------
  SAheart<-Prep_SAheart()
  Fold.vec = Random_Folds(SAheart$n_Elements,4)
  Fold.n   = 4
  KNNLearnCV.List = KNNLearnCV(SAheart$TrainingData, SAheart$TrainingLabels, 30, Fold.vec, Fold.n)

  barplot(KNNLearnCV.List$TestMeanError.Means,main = "SAheart: KNNLearnCV L2 Mean Error",xlab = "KNN Compared",ylab = "Error",beside = TRUE)
  #------------------------------------------------------------------------

```
```{r}
  plot(colMeans(as.matrix(KNNLearnCV.List$TestMeanError.mat)),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "",ylim=c(0,1))
  lines(colMeans(as.matrix(KNNLearnCV.List$TrainMeanError.mat)),type="o", col = "gray")
  
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
    lines(as.matrix((SL_L2CV.TestError.Mat[2:NROW(SL_L2CV.TestError.Mat)])),type="o", col = "orange")
    lines(as.matrix((SL_L2CV.TrainError.Mat[2:NROW(SL_L2CV.TrainError.Mat)])),type="o", col = "gold")
    #------------------------------------------------------------------------
dot.x <- KNNLearnCV.List$selected.KNN
dot.y <- KNNLearnCV.List$TestMeanError.Means[dot.x]

matpoints(x = dot.x,
          y = dot.y,
          col = 2,
          pch = 19)
 # legend(25,30,c("decay","growth"),col=c("blue","red"),pch=c(15,19), y.intersp=1800)
```


## Data set 3: Zip.train
```{r}
  ZipTrain<-Prep_ZipTrain()
```
### Code 
```{r}
  #-------------------- Data set 3: Zip.train ----------------------------------
  LossMat.fold.n  <- 4
  LossMat.fold.vec<-Random_Folds((ZipTrain$n_Elements),LossMat.fold.n)
  Data = cbind(ZipTrain$TrainingData ,ZipTrain$TrainingLabels)
  #Randomly shuffle the data
  #Data<-Data[sample(nrow(Data)),]

  SLI.WholeError.Mat = 0
  SLI.TrainError.Mat = 0
  SLI.TestError.Mat  = 0
  
  ES_CV_Error.WholeError.Mat = 0
  ES_CV_Error.TrainError.Mat = 0
  ES_CV_Error.TestError.Mat  = 0
  
  SL_L2.WholeError.Mat = 0
  SL_L2.TrainError.Mat = 0
  SL_L2.TestError.Mat  = 0
  
  SL_L2_Pen.WholeError.Mat = 0
  SL_L2_Pen.TrainError.Mat = 0
  SL_L2_Pen.TestError.Mat  = 0
  
  SL_L2CV.WholeError.Mat = 0
  SL_L2CV.TrainError.Mat = 0
  SL_L2CV.TestError.Mat  = 0
  #Perform folds.n fold cross validation
  for(i in 1:(LossMat.fold.n))
  {
    testIndexes <- which(LossMat.fold.vec==i,arr.ind=TRUE)
    testData    <- as.matrix(Data[testIndexes, ])
    trainData   <- as.matrix(Data[-testIndexes, ])
  
    print("Please wait this might take some time, Iteration:")
    print(i)
    
    Scalar.Step = 0.01
    Iterations  = 30
    
    #-------------------------LMSquareLossIterations (SLI_Error)-------------
    LMSquareLossIterations.W.Vec <- LMSquareLossIterations(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],Iterations,Scalar.Step)
    SLI.WholeError.Vec <-Find_Wmatrix_MeanL2Error(ZipTrain$TrainingData, ZipTrain$TrainingLabels,as.matrix(LMSquareLossIterations.W.Vec),ZipTrain$BinaryClassification)
    print("dim SLI.WHoleError")
    print(dim(as.matrix(SLI.WholeError.Vec)))
    SLI.WholeError.Mat <- cbind(SLI.WholeError.Mat,SLI.WholeError.Vec)

    SLI.TrainError.Vec <-Find_Wmatrix_MeanL2Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],as.matrix(LMSquareLossIterations.W.Vec),ZipTrain$BinaryClassification)
    SLI.TrainError.Mat <- cbind(SLI.TrainError.Mat,SLI.TrainError.Vec)

    SLI.TestError.Vec  <-Find_Wmatrix_MeanL2Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],as.matrix(LMSquareLossIterations.W.Vec),ZipTrain$BinaryClassification)
   SLI.TestError.Mat<- cbind(SLI.TestError.Mat,SLI.TestError.Vec)
    #------------------------------------------------------------------------

    #--------------------LMSquareLossEarlyStoppingCV (ES_CV_Error)-----------
    ES_CV_Error.fold.n  <- 4
    ES_CV_Error.fold.vec<-Random_Folds(NROW(trainData),ES_CV_Error.fold.n)

    #ES_CV_Error.W.Vec <- LMSquareLossEarlyStoppingCV(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],ZipTrain$Folds.Vec,ZipTrain$Folds.n,ZipTrain$Iterations)
    ES_CV_Error.List <- LMSquareLossEarlyStoppingCV(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],ES_CV_Error.fold.vec,ES_CV_Error.fold.n,Iterations)
    
    ES_CV_Error.WholeError.Vec   <-Find_Wmatrix_MeanL2Error(ZipTrain$TrainingData, ZipTrain$TrainingLabels,as.matrix(ES_CV_Error.List$w.mat),ZipTrain$BinaryClassification)
    #print("dim ES_CV_Error.WholeError.Vec")
    #print(dim(as.matrix(ES_CV_Error.WholeError.Vec)))
    #ES_CV_Error.WholeError.Mat <- cbind(ES_CV_Error.WholeError.Mat,ES_CV_Error.WholeError.Vec)

    ES_CV_Error.TrainError.Vec <-Find_Wmatrix_MeanL2Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],as.matrix(ES_CV_Error.List$w.mat),ZipTrain$BinaryClassification)
    #ES_CV_Error.TrainError.Mat <- cbind(ES_CV_Error.TrainError.Mat,ES_CV_Error.TrainError.Vec)

    ES_CV_Error.TestError.Vec  <-Find_Wmatrix_MeanL2Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],as.matrix(ES_CV_Error.List$w.mat),ZipTrain$BinaryClassification)
    #ES_CV_Error.TestError.Mat  <- cbind(ES_CV_Error.TestError.Mat,ES_CV_Error.TestError.Vec)
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2--(SL_L2_Error)------------------
    Normalized_TrainingData_List <- NormalizeMatrix_List(trainData[,1:NCOL(trainData)-1])
    Penalty.Scalar = 1
    opt.thresh = .2

    SL_L2.W.Matrix <-LMSquareLossL2(Normalized_TrainingData_List$NormalizedMatrix, trainData[,NCOL(trainData)], Penalty.Scalar, opt.thresh,ZipTrain$Initial.Vector)
    DeNormalized.W.Matrix <-(t(SL_L2.W.Matrix))/Normalized_TrainingData_List$sd

    #DeNorm.Error <-Find_Wmatrix_MeanL2Error(ZipTrain$TrainingData, ZipTrain$TrainingLabels,t(DeNormalized.W.Matrix),ZipTrain$BinaryClassification)
    SL_L2.WholeError.Vec <-Find_Wmatrix_MeanL1Error(ZipTrain$TrainingData, ZipTrain$TrainingLabels,t(DeNormalized.W.Matrix),ZipTrain$BinaryClassification)
    SL_L2.WholeError.Mat <- rbind(SL_L2.WholeError.Mat,SL_L2.WholeError.Vec)
    
    SL_L2.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],t(DeNormalized.W.Matrix),ZipTrain$BinaryClassification)
    SL_L2.TestError.Mat <- rbind(SL_L2.TestError.Mat,SL_L2.TestError.Vec)
    
    SL_L2.TrainError.Vec <-Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],t(DeNormalized.W.Matrix),ZipTrain$BinaryClassification)
    SL_L2.TrainError.Mat <- rbind(SL_L2.TrainError.Mat,SL_L2.TrainError.Vec)
    #------------------------------------------------------------------------
    
    #-----------------------LMSquareLossL2penalties--(SL_L2_Pen)-------------
    Penalty.Scalar = 1
    opt.thresh = .2
    SL_L2_Pen.W.Mat <-LMSquareLossL2penalties(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)], ZipTrain$Penalty.Vector)

    #DeNorm.Error <-Find_Wmatrix_MeanL2Error(ZipTrain$TrainingData, ZipTrain$TrainingLabels,t(DeNormalized.W.Matrix),ZipTrain$BinaryClassification)
    #print("SL_L2_Pen.WholeError.Vec")
    SL_L2_Pen.WholeError.Vec <-Find_Wmatrix_MeanL1Error(ZipTrain$TrainingData, ZipTrain$TrainingLabels,(SL_L2_Pen.W.Mat),ZipTrain$BinaryClassification)
    SL_L2_Pen.WholeError.Mat <- rbind(SL_L2_Pen.WholeError.Mat,SL_L2_Pen.WholeError.Vec)
    
    #print("SL_L2_Pen.TestError.Vec")
    SL_L2_Pen.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],(SL_L2_Pen.W.Mat),ZipTrain$BinaryClassification)
    SL_L2_Pen.TestError.Mat <- rbind(SL_L2_Pen.TestError.Mat,SL_L2_Pen.TestError.Vec)
    
    #print("SL_L2_Pen.TrainError.Vec")
    SL_L2_Pen.TrainError.Vec <- Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],(SL_L2_Pen.W.Mat),ZipTrain$BinaryClassification)
    SL_L2_Pen.TrainError.Mat <- rbind(SL_L2_Pen.TrainError.Mat,SL_L2_Pen.TrainError.Vec)
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
      SL_L2CV.fold.n  <- 4
      SL_L2CV.fold.vec<-Random_Folds(NROW(trainData),SL_L2CV.fold.n)
      
      SL_L2CV.List <- LMSquareLossL2CV(as.matrix(trainData[,1:NCOL(trainData)-1]), as.matrix(trainData[,NCOL(trainData)]),as.matrix(t(SL_L2CV.fold.vec)), as.matrix(ZipTrain$Penalty.Vector))
      

      SL_L2CV.WholeError.Vec <- Find_Wmatrix_MeanL1Error(ZipTrain$TrainingData, ZipTrain$TrainingLabels,(SL_L2CV.List$weight.vec),ZipTrain$BinaryClassification)
      SL_L2CV.WholeError.Mat <- rbind(SL_L2CV.WholeError.Mat,SL_L2CV.WholeError.Vec)
      
      
      #SL_L2CV.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],t(SL_L2CV.W.Mat),ZipTrain$BinaryClassification)
      SL_L2CV.TestError.Vec <- SL_L2CV.List$mean.validation.loss
      SL_L2CV.TestError.Mat <- rbind(SL_L2CV.TestError.Mat,SL_L2CV.TestError.Vec)
      #print("SL_L2CV.TestError.Vec")
      #print(dim(as.matrix(SL_L2CV.TestError.Vec)))
      
      #SL_L2CV.TrainError.Vec <- Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],t(SL_L2CV.W.Mat),ZipTrain$BinaryClassification)
      SL_L2CV.TrainError.Vec <- SL_L2CV.List$mean.train.loss.vec
      SL_L2CV.TrainError.Mat <- rbind(SL_L2CV.TrainError.Mat,SL_L2CV.TrainError.Vec)
      #print("SL_L2CV.TrainError.Vec")
      #print(dim(as.matrix(SL_L2CV.TrainError.Vec)))
    #------------------------------------------------------------------------
  }


  #------------------------------------------------------------------------
```
### Matrix of loss values
```{r}
#-------------------Matrix of Cross Validation loss values-------------------------------------

#print(SLI.TestError.Mat)

  #ZipTrain_LossMatrix <-rbind(,ES_CV_Error.TestError.Mat[2,2:5])
  ZipTrain_LossMatrix <-rbind(as.double(SLI.TestError.Mat[30,2:5]),as.double(as.double(t(SL_L2.TestError.Mat[2:5]))))
  ZipTrain_LossMatrix <-rbind(ZipTrain_LossMatrix,as.double(colMeans(SL_L2_Pen.TestError.Mat)[2:5]))
  ZipTrain_LossMatrix <-rbind(ZipTrain_LossMatrix,as.double(SL_L2CV.TrainError.Mat[2:5]))

  

  colnames(ZipTrain_LossMatrix)<-c("Fold1","Fold2","Fold3","Fold4")
  rownames(ZipTrain_LossMatrix)<-c("Linear Squared 30 Iterations","L2 Reg","L2 Reg with Penalty vector","L2 Reg with CV")
  barplot(ZipTrain_LossMatrix, xlab = "Iterations", ylab = "Error",main = "_LossMatrix",legend = (rownames(Spam_LossMatrix)),beside = TRUE)
```

### Train/validation loss plot
```{r}
  #-------------------- Graph Data Set: 2 SAheart -----------------------------
    #-------------------------LMSquareLossIterations (SLI_Error)--------------
    plot(rowMeans(as.matrix(SLI.TrainError.Mat)),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "SAheart Loss of each matrix",ylim=c(0,200))
    lines(rowMeans(as.matrix(SLI.TestError.Mat)),type="o", col = "light blue")
    #------------------------------------------------------------------------
    
    #--------------------LMSquareLossEarlyStoppingCV (ES_CV_Error)-----------
    lines(rowMeans(as.matrix(ES_CV_Error.TestError.Vec)),type="o", col = "green")
    lines(rowMeans(as.matrix(ES_CV_Error.TrainError.Vec)),type="o", col = "Light green")
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2--(SL_L2_Error)------------------
    lines(as.matrix((SL_L2.TestError.Mat[2:NROW(SL_L2.TestError.Mat)])),type="o", col = "black")
    lines(as.matrix(SL_L2.TrainError.Mat[2:NROW(SL_L2.TrainError.Mat)]),type="o", col = "gray")
    #------------------------------------------------------------------------
    
    #-----------------------LMSquareLossL2penalties--(SL_L2_Pen)-------------
    
    lines(as.matrix((SL_L2_Pen.TrainError.Mat[2:NROW(SL_L2_Pen.TrainError.Mat)])),type="o", col = "pink")
    lines(as.matrix((SL_L2_Pen.TestError.Mat[2:NROW(SL_L2_Pen.TestError.Mat)])),type="o", col = "red")
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
    lines(as.matrix((SL_L2CV.TestError.Mat[2:NROW(SL_L2CV.TestError.Mat)])),type="o", col = "orange")
    lines(as.matrix((SL_L2CV.TrainError.Mat[2:NROW(SL_L2CV.TrainError.Mat)])),type="o", col = "gold")
    #------------------------------------------------------------------------
  #------------------------------------------------------------------------
```
### Extra Credit attempt: compaire The LinearModels to the KNN
```{r}
  #--------------------KNNLearnCV------------------------------------
  Ziptrain<-Prep_ZipTrain()
  Fold.vec = Random_Folds(Ziptrain$n_Elements,4)
  Fold.n   = 4
  KNNLearnCV.List = KNNLearnCV(Ziptrain$TrainingData, Ziptrain$TrainingLabels, 30, Fold.vec, Fold.n)

  barplot(KNNLearnCV.List$TestMeanError.Means,main = "Ziptrain: KNNLearnCV L2 Mean Error",xlab = "KNN Compared",ylab = "Error",beside = TRUE)
  #------------------------------------------------------------------------

```



## Data set 4: Prostate
```{r}
   Prostate<-Prep_Prostate()
```
### Code 
```{r}
  #-------------------- Data set 4: Prostate ----------------------------------
  LossMat.fold.n  <- 4
  LossMat.fold.vec<-Random_Folds((Prostate$n_Elements),LossMat.fold.n)
  Data = cbind(Prostate$TrainingData ,Prostate$TrainingLabels)
  #Randomly shuffle the data
  #Data<-Data[sample(nrow(Data)),]

  SLI.WholeError.Mat = 0
  SLI.TrainError.Mat = 0
  SLI.TestError.Mat  = 0
  
  ES_CV_Error.WholeError.Mat = 0
  ES_CV_Error.TrainError.Mat = 0
  ES_CV_Error.TestError.Mat  = 0
  
  SL_L2.WholeError.Mat = 0
  SL_L2.TrainError.Mat = 0
  SL_L2.TestError.Mat  = 0
  
  SL_L2_Pen.WholeError.Mat = 0
  SL_L2_Pen.TrainError.Mat = 0
  SL_L2_Pen.TestError.Mat  = 0
  
  SL_L2CV.WholeError.Mat = 0
  SL_L2CV.TrainError.Mat = 0
  SL_L2CV.TestError.Mat  = 0
  #Perform folds.n fold cross validation
  for(i in 1:(LossMat.fold.n))
  {
    testIndexes <- which(LossMat.fold.vec==i,arr.ind=TRUE)
    testData    <- as.matrix(Data[testIndexes, ])
    trainData   <- as.matrix(Data[-testIndexes, ])
  
    print("Please wait this might take some time, Iteration:")
    print(i)
    
    Scalar.Step = 0.1
    Iterations  = 30

    #-------------------------LMSquareLossIterations (SLI_Error)-------------

    LMSquareLossIterations.W.Vec <- LMSquareLossIterations(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],Iterations,Scalar.Step)
    SLI.WholeError.Vec <-Find_Wmatrix_MeanL2Error(Prostate$TrainingData, Prostate$TrainingLabels,as.matrix(LMSquareLossIterations.W.Vec),Prostate$BinaryClassification)
    print("dim SLI.WHoleError")
    print(dim(as.matrix(SLI.WholeError.Vec)))
    SLI.WholeError.Mat <- cbind(SLI.WholeError.Mat,SLI.WholeError.Vec)

    SLI.TrainError.Vec <-Find_Wmatrix_MeanL2Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],as.matrix(LMSquareLossIterations.W.Vec),Prostate$BinaryClassification)
    SLI.TrainError.Mat <- cbind(SLI.TrainError.Mat,SLI.TrainError.Vec)

    SLI.TestError.Vec  <-Find_Wmatrix_MeanL2Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],as.matrix(LMSquareLossIterations.W.Vec),Prostate$BinaryClassification)
    SLI.TestError.Mat<- cbind(SLI.TestError.Mat,SLI.TestError.Vec)
    #------------------------------------------------------------------------

    #--------------------LMSquareLossEarlyStoppingCV (ES_CV_Error)-----------
    ES_CV_Error.fold.n  <- 4
    ES_CV_Error.fold.vec<-Random_Folds(NROW(trainData),ES_CV_Error.fold.n)
    Scalar.Step = 0.1
    Iterations  = 30
    #ES_CV_Error.W.Vec <- LMSquareLossEarlyStoppingCV(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],Prostate$Folds.Vec,Prostate$Folds.n,Prostate$Iterations)
    ES_CV_Error.List <- LMSquareLossEarlyStoppingCV(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],ES_CV_Error.fold.vec,ES_CV_Error.fold.n,Iterations)
    
    ES_CV_Error.WholeError.Vec   <-Find_Wmatrix_MeanL2Error(Prostate$TrainingData, Prostate$TrainingLabels,as.matrix(ES_CV_Error.List$w.mat),Prostate$BinaryClassification)
    #print("dim ES_CV_Error.WholeError.Vec")
    #print(dim(as.matrix(ES_CV_Error.WholeError.Vec)))
    #ES_CV_Error.WholeError.Mat <- cbind(ES_CV_Error.WholeError.Mat,ES_CV_Error.WholeError.Vec)

   ES_CV_Error.TrainError.Vec <-Find_Wmatrix_MeanL2Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],as.matrix(ES_CV_Error.List$w.mat),Prostate$BinaryClassification)
   #ES_CV_Error.TrainError.Mat <- cbind(ES_CV_Error.TrainError.Mat,ES_CV_Error.TrainError.Vec)

   ES_CV_Error.TestError.Vec  <-Find_Wmatrix_MeanL2Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],as.matrix(ES_CV_Error.List$w.mat),Prostate$BinaryClassification)
   #ES_CV_Error.TestError.Mat  <- cbind(ES_CV_Error.TestError.Mat,ES_CV_Error.TestError.Vec)
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2--(SL_L2_Error)------------------
    Normalized_TrainingData_List <- NormalizeMatrix_List(trainData[,1:NCOL(trainData)-1])
    Penalty.Scalar = 1
    opt.thresh = .2

    SL_L2.W.Matrix <-LMSquareLossL2(Normalized_TrainingData_List$NormalizedMatrix, trainData[,NCOL(trainData)], Penalty.Scalar, opt.thresh,Prostate$Initial.Vector)
    DeNormalized.W.Matrix <-(t(SL_L2.W.Matrix))/Normalized_TrainingData_List$sd

    #DeNorm.Error <-Find_Wmatrix_MeanL2Error(Prostate$TrainingData, Prostate$TrainingLabels,t(DeNormalized.W.Matrix),Prostate$BinaryClassification)
    SL_L2.WholeError.Vec <-Find_Wmatrix_MeanL1Error(Prostate$TrainingData, Prostate$TrainingLabels,t(DeNormalized.W.Matrix),Prostate$BinaryClassification)
    SL_L2.WholeError.Mat <- rbind(SL_L2.WholeError.Mat,SL_L2.WholeError.Vec)
    
    SL_L2.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],t(DeNormalized.W.Matrix),Prostate$BinaryClassification)
    SL_L2.TestError.Mat <- rbind(SL_L2.TestError.Mat,SL_L2.TestError.Vec)
    
    SL_L2.TrainError.Vec <-Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],t(DeNormalized.W.Matrix),Prostate$BinaryClassification)
    SL_L2.TrainError.Mat <- rbind(SL_L2.TrainError.Mat,SL_L2.TrainError.Vec)
    #------------------------------------------------------------------------
    
    #-----------------------LMSquareLossL2penalties--(SL_L2_Pen)-------------
    Penalty.Scalar = 1
    opt.thresh = .2
    SL_L2_Pen.W.Mat <-LMSquareLossL2penalties(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)], Prostate$Penalty.Vector)

    #DeNorm.Error <-Find_Wmatrix_MeanL2Error(Prostate$TrainingData, Prostate$TrainingLabels,t(DeNormalized.W.Matrix),Prostate$BinaryClassification)
    #print("SL_L2_Pen.WholeError.Vec")
    SL_L2_Pen.WholeError.Vec <-Find_Wmatrix_MeanL1Error(Prostate$TrainingData, Prostate$TrainingLabels,(SL_L2_Pen.W.Mat),Prostate$BinaryClassification)
    SL_L2_Pen.WholeError.Mat <- rbind(SL_L2_Pen.WholeError.Mat,SL_L2_Pen.WholeError.Vec)
    
    #print("SL_L2_Pen.TestError.Vec")
    SL_L2_Pen.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],(SL_L2_Pen.W.Mat),Prostate$BinaryClassification)
    SL_L2_Pen.TestError.Mat <- rbind(SL_L2_Pen.TestError.Mat,SL_L2_Pen.TestError.Vec)
    
    #print("SL_L2_Pen.TrainError.Vec")
    SL_L2_Pen.TrainError.Vec <- Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],(SL_L2_Pen.W.Mat),Prostate$BinaryClassification)
    SL_L2_Pen.TrainError.Mat <- rbind(SL_L2_Pen.TrainError.Mat,SL_L2_Pen.TrainError.Vec)
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
      SL_L2CV.fold.n  <- 4
      SL_L2CV.fold.vec<-Random_Folds(NROW(trainData),SL_L2CV.fold.n)
      
      SL_L2CV.List <- LMSquareLossL2CV(as.matrix(trainData[,1:NCOL(trainData)-1]), as.matrix(trainData[,NCOL(trainData)]),as.matrix(t(SL_L2CV.fold.vec)), as.matrix(Prostate$Penalty.Vector))
      

      SL_L2CV.WholeError.Vec <- Find_Wmatrix_MeanL1Error(Prostate$TrainingData, Prostate$TrainingLabels,(SL_L2CV.List$weight.vec),Prostate$BinaryClassification)
      SL_L2CV.WholeError.Mat <- rbind(SL_L2CV.WholeError.Mat,SL_L2CV.WholeError.Vec)
      
      
      #SL_L2CV.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],t(SL_L2CV.W.Mat),Prostate$BinaryClassification)
      SL_L2CV.TestError.Vec <- SL_L2CV.List$mean.validation.loss
      SL_L2CV.TestError.Mat <- rbind(SL_L2CV.TestError.Mat,SL_L2CV.TestError.Vec)
      #print("SL_L2CV.TestError.Vec")
      #print(dim(as.matrix(SL_L2CV.TestError.Vec)))
      
      #SL_L2CV.TrainError.Vec <- Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],t(SL_L2CV.W.Mat),Prostate$BinaryClassification)
      SL_L2CV.TrainError.Vec <- SL_L2CV.List$mean.train.loss.vec
      SL_L2CV.TrainError.Mat <- rbind(SL_L2CV.TrainError.Mat,SL_L2CV.TrainError.Vec)
      #print("SL_L2CV.TrainError.Vec")
      #print(dim(as.matrix(SL_L2CV.TrainError.Vec)))
      
    #------------------------------------------------------------------------
  }

  #------------------------------------------------------------------------
```
### Matrix of loss values
```{r}
#-------------------Matrix of Cross Validation loss values-------------------------------------


```
### Train/validation loss plot
```{r}
  #-------------------- Data set 4: Prostate -----------------------------
    #-------------------------LMSquareLossIterations (SLI_Error)--------------
    plot(rowMeans(as.matrix(SLI.TrainError.Mat)),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "SAheart Loss of each matrix")
    lines(rowMeans(as.matrix(SLI.TestError.Mat)),type="o", col = "light blue")
    #------------------------------------------------------------------------
    
    #--------------------LMSquareLossEarlyStoppingCV (ES_CV_Error)-----------
    lines(rowMeans(as.matrix(ES_CV_Error.TestError.Vec)),type="o", col = "green")
    lines(rowMeans(as.matrix(ES_CV_Error.TrainError.Vec)),type="o", col = "Light green")
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2--(SL_L2_Error)------------------
    lines(as.matrix((SL_L2.TestError.Mat[2:NROW(SL_L2.TestError.Mat)])),type="o", col = "black")
    lines(as.matrix(SL_L2.TrainError.Mat[2:NROW(SL_L2.TrainError.Mat)]),type="o", col = "gray")
    #------------------------------------------------------------------------
    
    #-----------------------LMSquareLossL2penalties--(SL_L2_Pen)-------------
    lines(as.matrix((SL_L2_Pen.TestError.Mat[2:NROW(SL_L2_Pen.TestError.Mat)])),type="o", col = "red")
    lines(as.matrix((SL_L2_Pen.TrainError.Mat[2:NROW(SL_L2_Pen.TrainError.Mat)])),type="o", col = "pink")
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
    lines(as.matrix((SL_L2CV.TestError.Mat[2:NROW(SL_L2CV.TestError.Mat)])),type="o", col = "orange")
    lines(as.matrix((SL_L2CV.TrainError.Mat[2:NROW(SL_L2CV.TrainError.Mat)])),type="o", col = "gold")
    #------------------------------------------------------------------------
  #------------------------------------------------------------------------
```
### Extra Credit attempt: compaire The LinearModels to the KNN
```{r}
  #--------------------NN1ToKmaxPredict------------------------------------
  Prostate<-Prep_Prostate()
  Fold.vec = Random_Folds(Prostate$n_Elements,4)
  Fold.n   = 4
  KNNLearnCV.List = KNNLearnCV(Prostate$TrainingData, Prostate$TrainingLabels, 30, Fold.vec, Fold.n)

  barplot(KNNLearnCV.List$TestMeanError.Means,main = "Prostate: KNNLearnCV L2 Mean Error",xlab = "KNN Compared",ylab = "Error",beside = TRUE)
  #------------------------------------------------------------------------
```



## Data set 5: Ozone
```{r}
  OZone<-Prep_Ozone()
```
### Code 
```{r}
  #-------------------- Data set 5: Ozone ----------------------------------
  LossMat.fold.n  <- 4
  LossMat.fold.vec<-Random_Folds((OZone$n_Elements),LossMat.fold.n)
  Data = cbind(OZone$TrainingData ,OZone$TrainingLabels)
  #Randomly shuffle the data
  #Data<-Data[sample(nrow(Data)),]

  SLI.WholeError.Mat = 0
  SLI.TrainError.Mat = 0
  SLI.TestError.Mat  = 0
  
  ES_CV_Error.WholeError.Mat = 0
  ES_CV_Error.TrainError.Mat = 0
  ES_CV_Error.TestError.Mat  = 0
  
  SL_L2.WholeError.Mat = 0
  SL_L2.TrainError.Mat = 0
  SL_L2.TestError.Mat  = 0
  
  SL_L2_Pen.WholeError.Mat = 0
  SL_L2_Pen.TrainError.Mat = 0
  SL_L2_Pen.TestError.Mat  = 0
  
  SL_L2CV.WholeError.Mat = 0
  SL_L2CV.TrainError.Mat = 0
  SL_L2CV.TestError.Mat  = 0
  #Perform folds.n fold cross validation
  for(i in 1:(LossMat.fold.n))
  {
    testIndexes <- which(LossMat.fold.vec==i,arr.ind=TRUE)
    testData    <- as.matrix(Data[testIndexes, ])
    trainData   <- as.matrix(Data[-testIndexes, ])
  
    print("Please wait this might take some time, Iteration:")
    print(i)
    

    #-------------------------LMSquareLossIterations (SLI_Error)-------------
    Scalar.Step = 0.1
    Iterations  = 30
    LMSquareLossIterations.W.Vec <- LMSquareLossIterations(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],Iterations,Scalar.Step)
    SLI.WholeError.Vec <-Find_Wmatrix_MeanL2Error(OZone$TrainingData, OZone$TrainingLabels,as.matrix(LMSquareLossIterations.W.Vec),OZone$BinaryClassification)
    print("dim SLI.WHoleError")
    print(dim(as.matrix(SLI.WholeError.Vec)))
    SLI.WholeError.Mat <- cbind(SLI.WholeError.Mat,SLI.WholeError.Vec)

    SLI.TrainError.Vec <-Find_Wmatrix_MeanL2Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],as.matrix(LMSquareLossIterations.W.Vec),OZone$BinaryClassification)
    SLI.TrainError.Mat <- cbind(SLI.TrainError.Mat,SLI.TrainError.Vec)

    SLI.TestError.Vec  <-Find_Wmatrix_MeanL2Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],as.matrix(LMSquareLossIterations.W.Vec),OZone$BinaryClassification)
   SLI.TestError.Mat<- cbind(SLI.TestError.Mat,SLI.TestError.Vec)
    #------------------------------------------------------------------------

    #--------------------LMSquareLossEarlyStoppingCV (ES_CV_Error)-----------
    ES_CV_Error.fold.n  <- 4
    ES_CV_Error.fold.vec<-Random_Folds(NROW(trainData),ES_CV_Error.fold.n)
    Scalar.Step = 0.1
    Iterations  = 30
    #ES_CV_Error.W.Vec <- LMSquareLossEarlyStoppingCV(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],OZone$Folds.Vec,OZone$Folds.n,OZone$Iterations)
    ES_CV_Error.List <- LMSquareLossEarlyStoppingCV(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],ES_CV_Error.fold.vec,ES_CV_Error.fold.n,Iterations)
    
    ES_CV_Error.WholeError.Vec   <-Find_Wmatrix_MeanL2Error(OZone$TrainingData, OZone$TrainingLabels,as.matrix(ES_CV_Error.List$w.mat),OZone$BinaryClassification)
    #print("dim ES_CV_Error.WholeError.Vec")
    #print(dim(as.matrix(ES_CV_Error.WholeError.Vec)))
    #ES_CV_Error.WholeError.Mat <- cbind(ES_CV_Error.WholeError.Mat,ES_CV_Error.WholeError.Vec)

    ES_CV_Error.TrainError.Vec <-Find_Wmatrix_MeanL2Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],as.matrix(ES_CV_Error.List$w.mat),OZone$BinaryClassification)
    #ES_CV_Error.TrainError.Mat <- cbind(ES_CV_Error.TrainError.Mat,ES_CV_Error.TrainError.Vec)

    ES_CV_Error.TestError.Vec  <-Find_Wmatrix_MeanL2Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],as.matrix(ES_CV_Error.List$w.mat),OZone$BinaryClassification)
    #ES_CV_Error.TestError.Mat  <- cbind(ES_CV_Error.TestError.Mat,ES_CV_Error.TestError.Vec)
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2--(SL_L2_Error)------------------
    Normalized_TrainingData_List <- NormalizeMatrix_List(trainData[,1:NCOL(trainData)-1])
    Penalty.Scalar = 1
    opt.thresh = .2

    SL_L2.W.Matrix <-LMSquareLossL2(Normalized_TrainingData_List$NormalizedMatrix, trainData[,NCOL(trainData)], Penalty.Scalar, opt.thresh,OZone$Initial.Vector)
    DeNormalized.W.Matrix <-(t(SL_L2.W.Matrix))/Normalized_TrainingData_List$sd

    #DeNorm.Error <-Find_Wmatrix_MeanL2Error(OZone$TrainingData, OZone$TrainingLabels,t(DeNormalized.W.Matrix),OZone$BinaryClassification)
    SL_L2.WholeError.Vec <-Find_Wmatrix_MeanL1Error(OZone$TrainingData, OZone$TrainingLabels,t(DeNormalized.W.Matrix),OZone$BinaryClassification)
    SL_L2.WholeError.Mat <- rbind(SL_L2.WholeError.Mat,SL_L2.WholeError.Vec)
    
    SL_L2.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],t(DeNormalized.W.Matrix),OZone$BinaryClassification)
    SL_L2.TestError.Mat <- rbind(SL_L2.TestError.Mat,SL_L2.TestError.Vec)
    
    SL_L2.TrainError.Vec <-Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],t(DeNormalized.W.Matrix),OZone$BinaryClassification)
    SL_L2.TrainError.Mat <- rbind(SL_L2.TrainError.Mat,SL_L2.TrainError.Vec)
    #------------------------------------------------------------------------
    
    #-----------------------LMSquareLossL2penalties--(SL_L2_Pen)-------------
    Penalty.Scalar = 1
    opt.thresh = .2
    SL_L2_Pen.W.Mat <-LMSquareLossL2penalties(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)], OZone$Penalty.Vector)

    #DeNorm.Error <-Find_Wmatrix_MeanL2Error(OZone$TrainingData, OZone$TrainingLabels,t(DeNormalized.W.Matrix),OZone$BinaryClassification)
    print("SL_L2_Pen.WholeError.Vec")
    SL_L2_Pen.WholeError.Vec <-Find_Wmatrix_MeanL1Error(OZone$TrainingData, OZone$TrainingLabels,(SL_L2_Pen.W.Mat),OZone$BinaryClassification)
    SL_L2_Pen.WholeError.Mat <- rbind(SL_L2_Pen.WholeError.Mat,SL_L2_Pen.WholeError.Vec)
    
    print("SL_L2_Pen.TestError.Vec")
    SL_L2_Pen.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],(SL_L2_Pen.W.Mat),OZone$BinaryClassification)
    SL_L2_Pen.TestError.Mat <- rbind(SL_L2_Pen.TestError.Mat,SL_L2_Pen.TestError.Vec)
    
    print("SL_L2_Pen.TrainError.Vec")
    SL_L2_Pen.TrainError.Vec <- Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],(SL_L2_Pen.W.Mat),OZone$BinaryClassification)
    SL_L2_Pen.TrainError.Mat <- rbind(SL_L2_Pen.TrainError.Mat,SL_L2_Pen.TrainError.Vec)
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
      SL_L2CV.fold.n  <- 4
      SL_L2CV.fold.vec<-Random_Folds(NROW(trainData),SL_L2CV.fold.n)
      
      SL_L2CV.List <- LMSquareLossL2CV(as.matrix(trainData[,1:NCOL(trainData)-1]), as.matrix(trainData[,NCOL(trainData)]),as.matrix(t(SL_L2CV.fold.vec)), as.matrix(OZone$Penalty.Vector))
      

      SL_L2CV.WholeError.Vec <- Find_Wmatrix_MeanL1Error(OZone$TrainingData, OZone$TrainingLabels,(SL_L2CV.List$weight.vec),OZone$BinaryClassification)
      SL_L2CV.WholeError.Mat <- rbind(SL_L2CV.WholeError.Mat,SL_L2CV.WholeError.Vec)
      
      
      #SL_L2CV.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],t(SL_L2CV.W.Mat),OZone$BinaryClassification)
      SL_L2CV.TestError.Vec <- SL_L2CV.List$mean.validation.loss
      SL_L2CV.TestError.Mat <- rbind(SL_L2CV.TestError.Mat,SL_L2CV.TestError.Vec)
      print("SL_L2CV.TestError.Vec")
      print(dim(as.matrix(SL_L2CV.TestError.Vec)))
      
      #SL_L2CV.TrainError.Vec <- Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],t(SL_L2CV.W.Mat),OZone$BinaryClassification)
      SL_L2CV.TrainError.Vec <- SL_L2CV.List$mean.train.loss.vec
      SL_L2CV.TrainError.Mat <- rbind(SL_L2CV.TrainError.Mat,SL_L2CV.TrainError.Vec)
      print("SL_L2CV.TrainError.Vec")
      print(dim(as.matrix(SL_L2CV.TrainError.Vec)))
    #------------------------------------------------------------------------
  }

  #------------------------------------------------------------------------
```

### Matrix of loss values
### Train/validation loss plot
```{r}
  #-------------------- Data set 5: Ozone -----------------------------
    #-------------------------LMSquareLossIterations (SLI_Error)--------------
    plot(rowMeans(as.matrix(SLI.TrainError.Mat)),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "SAheart Loss of each matrix",ylim=c(1900 ,5000))
    lines(rowMeans(as.matrix(SLI.TestError.Mat)),type="o", col = "light blue")
    #------------------------------------------------------------------------
    
    #--------------------LMSquareLossEarlyStoppingCV (ES_CV_Error)-----------
    lines(rowMeans(as.matrix(ES_CV_Error.TestError.Vec)),type="o", col = "green")
    lines(rowMeans(as.matrix(ES_CV_Error.TrainError.Vec)),type="o", col = "Light green")
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2--(SL_L2_Error)------------------
    lines(as.matrix((SL_L2.TestError.Mat[2:NROW(SL_L2.TestError.Mat)])),type="o", col = "black")
    lines(as.matrix(SL_L2.TrainError.Mat[2:NROW(SL_L2.TrainError.Mat)]),type="o", col = "gray")
    #------------------------------------------------------------------------
    
    #-----------------------LMSquareLossL2penalties--(SL_L2_Pen)-------------
    lines(as.matrix((SL_L2_Pen.TestError.Mat[2:NROW(SL_L2_Pen.TestError.Mat)])),type="o", col = "red")
    lines(as.matrix((SL_L2_Pen.TrainError.Mat[2:NROW(SL_L2_Pen.TrainError.Mat)])),type="o", col = "pink")
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
    lines(as.matrix((SL_L2CV.TestError.Mat[2:NROW(SL_L2CV.TestError.Mat)])),type="o", col = "orange")
    lines(as.matrix((SL_L2CV.TrainError.Mat[2:NROW(SL_L2CV.TrainError.Mat)])),type="o", col = "gold")
    #------------------------------------------------------------------------
  #------------------------------------------------------------------------
```

### Extra Credit attempt: compaire The LinearModels to the KNN
```{r}
  #--------------------NN1ToKmaxPredict------------------------------------
  Ozone<-Prep_Ozone()
  print("NN1ToKmax_Ozone_Tests: ")
  Fold.vec = Random_Folds(Ozone$n_Elements,4)
  Fold.n   = 4
  KNNLearnCV.List = KNNLearnCV(Ozone$TrainingData, Ozone$TrainingLabels, 30, Fold.vec, Fold.n)

  barplot(KNNLearnCV.List$TestMeanError.Means,main = "Ozone: KNNLearnCV L2 Mean Error",xlab = "KNN Compared",ylab = "Error",beside = TRUE)
  #------------------------------------------------------------------------
```

