---
title: "Project 3: Single Layer Neural Network"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Report for project 1 on K Nearest Neighbors}
  %\VignetteEngine{knitr::rmarkdown}
  %\usepackage[UTF-8]{inputenc}
  %\VignetteEncoding{UTF-8}
  
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
```{r}
library(NeuralNetwork1)
print(getwd())
#When knitting document, have working directory as the document directory.
source("../tests/testthat/Prep_Libraries.R")
source("../R/General.R")
knitr::opts_chunk$set(fig.width=12, fig.height=8) 
```

## Data set 1: spam
### Code
Note, we some of the graphs are using some 'Prep' functions made to make managing the code easier, and more Efficient, and less repetitive.


```{r}
Spam<-Prep_Spam()
```



### Matrix of loss values 
NNItterations_Spam_LossMatrix:

```{r, results='hide'}

folds.n = 4
Scalar.Step = 0.4
max.iterations = 100L
fold.vec = as.double(sample(1:(folds.n),Spam$n_Elements,replace=T))
is.train = rep(TRUE,length(Spam$n_Elements))
PredMat.ByFold = c()
for(fold.number in 1:folds.n){

  is.train[which(fold.vec == fold.number)] = FALSE
  is.train[which(fold.vec != fold.number)] = TRUE
  #X.scaled.mat = scale(X.train,center = TRUE,scale = TRUE)
  #
  train.index = which(is.train==TRUE)
  validation.index = which(is.train!=TRUE)
  X.train = Spam$TrainingData[train.index,]
  y.train = Spam$TrainingLabels[train.index]
  X.validation = Spam$TrainingData[validation.index,]
  y.validation = Spam$TrainingLabels[validation.index]
  
  return.list <-NNetIterations(Spam$TrainingData, Spam$TrainingLabels,max.iterations,Scalar.Step,10,is.train)
  PredMat.ByFold<-cbind(PredMat.ByFold,colMeans(return.list$pred.mat))
}
  return.list = NNetEarlyStoppingCV(Spam$TrainingData, Spam$TrainingLabels,fold.vec,max.iterations,Scalar.Step,10,n.folds = 4)
```
```{r, results='hide'}
  print(dim(PredMat.ByFold))
  print(as.double(PredMat.ByFold[1,]))
  print(as.double(PredMat.ByFold[NROW(PredMat.ByFold),]))
  Spam_LossMatrix <-rbind(as.double(PredMat.ByFold[1,]),PredMat.ByFold[NROW(PredMat.ByFold),])
  Spam_LossMatrix <-rbind(Spam_LossMatrix,as.double(PredMat.ByFold[return.list$selected.steps,]))
  #as.double(PredMat.ByFold[NROW(PredMat.ByFold),2])
  
  colnames(Spam_LossMatrix)<-c("Fold1","Fold2","Fold3","Fold4")
  rownames(Spam_LossMatrix)<-c("Untrained","Trained","Early Stopping")
  barplot(Spam_LossMatrix, xlab = "Folds", ylab = "Error",main = "NNItterations_Spam_LossMatrix",legend = (rownames(Spam_LossMatrix)),beside = TRUE)
```

Here is the Loss graph for Spam. For each fold the graph shows that the error was relatively constant however the untrained data was significantly higher in each case. The trained and early stopping data were both almost exactly the same in each fold signifying that neither was better than the other in terms of loss. 

### Train/validation loss plot
```{r, results='hide'}
print("NNetIterations_Spam_Tests: Question:1")
Spam<-Prep_Spam()

Scalar.Step = 0.4
max.iterations = 100L

folds.vec = as.logical(Random_Folds(Spam$n_Elements,1))
length(folds.vec)=Spam$n_Elements

List <-NNetIterations(Spam$TrainingData, Spam$TrainingLabels,max.iterations,Scalar.Step,10,folds.vec)
```


```{r, results='hide'}
train.index = which(folds.vec==TRUE)
validation.index = which(folds.vec!=TRUE)
train.pred.mat = as.matrix(List$pred.mat[train.index,])

#print(List$pred.mat[validation.index])
validation.pred.mat = as.matrix(List$pred.mat[validation.index,])

names(validation.pred.mat)="Validation Error"
plot(as.matrix(colMeans(validation.pred.mat)),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "NNItterations: Spam Loss of each matrix")
lines(colMeans(as.matrix(train.pred.mat)),type="o", col = "red")
```
For the above graph "NNItterations: Spam Loss", the Blue line is the validation set error, and Redline is the Test set error. 
You can start to see the divergence of Validation and Training data, as the model continues to become more and more “over fit” for the training data.

NNetEarlyStoppingCV_Spam_Tests

```{r, results='hide'}
print("NNetEarlyStoppingCV_Spam_Tests:")
Scalar.Step = .99
folds.n = 4L
max.iterations = 100L
n.hidden.units = 10
folds.vec=as.double(sample(1:(folds.n),Spam$n_Elements,replace=T))
length(folds.vec)=Spam$n_Elements


NNetEarlyStoppingList<-NNetEarlyStoppingCV(Spam$TrainingData, Spam$TrainingLabels,folds.vec,max.iterations,Scalar.Step,n.hidden.units,folds.n)
```

```{r}
plot(as.matrix(NNetEarlyStoppingList$mean.train.loss.vec),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "NNetEarlyStoppingCV: Spam Loss of each matrix",ylim=c(.1,1))
lines((NNetEarlyStoppingList$mean.validation.loss.vec),type="o", col = "gray")

dot.x <- NNetEarlyStoppingList$selected.steps
dot.y <- NNetEarlyStoppingList$mean.validation.loss.vec[dot.x]

print(dot.x)
print(dot.y)
matpoints(x = dot.x,
          y = dot.y,
          col = 2,
          pch = 19)
```
For "NNetEarlyStoppingCV: Spam Loss", we can see that the training and test data are diverging here as well causing the overfit graph we see.


## Data set 2: SAheart
### Code
```{r}
  SAheart<-Prep_SAheart()
```


### Matrix of loss values
NNItterations_SAheart_LossMatrix:

```{r, results='hide'}
folds.n = 4
max.iterations = 100L

fold.vec = as.double(sample(1:(folds.n),SAheart$n_Elements,replace=T))
is.train = rep(TRUE,length(SAheart$n_Elements))
PredMat.ByFold = c()
for(fold.number in 1:folds.n){

  is.train[which(fold.vec == fold.number)] = FALSE
  is.train[which(fold.vec != fold.number)] = TRUE
  #X.scaled.mat = scale(X.train,center = TRUE,scale = TRUE)
  #
  train.index = which(is.train==TRUE)
  validation.index = which(is.train!=TRUE)
  X.train = SAheart$TrainingData[train.index,]
  y.train = SAheart$TrainingLabels[train.index]
  X.validation = SAheart$TrainingData[validation.index,]
  y.validation = SAheart$TrainingLabels[validation.index]
  
  return.list <-NNetIterations(SAheart$TrainingData, SAheart$TrainingLabels,max.iterations,Scalar.Step,10,is.train)
  PredMat.ByFold<-cbind(PredMat.ByFold,colMeans(return.list$pred.mat))
}
return.list = NNetEarlyStoppingCV(SAheart$TrainingData, SAheart$TrainingLabels,fold.vec,max.iterations,Scalar.Step,10,n.folds = 4)
```
```{r, results='hide'}
  print(dim(PredMat.ByFold))
  print(as.double(PredMat.ByFold[1,]))
  print(as.double(PredMat.ByFold[NROW(PredMat.ByFold),]))
  SAheart_LossMatrix <-rbind(as.double(PredMat.ByFold[1,]),PredMat.ByFold[NROW(PredMat.ByFold),])
  SAheart_LossMatrix <-rbind(SAheart_LossMatrix,as.double(PredMat.ByFold[return.list$selected.steps,]))
  #as.double(PredMat.ByFold[NROW(PredMat.ByFold),2])
  
  colnames(SAheart_LossMatrix)<-c("Fold1","Fold2","Fold3","Fold4")
  rownames(SAheart_LossMatrix)<-c("Untrained","Trained","Early Stopping")
  barplot(SAheart_LossMatrix, xlab = "Folds", ylab = "Error",main = "NNItterations_SAHeart_LossMatrix",legend = (rownames(SAheart_LossMatrix)),beside = TRUE)
```


### Train/validation loss plot
```{r, results='hide'}

Scalar.Step = 0.4
max.iterations = 100L
folds.vec = as.logical(Random_Folds(SAheart$n_Elements,1))
length(folds.vec)=SAheart$n_Elements

List <-NNetIterations(SAheart$TrainingData, SAheart$TrainingLabels,max.iterations,Scalar.Step,10,folds.vec)
```

```{r, results='hide'}
train.index = which(folds.vec==TRUE)
validation.index = which(folds.vec!=TRUE)
train.pred.mat = as.matrix(List$pred.mat[train.index,])

#print(List$pred.mat[validation.index])
validation.pred.mat = as.matrix(List$pred.mat[validation.index,])

plot(as.matrix(colMeans(validation.pred.mat)),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "NNItterations: SAheart Loss",ylim=c(.1,.9))
lines(colMeans(as.matrix(train.pred.mat)),type="o", col = "red")
```

For the above graph "NNIterations SAheart Loss", the Blue line is the validation set error, and Redline is the Test set error. As we can see here we have less overfitting than previously. 


NNetEarlyStoppingCV_Spam_Tests

```{r, results='hide'}
print("NNetEarlyStoppingCV_Spam_Tests:")
Scalar.Step = 0.5
folds.n = 4L
n.hidden.units=10
folds.vec=as.double(sample(1:(folds.n),SAheart$n_Elements,replace=T))
length(folds.vec)=SAheart$n_Elements


SAheart.NNetEarlyStoppingList<-NNetEarlyStoppingCV(SAheart$TrainingData, SAheart$TrainingLabels,folds.vec,100L,Scalar.Step,n.hidden.units,folds.n)
```

```{r}
plot(as.matrix(SAheart.NNetEarlyStoppingList$mean.train.loss.vec),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "NNetEarlyStoppingCV: SAheart Loss of each matrix",ylim=c(.1,1))
lines((SAheart.NNetEarlyStoppingList$mean.validation.loss.vec),type="o", col = "gray")

dot.x <- SAheart.NNetEarlyStoppingList$selected.steps
dot.y <- SAheart.NNetEarlyStoppingList$mean.validation.loss.vec[dot.x]

print(dot.x)
print(dot.y)
matpoints(x = dot.x,
          y = dot.y,
          col = 2,
          pch = 19)
```
Here we have the training data in blue and the validation data in grey. Just like in the previous graph we have less overfitting and a more just fit. 

## Data set 4: prostate
### Code
```{r}
  Prostate<-Prep_Prostate()
```

### Matrix of loss values
```{r, results='hide'}
folds.n = 4
Scalar.Step=.4
max.iterations = 100L

fold.vec = as.double(sample(1:(folds.n),Prostate$n_Elements,replace=T))
is.train = rep(TRUE,length(Prostate$n_Elements))
PredMat.ByFold = c()
for(fold.number in 1:folds.n){

  is.train[which(fold.vec == fold.number)] = FALSE
  is.train[which(fold.vec != fold.number)] = TRUE
  #X.scaled.mat = scale(X.train,center = TRUE,scale = TRUE)
  #
  train.index = which(is.train==TRUE)
  validation.index = which(is.train!=TRUE)
  X.train = Prostate$TrainingData[train.index,]
  y.train = Prostate$TrainingLabels[train.index]
  X.validation = Prostate$TrainingData[validation.index,]
  y.validation = Prostate$TrainingLabels[validation.index]
  
  return.list <-NNetIterations(Prostate$TrainingData, Prostate$TrainingLabels,max.iterations,Scalar.Step,10,is.train)
  PredMat.ByFold<-cbind(PredMat.ByFold,colMeans(return.list$pred.mat))
}

  return.list = NNetEarlyStoppingCV(Prostate$TrainingData, Prostate$TrainingLabels,fold.vec,max.iterations,Scalar.Step,10,n.folds = 4)
```


```{r, results='hide'}
print(return.list$selected.steps)
  print(as.matrix(return.list$mean.validation.loss.vec))
  print(dim(PredMat.ByFold))
  print(as.double(PredMat.ByFold[1,]))
  print(as.double(PredMat.ByFold[NROW(PredMat.ByFold),]))
  Prostate_LossMatrix = c()
  Prostate_LossMatrix <-rbind(as.double(PredMat.ByFold[1,]),PredMat.ByFold[NROW(PredMat.ByFold),])
  Prostate_LossMatrix <-rbind(Prostate_LossMatrix,as.double(PredMat.ByFold[return.list$selected.steps,]))
  #as.double(PredMat.ByFold[NROW(PredMat.ByFold),2])
  
  print(dim(Prostate_LossMatrix))
  
  colnames(Prostate_LossMatrix)<-c("Fold1","Fold2","Fold3","Fold4")
  rownames(Prostate_LossMatrix)<-c("Untrained","Trained","EarlyStopping")
  barplot(Prostate_LossMatrix, xlab = "Folds", ylab = "Error",main = "NNItterations_Prostate_LossMatrix",legend = (rownames(Prostate_LossMatrix)),beside = TRUE)
```

### Train/validation loss plot
```{r, results='hide'}
step.size = .5
n.hidden.units = 4
max.iterations = 100L
n.hidden.units=10
data(prostate, package = "ElemStatLearn")
prostate = list(dataset = as.matrix(
  prostate[, 1:8]),
  labels = prostate$lpsa)

  X.mat = prostate$dataset
  y.vec = prostate$labels
  fold.vec = fold.vec = sample(rep(1:4), length(y.vec),TRUE)


  return.list = NNetEarlyStoppingCV(X.mat, y.vec,fold.vec,max.iterations,
                                    step.size,n.hidden.units,n.folds = 4)

  mean.loss.train = return.list$mean.train.loss.vec

  mean.loss.validation = return.list$mean.validation.loss.vec

  select.step = return.list$selected.steps

  plot(c(1:max.iterations),mean.loss.validation,type="s",xlim = c(1,max.iterations), ylim=c(0,1),
       xlab="interations",ylab="mean.validation",
       col="red",main="loss value",pch=c(15))

  par(new=TRUE)

  plot(c(1:max.iterations),mean.loss.train,type="s",xlim = c(1,max.iterations), ylim=c(0,1),
       xlab="interations",ylab="mean.validation",
       col="blue",main="NNetEarlyStoppingCV Prostate",pch=c(15))
  dot.x <- select.step
dot.y <- return.list$mean.validation.loss.vec[dot.x]

print(dot.x)
print(dot.y)
matpoints(x = dot.x,
          y = dot.y,
          col = 2,
          pch = 19)
```
For the above graph "NNetEarlyStoppingCV Prostate", the Blue line is the validation set error, and Redline is the Test set error. With a step.size of : `r step.size`, and `r n.hidden.units` number of n.hidden.units, the optimal number of steps found was: `r return.list$selected.steps`. By tweaking these values, a more optimal solution might be possible.

## Data set 5: ozone
### Code
```{r}
  Ozone<-Prep_Ozone()
```


```{r}
  data(ozone, package = "ElemStatLearn")

step.size = .5
n.hidden.units = 5
  X.mat = as.matrix((ozone[,2:4]))
  y.vec = as.vector(ozone[,1])
  print(dim(X.mat))
  print(length(y.vec))
  fold.vec = fold.vec = sample(rep(1:4), length(y.vec),TRUE)



```

### Matrix of loss values
```{r, results='hide'}
folds.n = 4
Scalar.Step=.4
fold.vec = as.double(sample(1:(folds.n),Ozone$n_Elements,replace=T))
is.train = rep(TRUE,length(Ozone$n_Elements))
PredMat.ByFold = c()
for(fold.number in 1:folds.n){

  is.train[which(fold.vec == fold.number)] = FALSE
  is.train[which(fold.vec != fold.number)] = TRUE
  #X.scaled.mat = scale(X.train,center = TRUE,scale = TRUE)
  #
  train.index = which(is.train==TRUE)
  validation.index = which(is.train!=TRUE)
  X.train = Ozone$TrainingData[train.index,]
  y.train = Ozone$TrainingLabels[train.index]
  X.validation = Ozone$TrainingData[validation.index,]
  y.validation = Ozone$TrainingLabels[validation.index]
  
  return.list <-NNetIterations(Ozone$TrainingData, Ozone$TrainingLabels,100L,Scalar.Step,10,is.train)
  PredMat.ByFold<-cbind(PredMat.ByFold,colMeans(return.list$pred.mat))
}

  return.list = NNetEarlyStoppingCV(X.mat, y.vec,fold.vec,max.iterations,
                                    step.size,n.hidden.units,n.folds = 4)
```


```{r, results='hide'}
  Prostate_LossMatrix = c()
  Prostate_LossMatrix <-rbind(as.double(PredMat.ByFold[2,]),PredMat.ByFold[NROW(PredMat.ByFold),])
  Prostate_LossMatrix <-rbind(Prostate_LossMatrix,as.double(return.list$mean.validation.loss.vec[return.list$selected.steps]))
  #as.double(PredMat.ByFold[NROW(PredMat.ByFold),2])
  
  print(dim(Prostate_LossMatrix))
  
  colnames(Prostate_LossMatrix)<-c("Fold1","Fold2","Fold3","Fold4")
  rownames(Prostate_LossMatrix)<-c("Untrained","Trained","EarlyStopping")
  barplot(Prostate_LossMatrix, xlab = "Folds", ylab = "Error",main = "NNItterations_Ozone_LossMatrix",legend = (rownames(Prostate_LossMatrix)),beside = TRUE)
```


### Train/validation loss plot
```{r, results='hide'}

data(ozone, package = "ElemStatLearn")

step.size = .01
n.hidden.units = 5
max.iterations= 100L
  X.mat = as.matrix((ozone[,2:4]))
  y.vec = as.vector(ozone[,1])
  print(dim(X.mat))
  print(length(y.vec))
  fold.vec = fold.vec = sample(rep(1:4), length(y.vec),TRUE)


  return.list = NNetEarlyStoppingCV(X.mat, y.vec,fold.vec,max.iterations,
                                    step.size,n.hidden.units,n.folds = 4)

  mean.loss.train = return.list$mean.train.loss.vec

  mean.loss.validation = return.list$mean.validation.loss.vec

  select.step = return.list$selected.steps

  plot(c(1:max.iterations),return.list$mean.validation.loss.vec,type="s",xlim = c(1,max.iterations),
       xlab="interations",ylab="mean.validation",
       col="red",pch=c(15))

  par(new=TRUE)

  plot(c(1:max.iterations),return.list$mean.train.loss.vec,type="s",xlim = c(1,max.iterations),
       xlab="interations",ylab="mean.validation",
       col="blue",main="NNetEarlyStoppingCV Ozone",pch=c(15))
  
dot.x <- select.step
dot.y <- return.list$mean.validation.loss.vec[dot.x]

matpoints(x = dot.x,
          y = dot.y,
          col = 2)
```
For the above graph "NNetEarlyStoppingCV Ozone", the Blue line is the validation set error, and Redline is the Test set error. 
With a step.size of : `r step.size`
,and
`r n.hidden.units` number of n.hidden.units, the optimal number of steps found was: 
`r return.list$selected.steps` . by tweaking these values, a more optimal solution might be possible.
