---
title: "Project 3: Single Layer Neural Network"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Report for project 1 on K Nearest Neighbors}
  %\VignetteEngine{knitr::rmarkdown}
  %\usepackage[UTF-8]{inputenc}
  %\VignetteEncoding{UTF-8}
  
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
```{r}
library(NeuralNetwork1)
print(getwd())
#When knitting document, have working directory as the document directory.
source("../tests/testthat/Prep_Libraries.R")
source("../R/General.R")
knitr::opts_chunk$set(fig.width=12, fig.height=8) 
```

## Data set 1: spam
### Code
Note, we some of the graphs are using some 'Prep' functions made to make managing the code easier, and more Efficient, and less repetitive.


```{r}
Spam<-Prep_Spam()
```



### Matrix of loss values 
NNItterations_Spam_LossMatrix:

```{r, results='hide'}

folds.n = 4
Scalar.Step = 0.4
max.iterations = 3000L
fold.vec = as.double(sample(1:(folds.n),Spam$n_Elements,replace=T))
is.train = rep(TRUE,length(Spam$n_Elements))
PredMat.ByFold = c()
for(fold.number in 1:folds.n){

  is.train[which(fold.vec == fold.number)] = FALSE
  is.train[which(fold.vec != fold.number)] = TRUE
  #X.scaled.mat = scale(X.train,center = TRUE,scale = TRUE)
  #
  train.index = which(is.train==TRUE)
  validation.index = which(is.train!=TRUE)
  X.train = Spam$TrainingData[train.index,]
  y.train = Spam$TrainingLabels[train.index]
  X.validation = Spam$TrainingData[validation.index,]
  y.validation = Spam$TrainingLabels[validation.index]
  
  return.list <-NNetIterations(Spam$TrainingData, Spam$TrainingLabels,max.iterations,Scalar.Step,10,is.train)
  PredMat.ByFold<-cbind(PredMat.ByFold,colMeans(return.list$pred.mat))
}
  return.list = NNetEarlyStoppingCV(Spam$TrainingData, Spam$TrainingLabels,fold.vec,max.iterations,Scalar.Step,10,n.folds = 4)
```
```{r, results='hide'}
  print(dim(PredMat.ByFold))
  print(as.double(PredMat.ByFold[1,]))
  print(as.double(PredMat.ByFold[NROW(PredMat.ByFold),]))
  Spam_LossMatrix <-rbind(as.double(PredMat.ByFold[1,]),PredMat.ByFold[NROW(PredMat.ByFold),])
  Spam_LossMatrix <-rbind(Spam_LossMatrix,as.double(PredMat.ByFold[return.list$selected.steps,]))
  #as.double(PredMat.ByFold[NROW(PredMat.ByFold),2])
  
  colnames(Spam_LossMatrix)<-c("Fold1","Fold2","Fold3","Fold4")
  rownames(Spam_LossMatrix)<-c("Untrained","Trained","Early Stopping")
  barplot(Spam_LossMatrix, xlab = "Folds", ylab = "Error",main = "NNItterations_Spam_LossMatrix",legend = (rownames(Spam_LossMatrix)),beside = TRUE)
```

Here is the Loss graph for Spam. The Trained and early stopping are the same because the best  resulting 

### Train/validation loss plot
```{r, results='hide'}
print("NNetIterations_Spam_Tests: Question:1")
Spam<-Prep_Spam()

Scalar.Step = 0.4
max.iterations = 1000L
folds.vec = as.logical(Random_Folds(Spam$n_Elements,1))
length(folds.vec)=Spam$n_Elements

List <-NNetIterations(Spam$TrainingData, Spam$TrainingLabels,max.iterations,Scalar.Step,10,folds.vec)
```


```{r, results='hide'}
train.index = which(folds.vec==TRUE)
validation.index = which(folds.vec!=TRUE)
train.pred.mat = as.matrix(List$pred.mat[train.index,])

#print(List$pred.mat[validation.index])
validation.pred.mat = as.matrix(List$pred.mat[validation.index,])

names(validation.pred.mat)="Validation Error"
plot(as.matrix(colMeans(validation.pred.mat)),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "NNItterations: Spam Loss of each matrix")
lines(colMeans(as.matrix(train.pred.mat)),type="o", col = "red")
```
For the graph NNItterations:Spam Loss of each matrix, the Blue line is the validation set error, and Redline is the Test set error.


NNetEarlyStoppingCV_Spam_Tests

```{r, results='hide'}
print("NNetEarlyStoppingCV_Spam_Tests:")
Scalar.Step = 0.5
folds.n = 4L
max.iterations = 1000L
folds.vec=as.double(sample(1:(folds.n),Spam$n_Elements,replace=T))
length(folds.vec)=Spam$n_Elements


NNetEarlyStoppingList<-NNetEarlyStoppingCV(Spam$TrainingData, Spam$TrainingLabels,folds.vec,max.iterations,Scalar.Step,10,folds.n)
```

```{r}
plot(as.matrix(NNetEarlyStoppingList$mean.train.loss.vec),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "NNetEarlyStoppingCV: Spam Loss of each matrix",ylim=c(.1,1))
lines((NNetEarlyStoppingList$mean.validation.loss.vec),type="o", col = "gray")

dot.x <- NNetEarlyStoppingList$selected.steps
dot.y <- NNetEarlyStoppingList$mean.validation.loss.vec[dot.x]

print(dot.x)
print(dot.y)
matpoints(x = dot.x,
          y = dot.y,
          col = 2,
          pch = 19)
```
The optimal number of steps is: **`r NNetEarlyStoppingList$selected.steps`**

## Data set 2: SAheart
### Code
```{r}
  SAheart<-Prep_SAheart()
```


### Matrix of loss values
NNItterations_SAheart_LossMatrix:

```{r, results='hide'}
folds.n = 4
max.iterations = 1000L

fold.vec = as.double(sample(1:(folds.n),SAheart$n_Elements,replace=T))
is.train = rep(TRUE,length(SAheart$n_Elements))
PredMat.ByFold = c()
for(fold.number in 1:folds.n){

  is.train[which(fold.vec == fold.number)] = FALSE
  is.train[which(fold.vec != fold.number)] = TRUE
  #X.scaled.mat = scale(X.train,center = TRUE,scale = TRUE)
  #
  train.index = which(is.train==TRUE)
  validation.index = which(is.train!=TRUE)
  X.train = SAheart$TrainingData[train.index,]
  y.train = SAheart$TrainingLabels[train.index]
  X.validation = SAheart$TrainingData[validation.index,]
  y.validation = SAheart$TrainingLabels[validation.index]
  
  return.list <-NNetIterations(SAheart$TrainingData, SAheart$TrainingLabels,max.iterations,Scalar.Step,10,is.train)
  PredMat.ByFold<-cbind(PredMat.ByFold,colMeans(return.list$pred.mat))
}
return.list = NNetEarlyStoppingCV(SAheart$TrainingData, SAheart$TrainingLabels,fold.vec,max.iterations,Scalar.Step,10,n.folds = 4)
```
```{r, results='hide'}
  print(dim(PredMat.ByFold))
  print(as.double(PredMat.ByFold[1,]))
  print(as.double(PredMat.ByFold[NROW(PredMat.ByFold),]))
  SAheart_LossMatrix <-rbind(as.double(PredMat.ByFold[1,]),PredMat.ByFold[NROW(PredMat.ByFold),])
  SAheart_LossMatrix <-rbind(SAheart_LossMatrix,as.double(PredMat.ByFold[return.list$selected.steps,]))
  #as.double(PredMat.ByFold[NROW(PredMat.ByFold),2])
  
  colnames(SAheart_LossMatrix)<-c("Fold1","Fold2","Fold3","Fold4")
  rownames(SAheart_LossMatrix)<-c("Untrained","Trained","Early Stopping")
  barplot(SAheart_LossMatrix, xlab = "Folds", ylab = "Error",main = "NNItterations_SAHeart_LossMatrix",legend = (rownames(SAheart_LossMatrix)),beside = TRUE)
```


### Train/validation loss plot
```{r, results='hide'}

Scalar.Step = 0.4
max.iterations = 1000L
folds.vec = as.logical(Random_Folds(SAheart$n_Elements,1))
length(folds.vec)=SAheart$n_Elements

List <-NNetIterations(SAheart$TrainingData, SAheart$TrainingLabels,max.iterations,Scalar.Step,10,folds.vec)
```

```{r, results='hide'}
train.index = which(folds.vec==TRUE)
validation.index = which(folds.vec!=TRUE)
train.pred.mat = as.matrix(List$pred.mat[train.index,])

#print(List$pred.mat[validation.index])
validation.pred.mat = as.matrix(List$pred.mat[validation.index,])

plot(as.matrix(colMeans(validation.pred.mat)),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "NNItterations: SAheart Loss of each matrix",ylim=c(.1,.9))
lines(colMeans(as.matrix(train.pred.mat)),type="o", col = "red")
```



NNetEarlyStoppingCV_Spam_Tests

```{r, results='hide'}
print("NNetEarlyStoppingCV_Spam_Tests:")
Scalar.Step = 0.5
folds.n = 4L
folds.vec=as.double(sample(1:(folds.n),SAheart$n_Elements,replace=T))
length(folds.vec)=SAheart$n_Elements


SAheart.NNetEarlyStoppingList<-NNetEarlyStoppingCV(SAheart$TrainingData, SAheart$TrainingLabels,folds.vec,100L,Scalar.Step,10,folds.n)
```

```{r}
plot(as.matrix(SAheart.NNetEarlyStoppingList$mean.train.loss.vec),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "NNetEarlyStoppingCV: SAheart Loss of each matrix",ylim=c(.1,1))
lines((SAheart.NNetEarlyStoppingList$mean.validation.loss.vec),type="o", col = "gray")

dot.x <- SAheart.NNetEarlyStoppingList$selected.steps
dot.y <- SAheart.NNetEarlyStoppingList$mean.validation.loss.vec[dot.x]

print(dot.x)
print(dot.y)
matpoints(x = dot.x,
          y = dot.y,
          col = 2,
          pch = 19)
```
The optimal number of steps is: **`r SAheart.NNetEarlyStoppingList$selected.steps`**


## Data set 4: prostate
### Code
```{r}
  Prostate<-Prep_Prostate()
```

### Matrix of loss values
```{r, results='hide'}
folds.n = 4
Scalar.Step=.4
max.iterations = 1000L

fold.vec = as.double(sample(1:(folds.n),Prostate$n_Elements,replace=T))
is.train = rep(TRUE,length(Prostate$n_Elements))
PredMat.ByFold = c()
for(fold.number in 1:folds.n){

  is.train[which(fold.vec == fold.number)] = FALSE
  is.train[which(fold.vec != fold.number)] = TRUE
  #X.scaled.mat = scale(X.train,center = TRUE,scale = TRUE)
  #
  train.index = which(is.train==TRUE)
  validation.index = which(is.train!=TRUE)
  X.train = Prostate$TrainingData[train.index,]
  y.train = Prostate$TrainingLabels[train.index]
  X.validation = Prostate$TrainingData[validation.index,]
  y.validation = Prostate$TrainingLabels[validation.index]
  
  return.list <-NNetIterations(Prostate$TrainingData, Prostate$TrainingLabels,max.iterations,Scalar.Step,10,is.train)
  PredMat.ByFold<-cbind(PredMat.ByFold,colMeans(return.list$pred.mat))
}

  return.list = NNetEarlyStoppingCV(Prostate$TrainingData, Prostate$TrainingLabels,fold.vec,max.iterations,Scalar.Step,10,n.folds = 4)
```


```{r, results='hide'}
print(return.list$selected.steps)
  print(as.matrix(return.list$mean.validation.loss.vec))
  print(dim(PredMat.ByFold))
  print(as.double(PredMat.ByFold[1,]))
  print(as.double(PredMat.ByFold[NROW(PredMat.ByFold),]))
  Prostate_LossMatrix = c()
  Prostate_LossMatrix <-rbind(as.double(PredMat.ByFold[1,]),PredMat.ByFold[NROW(PredMat.ByFold),])
  Prostate_LossMatrix <-rbind(Prostate_LossMatrix,as.double(PredMat.ByFold[return.list$selected.steps,]))
  #as.double(PredMat.ByFold[NROW(PredMat.ByFold),2])
  
  print(dim(Prostate_LossMatrix))
  
  colnames(Prostate_LossMatrix)<-c("Fold1","Fold2","Fold3","Fold4")
  rownames(Prostate_LossMatrix)<-c("Untrained","Trained","EarlyStopping")
  barplot(Prostate_LossMatrix, xlab = "Folds", ylab = "Error",main = "NNItterations_Prostate_LossMatrix",legend = (rownames(Prostate_LossMatrix)),beside = TRUE)
```

### Train/validation loss plot
```{r, results='hide'}
step.size = .5
n.hidden.units = 4
max.iterations = 1000L
data(prostate, package = "ElemStatLearn")
prostate = list(dataset = as.matrix(
  prostate[, 1:8]),
  labels = prostate$lpsa)

  X.mat = prostate$dataset
  y.vec = prostate$labels
  fold.vec = fold.vec = sample(rep(1:4), length(y.vec),TRUE)


  return.list = NNetEarlyStoppingCV(X.mat, y.vec,fold.vec,max.iterations,
                                    step.size,n.hidden.units,n.folds = 4)

  mean.loss.train = return.list$mean.train.loss.vec

  mean.loss.validation = return.list$mean.validation.loss.vec

  select.step = return.list$selected.steps

  plot(c(1:max.iterations),mean.loss.validation,type="s",xlim = c(1,max.iterations), ylim=c(0,1),
       xlab="interations",ylab="mean.validation",
       col="gray",main="loss value",pch=c(15))

  par(new=TRUE)

  plot(c(1:max.iterations),mean.loss.train,type="s",xlim = c(1,max.iterations), ylim=c(0,1),
       xlab="interations",ylab="mean.validation",
       col="blue",main="NNetEarlyStoppingCV Prostate results",pch=c(15))
  dot.x <- select.step
dot.y <- return.list$mean.validation.loss.vec[dot.x]

print(dot.x)
print(dot.y)
matpoints(x = dot.x,
          y = dot.y,
          col = 2,
          pch = 19)
```
The optimal number of steps is: **`r select.step`**
For the graph NNItterations:Spam Loss of each matrix, the Blue line is the validation set error, and Redline is the Test set error.

## Data set 5: ozone
### Code
```{r}
  Ozone<-Prep_Ozone()
```


```{r}
  data(ozone, package = "ElemStatLearn")

step.size = .5
n.hidden.units = 5
  X.mat = as.matrix((ozone[,2:4]))
  y.vec = as.vector(ozone[,1])
  print(dim(X.mat))
  print(length(y.vec))
  fold.vec = fold.vec = sample(rep(1:4), length(y.vec),TRUE)



```

### Matrix of loss values
```{r, results='hide'}
folds.n = 4
Scalar.Step=.4
fold.vec = as.double(sample(1:(folds.n),Ozone$n_Elements,replace=T))
is.train = rep(TRUE,length(Ozone$n_Elements))
PredMat.ByFold = c()
for(fold.number in 1:folds.n){

  is.train[which(fold.vec == fold.number)] = FALSE
  is.train[which(fold.vec != fold.number)] = TRUE
  #X.scaled.mat = scale(X.train,center = TRUE,scale = TRUE)
  #
  train.index = which(is.train==TRUE)
  validation.index = which(is.train!=TRUE)
  X.train = Ozone$TrainingData[train.index,]
  y.train = Ozone$TrainingLabels[train.index]
  X.validation = Ozone$TrainingData[validation.index,]
  y.validation = Ozone$TrainingLabels[validation.index]
  
  return.list <-NNetIterations(Ozone$TrainingData, Ozone$TrainingLabels,100L,Scalar.Step,10,is.train)
  PredMat.ByFold<-cbind(PredMat.ByFold,colMeans(return.list$pred.mat))
}

  return.list = NNetEarlyStoppingCV(X.mat, y.vec,fold.vec,max.iterations,
                                    step.size,n.hidden.units,n.folds = 4)
```


```{r, results='hide'}
  Prostate_LossMatrix = c()
  Prostate_LossMatrix <-rbind(as.double(PredMat.ByFold[2,]),PredMat.ByFold[NROW(PredMat.ByFold),])
  Prostate_LossMatrix <-rbind(Prostate_LossMatrix,as.double(return.list$mean.validation.loss.vec[return.list$selected.steps]))
  #as.double(PredMat.ByFold[NROW(PredMat.ByFold),2])
  
  print(dim(Prostate_LossMatrix))
  
  colnames(Prostate_LossMatrix)<-c("Fold1","Fold2","Fold3","Fold4")
  rownames(Prostate_LossMatrix)<-c("Untrained","Trained","EarlyStopping")
  barplot(Prostate_LossMatrix, xlab = "Folds", ylab = "Error",main = "NNItterations_Ozone_LossMatrix",legend = (rownames(Prostate_LossMatrix)),beside = TRUE)
```


### Train/validation loss plot
```{r, results='hide'}

data(ozone, package = "ElemStatLearn")

step.size = .5
n.hidden.units = 5
  X.mat = as.matrix((ozone[,2:4]))
  y.vec = as.vector(ozone[,1])
  print(dim(X.mat))
  print(length(y.vec))
  fold.vec = fold.vec = sample(rep(1:4), length(y.vec),TRUE)


  return.list = NNetEarlyStoppingCV(X.mat, y.vec,fold.vec,max.iterations,
                                    step.size,n.hidden.units,n.folds = 4)

  mean.loss.train = return.list$mean.train.loss.vec

  mean.loss.validation = return.list$mean.validation.loss.vec

  select.step = return.list$selected.steps

  plot(c(1:max.iterations),return.list$mean.validation.loss.vec,type="s",xlim = c(1,max.iterations),
       xlab="interations",ylab="mean.validation",
       col="gray",pch=c(15))

  par(new=TRUE)

  plot(c(1:max.iterations),return.list$mean.train.loss.vec,type="s",xlim = c(1,max.iterations),
       xlab="interations",ylab="mean.validation",
       col="blue",main="Ozone NNetEarlyStoppingCV",pch=c(15))
  
dot.x <- select.step
dot.y <- return.list$mean.validation.loss.vec[dot.x]

matpoints(x = dot.x,
          y = dot.y,
          col = 2)
```
The optimal number of steps is: **`r select.step`**
