---
title: "Linear Models Report"
author: "Anthony Schroeder,"
date: ""
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}

---
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

<a id="top"></a>

# Project 2 LM Vignette
### <authors>
#### NAU March 1, 2019

> [Introduction](#intro)
> [Installation](#install)
> [Steps to Run](#qs)
> [Linear Regression](#lin)
> [LMSquareLossIterations](#lin.SqrLoss)
> [LMSquareLossEarlyStoppingCV](#lin.SqrLossCV)
> [LMSquareLossL2](#lin.SqrLossL2)
> [LMSquareLossL2penalties](#lin.SqrLossPen)
> [LMSquareLossL2CV](#lin.SqrLossL2CV)
> [Logistic Regression](#log)
> [LMLogisticLossIterations](#log.LogLoss)
> [LMLogisticLossEarlyStoppingCV](#log.LogLossCV)
> [LMLogLossL2](#log.LogLossL2)
> [LMLogLossL2penalties](#log.LogLossPen)
> [LMLogLossL2CV](#log.LogLossL2CV)

<a id="intro"></a>

## Introduction

The second machine learning project for our class is to create Linear Learning Models using Logistic Loss for binary classification and Square Loss for other classification problems. We have to use what we discussed in class to code the linear regression and cross validation functions in order to minimize the loss. 

### Linear Regression


#### LMSquareLossIterations


**Fromula: ** **$\hat{y} = XW$**
Y.hat = data.matrix(TestingData) %*% data.matrix(W.Matrix)

**Gradient: ** **${W'} = 2X^T(X-y)^2$**
Gradient = 2*sum(W.Vector*TrainingData + TrainingLabels)*TrainingLabels

#### Linear L2 equations
**We are Looking for : ** **$\hat{W}^{Lambda} = argmin_{W\in R^{P}}$**

**Regularization Gradient: ** **${W'} = 2X^T(X-y)^2 + 2*(Labmda)*W$**







### Logistic equations

#### Logistic Iteration equations
**Fromula: ** **$\hat{y} = 1/(1+e^{XW})$**
Y.hat = 1/(1+exp(-TrainingLabels%*% data.matrix(TrainingData) %*% data.matrix(W.Vector)))

**Gradient: ** **${W'} = 1/(1+e^{-yXW})-y$**
Gradient = 2*sum(W.Vector*TrainingData + TrainingLabels)*TrainingLabels

#### Linear L2 equations
**We are Looking for : ** **$\hat{W}^{Lambda} = argmin_{W\in R^{P}}$**

**Regularization Gradient: ** **${W'} = 2X^T(X-y)^2 + 2*(Labmda)*W$**


## Testing With Data Sets 
we are going to run our code on the following data sets.




# Tests with ''Libraries to 




## Data set 1: spam

### Matrix of loss values

Here is an example of how to use the LMSquareLossIterations function:

//Currently having build issues, but code works within R.Studio 
```{r}
library(LinearModel)

# Data set 1: spam
#----------------------Data Initialization-------------------------------
  Folds <- 3
  MaxNeighbors <- 30
  Local_spam<- ElemStatLearn::spam

  BinaryClassification = 1
  Local_spam$spam <- sapply(as.character(Local_spam$spam),switch,"spam"=1,"email"=0)
  MaxSample_ofType = length(Local_spam[Local_spam$spam == 1,][,1])


  Spam <- data.frame(Local_spam[Local_spam$spam == 1,])
  print(NROW(Spam))


  email = Local_spam[0,]
  email <- head(Local_spam[Local_spam$spam == 0,],MaxSample_ofType)
  #print(NROW(email))

  Cliped<-rbind(Spam,email)
  Cliped<-Cliped[sample(nrow(Cliped)),]
  DataColsStart = 0
  DataColsEnd   = NCOL(Cliped) - 1
  LabelCol      = NCOL(Cliped)
  Rows          = NROW(Cliped)

  #Create New Fold Column to hold Fold Values
  fold.vec <- Random_Folds(Rows,Folds)
  
  TrainingLabels <- (Cliped[,LabelCol])
  TrainingData   <- (Cliped[,DataColsStart:DataColsEnd])
#------------------------------------------------------------------------
  
    
  
  
#Note on Functional Parameters: LMSquareLossIterations(
#  
#)
  
  # This Function Returns a Weights Matrix of each iteration between taking the gradient:
  # Gradient = 2*sum(W.Vector*TrainingData + TrainingLabels)*TrainingLabels
  
#--------------------LMSquareLossIterations------------------------
  #LMSquareLoss_Weights <- LMSquareLossIterations(TrainingLabels, TrainingData,20,0.1)
  DeNormalizedWeights <- LMSquareLossIterations(Cliped[,DataColsStart:DataColsEnd], Cliped[,LabelCol],20,0.1)

#------------------------------------------------------------------------
  
  
  #Note on Functional Parameters: LMSquareLossEarlyStoppingCV(
#  
#)
  
  # This Function Returns ...
  
  
#--------------------LMSquareLossEarlyStoppingCV------------------------
 # LMSquareLossES_Weights<- LMSquareLossEarlyStoppingCV(Cliped[,DataColsStart:DataColsEnd], Cliped[,LabelCol], fold.vec,Folds,30)
  
  ES.List <-LMSquareLossEarlyStoppingCV(Cliped[,DataColsStart:DataColsEnd], Cliped[,LabelCol], fold.vec,Folds,30)
  #print(ES.List)
  LMSquareLossES_Weights <- ES.List$w.mat
  #print(LMSquareLossES_Weights)

#------------------------------------------------------------------------
```


comment on difference in accuracy.
The Accuracy inttially improves drametically, but then deviates away from the optimal solution. 

### Train/validation loss plot

plot the two loss functions.


```{r}
#--------------------LMSquareLossIterations------------------------
  #Find the Error for each Iteration:
LMSquareLoss_Error <-Find_Wmatrix_MeanL2Error(Cliped[,DataColsStart:DataColsEnd], Cliped[,LabelCol],DeNormalizedWeights,BinaryClassification)

  #Here is a Matrix of the Error of each iteration of the weight Matrix:
  #print(LMSquareLoss_Error)
  
  #Here is a plot the weight Matrix found with LMSquareLossIterations:
barplot(LMSquareLoss_Error,main = "LMSquareLossIterations Vs Error \n 'Library'spam",xlab = "mean loss value",beside = TRUE)

  #print(DeNorm.Error)
  
#------------------------------------------------------------------------
  
  
  
#--------------------LMSquareLossEarlyStoppingCV------------------------
  #Find the Error for each Iteration:
  LMSquareLossES_Error <-Find_Wmatrix_MeanL2Error(Cliped[,DataColsStart:DataColsEnd], Cliped[,LabelCol],LMSquareLossES_Weights,BinaryClassification)

  #Here is a Matrix of the Error of each iteration of the weight Matrix:
  #print(LMSquareLossES_Error)
  
  #Here is a plot the weight Matrix found with LMSquareLossIterations:
  barplot(LMSquareLossES_Error,main = "EarlyStoping",xlab = "mean loss value",beside = TRUE)
#------------------------------------------------------------------------
```

For each data set, use 4-fold cross-validation to evaluate the prediction accuracy of your code. For each split s=1 to 4, set aside the data in fold s as a test set. Use ___CV to train a model on the other folds (which should be used in your ___CV function as internal train/validation sets/splits), then make a prediction on the test fold s.

For each train/test split, to show that your algorithm is actually learning something non-trivial from the inputs/features, compute a baseline predictor that ignores the inputs/features.
Regression: the mean of the training labels/outputs.
Binary classification: the most frequent class/label/output in the training data.


What are the optimal regularization parameters?

For each data set, compute a 4 x 3 matrix of mean test loss values:
each of the four rows are for a specific test set,
the first column is for the early stopping predictor,
the second column is for the L2 regularized predictor,
the third column is for the baseline/un-informed predictor.


## Error results table
| Test Set  | Early Stoping | L2 Regularized Predictor | Baseline (30 Iterations)(0.1 step)  | 
|:----------|:--------------|:-------------------------|:------------------------------------|
| 1         |               |                          |  0.3963045                          |  
| 2         |               |                          |                                     |  
| 3         |               |                          |                                     |  
| 4         |               |                          |                                     |  
| Avg.      |               |                          |                                     |



## Data set 2: 

### Matrix of loss values

Here is an example of how to use the LMSquareLossIterations function:
//Currently having build issues, but code works within R.Studio 
```{r}
library(LinearModel)

# Data set 2: SAheart_Test
#----------------------Data Initialization-------------------------------
  Folds <- 3
  MaxNeighbors <- 30
  Local_SAheart<- ElemStatLearn::SAheart


  DataColsStart = 0
  DataColsEnd   = length(Local_SAheart) - 1
  LabelCol      = length(Local_SAheart)
  Rows          = length(Local_SAheart[,1])

  #accesses first two Rows:
  #Local_SAheart[1:2,]
  famhist<-factor(c("Present" = 1, "Absent" = 0))

  #Local_SAheart[,5] <- as.integer(factor(Local_SAheart[,5],levels=c("Present" = 1, "Absent" = 2)))
  Local_SAheart[,5] <- sapply(as.character(Local_SAheart[,5]),switch,"Present"=1,"Absent"=0)
  #print(Local_SAheart)
  #Local_SAheart <-factor(Local_SAheart[,5])

  #Create New Fold Column to hold Fold Values
  Local_SAheart.Fold <- Random_Folds(length(Local_SAheart[,1]),Folds)



  #Create New Fold Column to hold Fold Values
  fold.vec <- Random_Folds(Rows,Folds)

#------------------------------------------------------------------------
  
    
  
  
#Note on Functional Parameters: LMSquareLossIterations(
#  
#)
  
  # This Function Returns a Weights Matrix of each iteration between taking the gradient:
  # Gradient = 2*sum(W.Vector*TrainingData + TrainingLabels)*TrainingLabels
  
#--------------------LMSquareLossIterations------------------------
  #LMSquareLoss_Weights <- LMSquareLossIterations(TrainingLabels, TrainingData,20,0.1)
  DeNormalizedWeights <- LMSquareLossIterations(Local_SAheart[,DataColsStart:DataColsEnd], Local_SAheart[,LabelCol],20,0.1)

LMSquareLossES_Error <-Find_Wmatrix_MeanL2Error(Cliped[,DataColsStart:DataColsEnd], Cliped[,LabelCol],DeNormalizedWeights,BinaryClassification)

barplot(LMSquareLossES_Error,main = "LMSquareLossIterations Vs Error \n 'Library'SA_Heart",xlab = "mean loss value",beside = TRUE)

#------------------------------------------------------------------------
  
  
  #Note on Functional Parameters: LMSquareLossEarlyStoppingCV(
#  
#)
  
  # This Function Returns ...
  
  
#--------------------LMSquareLossEarlyStoppingCV------------------------
 # LMSquareLossES_Weights<- LMSquareLossEarlyStoppingCV(Cliped[,DataColsStart:DataColsEnd], Cliped[,LabelCol], fold.vec,Folds,30)
  
#ES.List <-LMSquareLossEarlyStoppingCV(Cliped[,DataColsStart:DataColsEnd], Cliped[,LabelCol], fold.vec,Folds,30)
  #print(ES.List)
#LMSquareLossES_Weights <- ES.List$w.mat
  #print(LMSquareLossES_Weights)

#------------------------------------------------------------------------
```


comment on difference in accuracy.
The Accuracy inttially improves drametically, but then deviates away from the optimal solution. 

### Train/validation loss plot

plot the two loss functions.


```{r}
#--------------------LMSquareLossIterations------------------------
  #Find the Error for each Iteration:
#LMSquareLoss_Error <-Find_Wmatrix_MeanL2Error(Cliped[,DataColsStart:DataColsEnd], Cliped[,LabelCol],DeNormalizedWeights,BinaryClassification)

  #Here is a Matrix of the Error of each iteration of the weight Matrix:
  #print(LMSquareLoss_Error)
  
  #Here is a plot the weight Matrix found with LMSquareLossIterations:
#barplot(LMSquareLoss_Error,main = "LMSquareLossIterations Vs Error \n 'Library'spam",xlab = "mean loss value",beside = TRUE)

  #print(DeNorm.Error)
  
#------------------------------------------------------------------------
  
  
  
#--------------------LMSquareLossEarlyStoppingCV------------------------
  #Find the Error for each Iteration:
#LMSquareLossES_Error <-Find_Wmatrix_MeanL2Error(Cliped[,DataColsStart:DataColsEnd], Cliped[,LabelCol],LMSquareLossES_Weights,BinaryClassification)

  #Here is a Matrix of the Error of each iteration of the weight Matrix:
  #print(LMSquareLossES_Error)
  
  #Here is a plot the weight Matrix found with LMSquareLossIterations:
#barplot(LMSquareLossES_Error,main = "EarlyStoping",xlab = "mean loss value",beside = TRUE)
#------------------------------------------------------------------------
```


comment on difference in accuracy.


### Train/validation loss plot

plot the two loss functions.

What are the optimal regularization parameters?

For each data set, use 4-fold cross-validation to evaluate the prediction accuracy of your code. For each split s=1 to 4, set aside the data in fold s as a test set. Use ___CV to train a model on the other folds (which should be used in your ___CV function as internal train/validation sets/splits), then make a prediction on the test fold s.

For each train/test split, to show that your algorithm is actually learning something non-trivial from the inputs/features, compute a baseline predictor that ignores the inputs/features.
Regression: the mean of the training labels/outputs.
Binary classification: the most frequent class/label/output in the training data.

For each data set, compute a 4 x 3 matrix of mean test loss values:
each of the four rows are for a specific test set,
the first column is for the early stopping predictor,
the second column is for the L2 regularized predictor,
the third column is for the baseline/un-informed predictor.

 

## Error results table
| Test Set  | Early Stoping | L2 Regularized Predictor | Baseline (30 Iterations)(0.1 step)  | 
|:----------|:--------------|:-------------------------|:------------------------------------|
| 1         |               |                          |                            |  
| 2         |               |                          |                                     |  
| 3         |               |                          |                                     |  
| 4         |               |                          |                                     |  
| Avg.      |               |                          |                                     |


## Data set 3: 

### Matrix of loss values

Here is an example of how to use the LMSquareLossIterations function:
//Currently having build issues, but code works within R.Studio 
```{r}


```


comment on difference in accuracy.


### Train/validation loss plot

plot the two loss functions.



For each data set, use 4-fold cross-validation to evaluate the prediction accuracy of your code. For each split s=1 to 4, set aside the data in fold s as a test set. Use ___CV to train a model on the other folds (which should be used in your ___CV function as internal train/validation sets/splits), then make a prediction on the test fold s.

For each train/test split, to show that your algorithm is actually learning something non-trivial from the inputs/features, compute a baseline predictor that ignores the inputs/features.
Regression: the mean of the training labels/outputs.
Binary classification: the most frequent class/label/output in the training data.


What are the optimal regularization parameters?

For each data set, compute a 4 x 3 matrix of mean test loss values:
each of the four rows are for a specific test set,
the first column is for the early stopping predictor,
the second column is for the L2 regularized predictor,
the third column is for the baseline/un-informed predictor.


## Error results table
| Test Set  | Early Stoping | L2 Regularized Predictor | Baseline (30 Iterations)(0.1 step)  | 
|:----------|:--------------|:-------------------------|:------------------------------------|
| 1         |               |                          |                            |  
| 2         |               |                          |                                     |  
| 3         |               |                          |                                     |  
| 4         |               |                          |                                     |  
| Avg.      |               |                          |                                     |

