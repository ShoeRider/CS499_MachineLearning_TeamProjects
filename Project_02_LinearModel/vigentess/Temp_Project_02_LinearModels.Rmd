---
title: "Linear Models Report"
author: "Anthony Schroeder,"
date: ""
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}

---
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

<a id="top"></a>

# Project 2 LM Vignette
### <authors>
#### NAU March 1, 2019

> [Introduction](#intro)
> [Installation](#install)
> [Steps to Run](#qs)
> [Linear Regression](#lin)
> [LMSquareLossIterations](#lin.SqrLoss)
> [LMSquareLossEarlyStoppingCV](#lin.SqrLossCV)
> [LMSquareLossL2](#lin.SqrLossL2)
> [LMSquareLossL2penalties](#lin.SqrLossPen)
> [LMSquareLossL2CV](#lin.SqrLossL2CV)
> [Logistic Regression](#log)
> [LMLogisticLossIterations](#log.LogLoss)
> [LMLogisticLossEarlyStoppingCV](#log.LogLossCV)
> [LMLogLossL2](#log.LogLossL2)
> [LMLogLossL2penalties](#log.LogLossPen)
> [LMLogLossL2CV](#log.LogLossL2CV)

<a id="intro"></a>

## Introduction

The second machine learning project for our class is to create Linear Learning Models using Logistic Loss for binary classification and Square Loss for other classification problems. We have to use what we discussed in class to code the linear regression and cross validation functions in order to minimize the loss. 

### Linear Regression


#### LMSquareLossIterations


**Fromula: ** **$\hat{y} = XW$**
Y.hat = data.matrix(TestingData) %*% data.matrix(W.Matrix)

**Gradient: ** **${W'} = 2X^T(X-y)^2$**
Gradient = 2*sum(W.Vector*TrainingData + TrainingLabels)*TrainingLabels

#### Linear L2 equations
**We are Looking for : ** **$\hat{W}^{Lambda} = argmin_{W\in R^{P}}$**

**Regularization Gradient: ** **${W'} = 2X^T(X-y)^2 + 2*(Labmda)*W$**







### Logistic equations

#### Logistic Iteration equations
**Fromula: ** **$\hat{y} = 1/(1+e^{XW})$**
Y.hat = 1/(1+exp(-TrainingLabels%*% data.matrix(TrainingData) %*% data.matrix(W.Vector)))

**Gradient: ** **${W'} = 1/(1+e^{-yXW})-y$**
Gradient = 2*sum(W.Vector*TrainingData + TrainingLabels)*TrainingLabels

#### Linear L2 equations
**We are Looking for : ** **$\hat{W}^{Lambda} = argmin_{W\in R^{P}}$**

**Regularization Gradient: ** **${W'} = 2X^T(X-y)^2 + 2*(Labmda)*W$**


## Testing With Data Sets 
we are going to run our code on the following data sets.




# Tests with ''Libraries to 


## Data set 1: spam

### Matrix of loss values

Here is an example of how to use the LMSquareLossIterations function:

//Currently having build issues, but code works within R.Studio 
```{r}
library(LinearModel)

# Data set 1: spam
#----------------------Data Initialization-------------------------------
  Folds <- 3
  MaxNeighbors <- 30
  Local_spam<- ElemStatLearn::spam

  BinaryClassification = 1
  Local_spam$spam <- sapply(as.character(Local_spam$spam),switch,"spam"=1,"email"=0)
  MaxSample_ofType = length(Local_spam[Local_spam$spam == 1,][,1])


  Spam <- data.frame(Local_spam[Local_spam$spam == 1,])
  print(NROW(Spam))


  email = Local_spam[0,]
  email <- head(Local_spam[Local_spam$spam == 0,],MaxSample_ofType)
  #print(NROW(email))

  Cliped<-rbind(Spam,email)
  Cliped<-Cliped[sample(nrow(Cliped)),]
  DataColsStart = 0
  DataColsEnd   = NCOL(Cliped) - 1
  LabelCol      = NCOL(Cliped)
  Rows          = NROW(Cliped)

  #Create New Fold Column to hold Fold Values
  fold.vec <- Random_Folds(Rows,Folds)
  
  TrainingLabels <- (Cliped[,LabelCol])
  TrainingData   <- (Cliped[,DataColsStart:DataColsEnd])
#------------------------------------------------------------------------
  
    
  
  
#Note on Functional Parameters: LMSquareLossIterations(
#  
#)
  
  # This Function Returns a Weights Matrix of each iteration between taking the gradient:
  # Gradient = 2*sum(W.Vector*TrainingData + TrainingLabels)*TrainingLabels
  
#--------------------LMSquareLossIterations------------------------
  #LMSquareLoss_Weights <- LMSquareLossIterations(TrainingLabels, TrainingData,20,0.1)
  DeNormalizedWeights <- LMSquareLossIterations(Cliped[,DataColsStart:DataColsEnd], Cliped[,LabelCol],20,0.1)

#------------------------------------------------------------------------
  
  
  #Note on Functional Parameters: LMSquareLossEarlyStoppingCV(
#  
#)
  
  # This Function Returns ...
  
  
#--------------------LMSquareLossEarlyStoppingCV------------------------
 # LMSquareLossES_Weights<- LMSquareLossEarlyStoppingCV(Cliped[,DataColsStart:DataColsEnd], Cliped[,LabelCol], fold.vec,Folds,30)
  ES.List <-LMSquareLossEarlyStoppingCV(Cliped[,DataColsStart:DataColsEnd], Cliped[,LabelCol], fold.vec,Folds,30)
  #print(ES.List)
  LMSquareLossES_Weights <- ES.List.w.mat
  

#------------------------------------------------------------------------
```


comment on difference in accuracy.
The Accuracy inttially improves drametically, but then deviates away from the optimal solution. 

### Train/validation loss plot

plot the two loss functions.


```{r}
#--------------------LMSquareLossIterations------------------------
  #Find the Error for each Iteration:
  LMSquareLoss_Error <-Find_Wmatrix_MeanL2Error(Cliped[,DataColsStart:DataColsEnd], Cliped[,LabelCol],DeNormalizedWeights,BinaryClassification)

  #Here is a Matrix of the Error of each iteration of the weight Matrix:
  #print(LMSquareLoss_Error)
  
  #Here is a plot the weight Matrix found with LMSquareLossIterations:
  barplot(LMSquareLoss_Error,main = "LMSquareLossIterations Vs Error \n 'Library'spam",xlab = "mean loss value",beside = TRUE)

  #print(DeNorm.Error)
  
#------------------------------------------------------------------------
  
  
  
#--------------------LMSquareLossEarlyStoppingCV------------------------
  #Find the Error for each Iteration:
  LMSquareLossES_Error <-Find_Wmatrix_MeanL2Error(Cliped[,DataColsStart:DataColsEnd], Cliped[,LabelCol],ES.List.w.mat,BinaryClassification)

  #Here is a Matrix of the Error of each iteration of the weight Matrix:
  #print(LMSquareLossES_Error)
  
  #Here is a plot the weight Matrix found with LMSquareLossIterations:
  barplot(LMSquareLossES_Error,main = "EarlyStoping",xlab = "mean loss value",beside = TRUE)
#------------------------------------------------------------------------
```



What are the optimal regularization parameters?








## Data set 2: 

### Matrix of loss values

Here is an example of how to use the LMSquareLossIterations function:
//Currently having build issues, but code works within R.Studio 
```{r}

```


comment on difference in accuracy.


### Train/validation loss plot

plot the two loss functions.







## Data set 3: 

### Matrix of loss values

Here is an example of how to use the LMSquareLossIterations function:
//Currently having build issues, but code works within R.Studio 
```{r}

```


comment on difference in accuracy.


### Train/validation loss plot

plot the two loss functions.
