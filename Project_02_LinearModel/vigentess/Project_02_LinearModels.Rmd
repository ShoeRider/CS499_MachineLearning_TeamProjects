---
title: "Linear Models Report"
author: "Anthony Schroeder"
contributor: "Chadd Frasier"
date: "03/08/2019"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Linear Models Report}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}

---
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

<a id="top"></a>

# Project 2 LM Vignette
### Anthony Schroeder
#### NAU March 1, 2019

> [Introduction](#intro)
> [Installation](#install)
> [Linear Regression](#lin)
> [LMSquareLossIterations](#lin.SqrLoss)
> [LMSquareLossEarlyStoppingCV](#lin.SqrLossCV)
> [LMSquareLossL2](#lin.SqrLossL2)
> [LMSquareLossL2penalties](#lin.SqrLossPen)
> [LMSquareLossL2CV](#lin.SqrLossL2CV)
> [Logistic Regression](#log)
> [LMLogisticLossIterations](#log.LogLoss)
> [LMLogisticLossEarlyStoppingCV](#log.LogLossCV)
> [LMLogLossL2](#log.LogLossL2)
> [LMLogLossL2penalties](#log.LogLossPen)
> [LMLogLossL2CV](#log.LogLossL2CV)
> [Testing With Data Sets](#test)



<a id="intro"></a>
## Introduction
  The second machine learning project for our class is to create Linear Models using different gradient decent algorithms. We have to use what we discussed in class to code the regression and cross validation functions in order to predict the labels of the test data which is based on two things; the weight vector(obtained through regression) and the test data we are given.
  We will use the cross validation function to select the optimal number of steps in the regression and then perform the linear regression on the training/validation data for all the folds to obtain the result matrix of all functions run on all train/validation splits. We can use the most accurate fold to predict the labels of the test data using $ cbind(1, X.mat) * W.mat$

  Gradient descent will be done using the gerneral form $w^t = w^(t-1) - \alpha\bigtriangledown\mathcal{l(w)}$.

<a id="lin"></a>
## Linear Regression
Linear regression is a linear modeling method that helps to analyse the relationship between a dependant variable and one or more independent variables. In this project we will be using multiple linear regression which will make a vector of predictions based on the relationship observed during gradient decent. We will be calculating our regressions first with the L1 Norm and then the L2 Norm.
- L1 $$L1 = \stackrel{argmax}{w\epsilon\rm I\!R^p}\sum_{i=1}^n|X_i|$$
- Squared L2 Loss $$\mathcal{L}(w) = \frac{1}{n}||Xw-y||^2_2$$
- Gradient L2 $$\bigtriangledown \mathcal{L}(w) = 2(Xw-y)*X^T$$

<a id="lin.SqrLoss"></a>
## LMSquareLossIterations
This function will take a matrix of training input (n_train x n_features), a vector of labels for the training data vectors (n_train x 1), it will take a scaler value that is the maximum number of iterations in the decent algorithm(scalar > 1), and lastly it will take in a step size which is the size of each step in the gradent descent(scaler > 0). This function will take the matrix of training data and the vector of labels to perform gradient descent on each vector and then create a matrix of weight vectors created from the gradient decent. The function will output th weight matrix by creating a vector for each iteration (n_features+1 x max.iterations). The matrix is larger than the training data so in order to perform matrix multiplication the first element of the weight vectors should be the intercept term. This function will need to minimize the mean loss, not the total loss or eles the gradient decent may not converge. Another method for convergence is to scale the trainign data matrix to a smaller size so the steps of the algorithm will not have to be as large and we will get a much more accurate convergence value. We will compute a scaled input matrix, with a mean of 0 and standard deviation of 1 for each column. This is how we accomplished the matrix normalization.
```{r}
NormalizeMatrix<-function(Matrix)
{
  # ---------- This creates a scaled vector with sd = 1 mean = 0 for each column in matrix-------
  scaled.mat = matrix(nrow = nrow(Matrix),ncol = ncol(Matrix))
  for(col in 1:ncol(Matrix))
  {
    scaled.mat[,col] = (Matrix[,col] - colMeans(Matrix)[col])/sd(Matrix[,col])

  }
  return(as.matrix(scaled.mat))
}
```
Code Block 1

<a id="lin.SqrLossCV"></a>
## LMSquareLossEarlyStoppingCV
This function will accept a matrix of data (n_train x n_features) and a vector of labels for the training data vectors (n_train x 1), as well as a fold vector (n_train x 1) as well as the maximum number of iterations that can be selected as optimal. This function will perform K-fold cross validation on all the train/validation splits to compute the validation loss of each model to determine the optimal number of steps and iterations where steps = max.iterations. Lastly, we will use LMSquareLossIterations(max.iterations=selected.steps) on the whole training data set to output a list with named elements; mean.validation.loss, mean.train.loss.vec, selected.steps, weight.vec, and predict(testX.mat), which will be a function that takes a test features matrix and returns a vector of predictions.

<a id="lin.SqrLossL2"></a>
## LMSquareLossL2
This function uses the L2 Squared Loss and the cost functiin with a penalty to perform the regression and create a simple fitted model. The inputs of the function will be the scaled input matrix (n_train x n_features) and the label vector (n_train x 1). The function will require an inputed penalty value (int > 0) and an optimum threshold value (int > 0) that the regression will aim to achieve. Lastly the function will take in a step size (int > 0) and the initial weigth vector of the matrix. This function assumes that the matrix is already scaled as discussed above. This function's job is to find the optimal weight vector that minimizes the following cost function.
$$\sum_{i=1}^n \mathcal{L}[w^T x_i, y_i] + \lambda||w||^2_2$$,
Where $\mathcal{L}$ is the square loss.
This function will output a weight vector for a given penalty parameter.

<a id="lin.SqrLossPen"></a>
## LMSquareLossL2penalties
This function will accept an unscaled matrix (n_train x n_features) that will need to be scaled as described above, as well as the label vector(n_train x 1). The last input that this function requires is a penalty vector that holds decreasing penalty values to try. This function will then loop over the penalty values calling the LMSquareLossL2 function to recieve a scaled optimal weight vector for each penalty. We then need to unscale the vector using the standard deviation of the columns and we will also use warm starting to converge on the optimal much faster. This function outputs a weight matrix (n_features+1 x n_penalties), weight matrix on original scale, that can be used to get predictions via $cbind(1, X.mat) %*% W.mat$

<a id="lin.SqrLossL2CV"></a>
## LMSquareLossL2CV
This function will accept inputs in the form of an input matrix (n_train x n_features) and the usual label vector (n_train x 1). For cross validation we will  need to recieve a fold vector that is size (n_train x 1) and a penalty vector to pass to the LMSquareLossL2penalties to compute the optimal L2 model. This function will use k fold cross validation to perform the modeling on the train/test split and then comput the mean.validation.loss for each fold to find the best fit model. We will be computing a vector of size n_penalties of mean validation loss values. Minimizing the mean validation loss will help us determine the optimal penalty value. Then we will use these optimals to plug back into the LMSquareLossL2penalties function with the whole data set. Lastly, we will output a list with the following named elements; mean.validation.loss, mean.train.loss.vec, penalty.vec, selected.penalty, weight.vec, the weight vector found by using gradient descent with selected.penalty on the whole training data set, and a predict(testX.mat), which is a function that takes a test features matrix and returns a vector of predictions.

**Fromula: ** **$\hat{y} = XW + B$**
Y.hat = data.matrix(TestingData) %*% data.matrix(W.Matrix) +

**Gradient: ** **${W'} = 2X^T(WX-y)^2$**
Gradient = 2*sum(W.Vector*TrainingData + TrainingLabels)*TrainingLabels

**Bias Gradient: ** **${W'} = 2I^T((WX+IB)-y)^2$**
#Gradient = 2*sum(W.Vector*TrainingData + TrainingLabels)*TrainingLabels


#### Linear L2 equations
**We are Looking for : ** **$\hat{W}^{Lambda} = argmin_{W\in R^{P}}$**

**Regularization Gradient: ** **${W'} = 2X^T(X-y)^2 + 2*(Labmda)*W$**


<a id="log"></a>
## Logistic Regression
Logistic regression is a linear model that can be used to estimate the parameters of a logistic model such as a win/loss or yes/no decision parameter using multiple independent variables. In thins project we will be using the logistic loss to find the log likelyhood of the test data. We will use the probability parameter to determine if the label is a 0 or 1 for a specific observation. The major difference in these two regression models is the

<a id="log.LogLoss"></a>
## LMLogisticLossIterations

This function will take a matrix of training input (n_train x n_features), a vector of labels for the training data vectors (n_train x 1), it will take a scaler value that is the maximum number of iterations in the decent algorithm(scalar > 1), and lastly it will take in a step size which is the size of each step in the gradent descent(scaler > 0). This function will take the matrix of training data and the vector of labels to perform gradient descent on each vector and then create a matrix of weight vectors created from the gradient decent. The function will output th weight matrix by creating a vector for each iteration (n_features+1 x max.iterations). The matrix is larger than the training data so in order to perform matrix multiplication the first element of the weight vectors should be the intercept term. This function will need to minimize the mean loss, not the total loss or eles the gradient decent may not converge. Another method for convergence is to scale the trainign data matrix to a smaller size so the steps of the algorithm will not have to be as large and we will get a much more accurate convergence value. We will compute a scaled input matrix, with a mean of 0 and standard deviation of 1 for each column. This is how we accomplished the matrix normalization.
The only major difference will be in the way the gradient decent is calculated. Below is how the logistic regression will be calulating the gradient loss.

- Binary Loss $$\bigtriangledown\mathcal{L(w)} =\sum_{i=1}^n\frac{exp(-\hat{y}w^Tx_i)}{1+exp(-\hat{y}w^Tx_i)}(-\hat{y}x_i)$$

<a id="log.LogLossCV"></a>
## LMLogisticLossEarlyStoppingCV

This function will accept a matrix of data (n_train x n_features) and a vector of labels for the training data vectors (n_train x 1), as well as a fold vector (n_train x 1) as well as the maximum number of iterations that can be selected as optimal. This function will perform K-fold cross validation on all the train/validation splits to compute the validation loss of each model to determine the optimal number of steps and iterations where steps = max.iterations. Lastly, we will use LMLogisticLossIterations(max.iterations=selected.steps) on the whole training data set to output a list with named elements; mean.validation.loss, mean.train.loss.vec, selected.steps, weight.vec, and predict(testX.mat), which will be a function that takes a test features matrix and returns a vector of predictions.


<a id="log.LogLossL2"></a>
## LMLogLossL2
This function uses the L2 Squared Loss and the cost functiin with a penalty to perform the regression and create a simple fitted model. The inputs of the function will be the scaled input matrix (n_train x n_features) and the label vector (n_train x 1). The function will require an inputed penalty value (int > 0) and an optimum threshold value that the regression will aim to achieve. Lastly the function will take in a step size (int > 0) and the initial weigth vector of the matrix. This function assumes that the matrix is already scaled as discussed above. This function's job is to find the optimal weight vector that minimizes the following cost function.
$$\sum_{i=1}^n \mathcal{L}[w^T x_i, y_i] + \lambda||w||^2_2$$,
Where $\mathcal{L}$ is the logistic loss.
This function will output a weight vector for a given penalty parameter.

<a id="log.LogLossPen"></a>
## LMLogLossL2penalties
This function will accept an unscaled matrix (n_train x n_features) that will need to be scaled as described above, as well as the label vector(n_train x 1). The last input that this function requires is a penalty vector that holds decreasing penalty values to try. This function will then loop over the penalty values calling the LMLogisticLossL2 function to recieve a scaled optimal weight vector for each penalty. We then need to unscale the vector using the standard deviation of the columns and we will also use warm starting to converge on the optimal much faster. This function outputs a weight matrix (n_features+1 x n_penalties), weight matrix on original scale, that can be used to get predictions via $cbind(1, X.mat) %*% W.mat$

<a id="log.LogLossL2CV"></a>
## LMLogLossL2CV
This function will accept inputs in the form of an input matrix (n_train x n_features) and the usual label vector (n_train x 1). For cross validation we will  need to recieve a fold vector that is size (n_train x 1) and a penalty vector to pass to the LMSquareLossL2penalties to compute the optimal L2 model. This function will use k fold cross validation to perform the modeling on the train/test split and then comput the mean.validation.loss for each fold to find the best fit model. We will be computing a vector of size n_penalties of mean validation loss values. Minimizing the mean validation loss will help us determine the optimal penalty value. Then we will use these optimals to plug back into the LMLogisticLossL2penalties function with the whole data set. Lastly, we will output a list with the following named elements; mean.validation.loss, mean.train.loss.vec, penalty.vec, selected.penalty, weight.vec, the weight vector found by using gradient descent with selected.penalty on the whole training data set, and a predict(testX.mat), which is a function that takes a test features matrix and returns a vector of predictions.

### Logistic equations
#### Logistic Iteration equations
**Fromula: ** **$\hat{y} = 1/(1+e^{XW})+B$**
Y.hat = 1/(1+exp(-TrainingLabels%*% data.matrix(TrainingData) %*% data.matrix(W.Vector)))

**Gradient: ** **${W'} = 1/(1+e^{-yXW})-y$**
Gradient = 2*sum(W.Vector*TrainingData + TrainingLabels)*TrainingLabels

#### Linear L2 equations
**We are Looking for : ** **$\hat{W}^{Lambda} = argmin_{W\in R^{P}}$**

**Regularization Gradient: ** **${W'} = 2X^T(X-y)^2 + 2*(Labmda)*W$**


<a id= 'test'></a>
## Testing With Data Sets
we are going to run our code on the following data sets for Binary Classification
- ElemStatLearn::spam 2-class [4601, 57] output is last column (spam).
- ElemStatLearn::SAheart 2-class [462, 9] output is last column (chd).
- ElemStatLearn::zip.train: 10-class [7291, 256] output is first column. (ignore classes other than 0 and 1)
And we will be using the following for regression:
- ElemStatLearn::prostate [97 x 8] output is lpsa column, ignore train column.
- ElemStatLearn::ozone [111 x 3] output is first column (ozone).

```{r, results='hide'}
# Spam Data Set
data(spam, package = "ElemStatLearn")

# SAheart Data Set
data(SAheart, package = "ElemStatLearn")

# zip.train Data Set
data(zip.train, package = "ElemStatLearn")

# prostate Data Set
data(prostate, package = "ElemStatLearn")

# OZone Data Set
data(ozone, package = "ElemStatLearn")
```

# How to use the 'ElemStatLearn' Libraries:

How To use this Linear Models Library:
1. Organize your data into:
  a. Training Data:
  Your data should be a matrix with the Column's containing Feature's, and Row's containing the different training Instances

  b.Training Labels:
  Your data should be a single colomn of numerical values,
    Note: if all labels are (0 or 1)  training will be classified as Binary classification, and be trained accordingly. otherwise, training will be Regressive.

## Testing 
# How to use the 'ElemStatLearn' Libraries:
we are going to run our code on the following data sets.

How To use this Linear Models Library:
1. Organize your data into:
  a. Training Data:
  Your data should be a matrix with the Column's containing Feature's, and Row's containing the different training Instances 
    
  b.Training Labels:
  Your data should be a single colomn of numerical values, 
    Note: if all labels are (0 or 1)  training will be classified as Binary classification, and be trained accordingly. otherwise, training will be Regressive.


- We will include the library(LinearModel), to have access to the given functionality.
- The R script Prep_Libraries.R in the tests/testthat folder contains some simple data manipulation to prep the data to be used for the LinearModel Algorithms.

```{r}
library(LinearModel)
#print(getwd())
source("tests/testthat/Prep_Libraries.R")
source("R/General.R")
knitr::opts_chunk$set(fig.width=12, fig.height=8) 
#source("tests/testthat/LinearModels_Test_Libraries.R")
```

# Binary Classification:
## Data set 1: spam
```{r}
# Data set 1: spam
  Spam<-Prep_Spam()
```
### Code

```{r}
# Data set 1: spam
Spam<-Prep_Spam()
LossMat.fold.n  <- 4
LossMat.fold.vec<-Random_Folds((Spam$n_Elements),LossMat.fold.n)
Data = cbind(Spam$TrainingData ,Spam$TrainingLabels)
#Randomly shuffle the data
#Data<-Data[sample(nrow(Data)),]
SLI.WholeError.Mat = 0
SLI.TrainError.Mat = 0
SLI.TestError.Mat  = 0

ES_CV_Error.WholeError.Mat = 0
ES_CV_Error.TrainError.Mat = 0
ES_CV_Error.TestError.Mat  = 0

SL_L2.WholeError.Mat = 0
SL_L2.TrainError.Mat = 0
SL_L2.TestError.Mat  = 0

SL_L2_Pen.WholeError.Mat = 0
SL_L2_Pen.TrainError.Mat = 0
SL_L2_Pen.TestError.Mat  = 0

SL_L2CV.WholeError.Mat = 0
SL_L2CV.TrainError.Mat = 0
SL_L2CV.TestError.Mat  = 0

#Perform folds.n fold cross validation
for(i in 1:(LossMat.fold.n))
{
  testIndexes <- which(LossMat.fold.vec==i,arr.ind=TRUE)
  testData    <- as.matrix(Data[testIndexes, ])
  trainData   <- as.matrix(Data[-testIndexes, ])

  print("Please wait this might take some time, Iteration:")
  print(i)
  Scalar.Step = 0.1
  Iterations  = 30
  
  #-------------------------LMSquareLossIterations (SLI_Error)--------------
    LMSquareLossIterations.W.Vec <- LMSquareLossIterations(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],Iterations,Scalar.Step)
    SLI.WholeError.Vec <-Find_Wmatrix_MeanL2Error(Spam$TrainingData, Spam$TrainingLabels,as.matrix(LMSquareLossIterations.W.Vec),Spam$BinaryClassification)
    #print("dim SLI.WHoleError")
    #print(dim(as.matrix(SLI.WholeError.Vec)))
    #SLI.WholeError.Mat <- cbind(SLI.WholeError.Mat,SLI.WholeError.Vec)
    
    SLI.TrainError.Vec <-Find_Wmatrix_MeanL2Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],as.matrix(LMSquareLossIterations.W.Vec),Spam$BinaryClassification)
    SLI.TrainError.Mat <- cbind(SLI.TrainError.Mat,SLI.TrainError.Vec)
  
    SLI.TestError.Vec  <-Find_Wmatrix_MeanL2Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],as.matrix(LMSquareLossIterations.W.Vec),Spam$BinaryClassification)
    SLI.TestError.Mat<- cbind(SLI.TestError.Mat,SLI.TestError.Vec)
  #------------------------------------------------------------------------
  
  #--------------------LMSquareLossEarlyStoppingCV (ES_CV_Error)-----------
    ES_CV_Error.fold.n  <- 4
    ES_CV_Error.fold.vec<-Random_Folds(NROW(trainData),ES_CV_Error.fold.n)
    Scalar.Step = 0.1
    Iterations  = 30
    #ES_CV_Error.W.Vec <- LMSquareLossEarlyStoppingCV(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],Spam$Folds.Vec,Spam$Folds.n,Spam$Iterations)
    ES_CV_Error.List <- LMSquareLossEarlyStoppingCV(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],ES_CV_Error.fold.vec,ES_CV_Error.fold.n,Iterations)
      
    #ES_CV_Error.WholeError.Vec   <-Find_Wmatrix_MeanL2Error(Spam$TrainingData, Spam$TrainingLabels,as.matrix(ES_CV_Error.List$w.mat),Spam$BinaryClassification)
    #ES_CV_Error.WholeError.Mat <- cbind(ES_CV_Error.WholeError.Mat,ES_CV_Error.WholeError.Vec)
  
    ES_CV_Error.TrainError.Vec <-Find_Wmatrix_MeanL2Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],as.matrix(ES_CV_Error.List$w.mat),Spam$BinaryClassification)
    #ES_CV_Error.TrainError.Mat <- cbind(ES_CV_Error.TrainError.Mat,ES_CV_Error.TrainError.Vec)
  
    ES_CV_Error.TestError.Vec  <-Find_Wmatrix_MeanL2Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],as.matrix(ES_CV_Error.List$w.mat),Spam$BinaryClassification)
    #ES_CV_Error.TestError.Mat  <- cbind(ES_CV_Error.TestError.Mat,ES_CV_Error.TestError.Vec)
  #------------------------------------------------------------------------
   
   
   #-------------------------LMSquareLossL2--(SL_L2_Error)------------------
    Normalized_TrainingData_List <- NormalizeMatrix_List(trainData[,1:NCOL(trainData)-1])
    Penalty.Scalar = 1
    opt.thresh = .2

    SL_L2.W.Matrix <-LMSquareLossL2(Normalized_TrainingData_List$NormalizedMatrix, trainData[,NCOL(trainData)], Penalty.Scalar, opt.thresh,Spam$Initial.Vector)
    DeNormalized.W.Matrix <-(t(SL_L2.W.Matrix))/Normalized_TrainingData_List$sd

    #DeNorm.Error <-Find_Wmatrix_MeanL2Error(Spam$TrainingData, Spam$TrainingLabels,t(DeNormalized.W.Matrix),Spam$BinaryClassification)
    SL_L2.WholeError.Vec <-Find_Wmatrix_MeanL1Error(Spam$TrainingData, Spam$TrainingLabels,t(DeNormalized.W.Matrix),Spam$BinaryClassification)
    SL_L2.WholeError.Mat <- rbind(SL_L2.WholeError.Mat,SL_L2.WholeError.Vec)
    
    SL_L2.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],t(DeNormalized.W.Matrix),Spam$BinaryClassification)
    SL_L2.TestError.Mat <- rbind(SL_L2.TestError.Mat,SL_L2.TestError.Vec)
    
    SL_L2.TrainError.Vec <-Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],t(DeNormalized.W.Matrix),Spam$BinaryClassification)
    SL_L2.TrainError.Mat <- rbind(SL_L2.TrainError.Mat,SL_L2.TrainError.Vec)
   #------------------------------------------------------------------------
    
   #-----------------------LMSquareLossL2penalties--(SL_L2_Pen)-------------
    SL_L2_Pen.W.Mat <-LMSquareLossL2penalties(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)], Spam$Penalty.Vector)

    #DeNorm.Error <-Find_Wmatrix_MeanL2Error(Spam$TrainingData, Spam$TrainingLabels,t(DeNormalized.W.Matrix),Spam$BinaryClassification)
    #print("SL_L2_Pen.WholeError.Vec")
    SL_L2_Pen.WholeError.Vec <-Find_Wmatrix_MeanL1Error(Spam$TrainingData, Spam$TrainingLabels,(SL_L2_Pen.W.Mat),Spam$BinaryClassification)
    SL_L2_Pen.WholeError.Mat <- rbind(SL_L2_Pen.WholeError.Mat,SL_L2_Pen.WholeError.Vec)
    
    #print("SL_L2_Pen.TestError.Vec")
    SL_L2_Pen.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],(SL_L2_Pen.W.Mat),Spam$BinaryClassification)
    SL_L2_Pen.TestError.Mat <- rbind(SL_L2_Pen.TestError.Mat,SL_L2_Pen.TestError.Vec)
    
    #print("SL_L2_Pen.TrainError.Vec")
    SL_L2_Pen.TrainError.Vec <- Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],(SL_L2_Pen.W.Mat),Spam$BinaryClassification)
    SL_L2_Pen.TrainError.Mat <- cbind(SL_L2_Pen.TrainError.Mat,SL_L2_Pen.TrainError.Vec)
   #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
    SL_L2CV.fold.n  <- 4
    SL_L2CV.fold.vec<-Random_Folds(NROW(trainData),SL_L2CV.fold.n)
    
    SL_L2CV.List <- LMSquareLossL2CV(as.matrix(trainData[,1:NCOL(trainData)-1]), as.matrix(trainData[,NCOL(trainData)]),as.matrix(t(SL_L2CV.fold.vec)), as.matrix(Spam$Penalty.Vector))
    

    SL_L2CV.WholeError.Vec <- Find_Wmatrix_MeanL1Error(Spam$TrainingData, Spam$TrainingLabels,(SL_L2CV.List$weight.vec),Spam$BinaryClassification)
    SL_L2CV.WholeError.Mat <- rbind(SL_L2CV.WholeError.Mat,SL_L2CV.WholeError.Vec)
    
    
    #SL_L2CV.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],t(SL_L2CV.W.Mat),Spam$BinaryClassification)
    SL_L2CV.TestError.Vec <- SL_L2CV.List$mean.validation.loss
    SL_L2CV.TestError.Mat <- rbind(SL_L2CV.TestError.Mat,SL_L2CV.TestError.Vec)
    #print("SL_L2CV.TestError.Vec")
    #print(dim(as.matrix(SL_L2CV.TestError.Vec)))
    
    #SL_L2CV.TrainError.Vec <- Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],t(SL_L2CV.W.Mat),Spam$BinaryClassification)
    SL_L2CV.TrainError.Vec <- SL_L2CV.List$mean.train.loss.vec
    SL_L2CV.TrainError.Mat <- rbind(SL_L2CV.TrainError.Mat,SL_L2CV.TrainError.Vec)
    #------------------------------------------------------------------------

}
```




### Matrix of loss values
```{r}
#-------------------Matrix of Cross Validation loss values-------------------------------------

#print(SLI.TestError.Mat)

  #Spam_LossMatrix <-rbind(as.double(SLI.TestError.Mat[30,2:5]),ES_CV_Error.TestError.Mat[2,2:5])
  Spam_LossMatrix <-rbind(as.double(SLI.TestError.Mat[30,2:5]),as.double(as.double(t(SL_L2.TestError.Mat[2:5]))))
  Spam_LossMatrix <-rbind(Spam_LossMatrix,as.double(colMeans(SL_L2_Pen.TestError.Mat)[2:5]))
  Spam_LossMatrix <-rbind(Spam_LossMatrix,as.double(SL_L2CV.TrainError.Mat[2:5]))
  print(Spam_LossMatrix)
  

  colnames(Spam_LossMatrix)<-c("Fold1","Fold2","Fold3","Fold4")
  rownames(Spam_LossMatrix)<-c("Linear Squared 30 Iterations","L2 Reg","L2 Reg with Penalty vector","L2 Reg with CV")
  barplot(Spam_LossMatrix, xlab = "Iterations", ylab = "Error",main = "LinearModels_Spam_LossMatrix",legend = (rownames(Spam_LossMatrix)),beside = TRUE)
```





### Train/validation loss plot
```{r}
  #-------------------- Data Set: 1 Spam ----------------------------------
    #-------------------------LMSquareLossIterations (SLI_Error)--------------
    plot(rowMeans(as.matrix(SLI.TrainError.Mat)),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "Spam Loss of each matrix",ylim=c(.1,1))
    lines(rowMeans(as.matrix(SLI.TestError.Mat)),type="o", col = "light blue")
    #------------------------------------------------------------------------
    
    #--------------------LMSquareLossEarlyStoppingCV (ES_CV_Error)-----------
    #lines(rowMeans(as.matrix(ES_CV_Error.TestError.Mat)),type="o", col = "green")
    #lines(rowMeans(as.matrix(ES_CV_Error.TrainError.Mat)),type="o", col = "Light green")
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2--(SL_L2_Error)------------------
    lines(as.matrix((SL_L2.TestError.Mat[2:NROW(SL_L2.TestError.Mat)])),type="o", col = "black")
    lines(as.matrix(SL_L2.TrainError.Mat[2:NROW(SL_L2.TrainError.Mat)]),type="o", col = "gray")
    #------------------------------------------------------------------------
    
    #-----------------------LMSquareLossL2penalties--(SL_L2_Pen)-------------
    lines(as.matrix((SL_L2_Pen.TestError.Mat[2:NROW(SL_L2_Pen.TestError.Mat)])),type="o", col = "red")
    lines(as.matrix((SL_L2_Pen.TrainError.Mat[2:NROW(SL_L2_Pen.TrainError.Mat)])),type="o", col = "pink")
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
    lines(as.matrix((SL_L2CV.TestError.Mat[2:NROW(SL_L2CV.TestError.Mat)])),type="o", col = "orange")
    lines(as.matrix((SL_L2CV.TrainError.Mat[2:NROW(SL_L2CV.TrainError.Mat)])),type="o", col = "gold")
    #------------------------------------------------------------------------
  #------------------------------------------------------------------------
```


### Extra Credit attempt: compaire The LinearModels to the KNN
```{r}
  #--------------------NN1ToKmaxPredict------------------------------------
  Spam<-Prep_Spam()
  Fold.vec = Random_Folds(Spam$n_Elements,4)
  Fold.n   = 4
  KNNLearnCV.List = KNNLearnCV(Spam$TrainingData, Spam$TrainingLabels, 30, Fold.vec, Fold.n)
  #------------------------------------------------------------------------

```


```{r}
  plot(colMeans(as.matrix(KNNLearnCV.List$TestMeanError.mat)),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "",ylim=c(0,1))
  lines(colMeans(as.matrix(KNNLearnCV.List$TrainMeanError.mat)),type="o", col = "gray")
  
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
    lines(as.matrix((SL_L2CV.TestError.Mat[2:NROW(SL_L2CV.TestError.Mat)])),type="o", col = "orange")
    lines(as.matrix((SL_L2CV.TrainError.Mat[2:NROW(SL_L2CV.TrainError.Mat)])),type="o", col = "gold")
    #------------------------------------------------------------------------
dot.x <- KNNLearnCV.List$selected.KNN
dot.y <- KNNLearnCV.List$TestMeanError.Means[dot.x]

matpoints(x = dot.x,
          y = dot.y,
          col = 2,
          pch = 19)
 # legend(25,30,c("decay","growth"),col=c("blue","red"),pch=c(15,19), y.intersp=1800)
```

## Data set 2: SAheart
```{r}
# Data set 2: SAheart
  SAheart<-Prep_SAheart()
```
### Code 
```{r}
  #-------------------- Data set 2: SAheart ----------------------------------
  LossMat.fold.n  <- 4
  LossMat.fold.vec<-Random_Folds((SAheart$n_Elements),LossMat.fold.n)
  Data = cbind(SAheart$TrainingData ,SAheart$TrainingLabels)
  #Randomly shuffle the data
  #Data<-Data[sample(nrow(Data)),]

  SLI.WholeError.Mat = 0
  SLI.TrainError.Mat = 0
  SLI.TestError.Mat  = 0
  
  ES_CV_Error.WholeError.Mat = 0
  ES_CV_Error.TrainError.Mat = 0
  ES_CV_Error.TestError.Mat  = 0
  
  SL_L2.WholeError.Mat = 0
  SL_L2.TrainError.Mat = 0
  SL_L2.TestError.Mat  = 0
  
  SL_L2_Pen.WholeError.Mat = 0
  SL_L2_Pen.TrainError.Mat = 0
  SL_L2_Pen.TestError.Mat  = 0
  
  SL_L2CV.WholeError.Mat = 0
  SL_L2CV.TrainError.Mat = 0
  SL_L2CV.TestError.Mat  = 0
  #Perform folds.n fold cross validation
  for(i in 1:(LossMat.fold.n))
  {
    testIndexes <- which(LossMat.fold.vec==i,arr.ind=TRUE)
    testData    <- as.matrix(Data[testIndexes, ])
    trainData   <- as.matrix(Data[-testIndexes, ])
  
    print("Please wait this might take some time, Iteration:")
    print(i)
    
    Scalar.Step = 0.01
    Iterations  = 30
    #-------------------------LMSquareLossIterations (SLI_Error)--------------
    LMSquareLossIterations.W.Vec <- LMSquareLossIterations(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],Iterations,Scalar.Step)
    SLI.WholeError.Vec <-Find_Wmatrix_MeanL2Error(SAheart$TrainingData, SAheart$TrainingLabels,as.matrix(LMSquareLossIterations.W.Vec),SAheart$BinaryClassification)
    #print("dim SLI.WHoleError")
    #print(dim(as.matrix(SLI.WholeError.Vec)))
    SLI.WholeError.Mat <- cbind(SLI.WholeError.Mat,SLI.WholeError.Vec)

   SLI.TrainError.Vec <-Find_Wmatrix_MeanL2Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],as.matrix(LMSquareLossIterations.W.Vec),SAheart$BinaryClassification)
   SLI.TrainError.Mat <- cbind(SLI.TrainError.Mat,SLI.TrainError.Vec)

    SLI.TestError.Vec  <-Find_Wmatrix_MeanL2Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],as.matrix(LMSquareLossIterations.W.Vec),SAheart$BinaryClassification)
    SLI.TestError.Mat<- cbind(SLI.TestError.Mat,SLI.TestError.Vec)
    #------------------------------------------------------------------------

    #--------------------LMSquareLossEarlyStoppingCV (ES_CV_Error)-----------
    ES_CV_Error.fold.n  <- 4
    ES_CV_Error.fold.vec<-Random_Folds(NROW(trainData),ES_CV_Error.fold.n)

    #ES_CV_Error.W.Vec <- LMSquareLossEarlyStoppingCV(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],SAheart$Folds.Vec,SAheart$Folds.n,SAheart$Iterations)
    ES_CV_Error.List <- LMSquareLossEarlyStoppingCV(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],ES_CV_Error.fold.vec,ES_CV_Error.fold.n,Iterations)
    
    ES_CV_Error.WholeError.Vec   <-Find_Wmatrix_MeanL2Error(SAheart$TrainingData, SAheart$TrainingLabels,as.matrix(ES_CV_Error.List$w.mat),SAheart$BinaryClassification)
    #print("dim ES_CV_Error.WholeError.Vec")
    #print(dim(as.matrix(ES_CV_Error.WholeError.Vec)))
    #ES_CV_Error.WholeError.Mat <- cbind(ES_CV_Error.WholeError.Mat,ES_CV_Error.WholeError.Vec)

    ES_CV_Error.TrainError.Vec <-Find_Wmatrix_MeanL2Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],as.matrix(ES_CV_Error.List$w.mat),SAheart$BinaryClassification)
    #ES_CV_Error.TrainError.Mat <- cbind(ES_CV_Error.TrainError.Mat,ES_CV_Error.TrainError.Vec)

    ES_CV_Error.TestError.Vec  <-Find_Wmatrix_MeanL2Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],as.matrix(ES_CV_Error.List$w.mat),SAheart$BinaryClassification)
    #ES_CV_Error.TestError.Mat  <- cbind(ES_CV_Error.TestError.Mat,ES_CV_Error.TestError.Vec)
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2--(SL_L2_Error)------------------
    Normalized_TrainingData_List <- NormalizeMatrix_List(trainData[,1:NCOL(trainData)-1])
    Penalty.Scalar = 1
    opt.thresh = .2

    SL_L2.W.Matrix <-LMSquareLossL2(Normalized_TrainingData_List$NormalizedMatrix, trainData[,NCOL(trainData)], Penalty.Scalar, opt.thresh,SAheart$Initial.Vector)
    DeNormalized.W.Matrix <-(t(SL_L2.W.Matrix))/Normalized_TrainingData_List$sd

    #DeNorm.Error <-Find_Wmatrix_MeanL2Error(SAheart$TrainingData, SAheart$TrainingLabels,t(DeNormalized.W.Matrix),SAheart$BinaryClassification)
    SL_L2.WholeError.Vec <-Find_Wmatrix_MeanL1Error(SAheart$TrainingData, SAheart$TrainingLabels,t(DeNormalized.W.Matrix),SAheart$BinaryClassification)
    SL_L2.WholeError.Mat <- rbind(SL_L2.WholeError.Mat,SL_L2.WholeError.Vec)
    
    SL_L2.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],t(DeNormalized.W.Matrix),SAheart$BinaryClassification)
    SL_L2.TestError.Mat <- rbind(SL_L2.TestError.Mat,SL_L2.TestError.Vec)
    
    SL_L2.TrainError.Vec <-Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],t(DeNormalized.W.Matrix),SAheart$BinaryClassification)
    SL_L2.TrainError.Mat <- rbind(SL_L2.TrainError.Mat,SL_L2.TrainError.Vec)
    #------------------------------------------------------------------------
    
    #-----------------------LMSquareLossL2penalties--(SL_L2_Pen)-------------
    Penalty.Scalar = 1
    opt.thresh = .2
    SL_L2_Pen.W.Mat <-LMSquareLossL2penalties(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)], SAheart$Penalty.Vector)

    #DeNorm.Error <-Find_Wmatrix_MeanL2Error(SAheart$TrainingData, SAheart$TrainingLabels,t(DeNormalized.W.Matrix),SAheart$BinaryClassification)
    #print("SL_L2_Pen.WholeError.Vec")
    SL_L2_Pen.WholeError.Vec <-Find_Wmatrix_MeanL1Error(SAheart$TrainingData, SAheart$TrainingLabels,(SL_L2_Pen.W.Mat),SAheart$BinaryClassification)
    SL_L2_Pen.WholeError.Mat <- rbind(SL_L2_Pen.WholeError.Mat,SL_L2_Pen.WholeError.Vec)
    
    #print("SL_L2_Pen.TestError.Vec")
    SL_L2_Pen.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],(SL_L2_Pen.W.Mat),SAheart$BinaryClassification)
    SL_L2_Pen.TestError.Mat <- rbind(SL_L2_Pen.TestError.Mat,SL_L2_Pen.TestError.Vec)
    
    #print("SL_L2_Pen.TrainError.Vec")
    SL_L2_Pen.TrainError.Vec <- Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],(SL_L2_Pen.W.Mat),SAheart$BinaryClassification)
    SL_L2_Pen.TrainError.Mat <- rbind(SL_L2_Pen.TrainError.Mat,SL_L2_Pen.TrainError.Vec)
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
    SL_L2CV.fold.n  <- 2
    SL_L2CV.fold.vec<-Random_Folds(NROW(trainData),SL_L2CV.fold.n)
      
    SL_L2CV.List <- LMSquareLossL2CV(as.matrix(trainData[,1:NCOL(trainData)-1]), as.matrix(trainData[,NCOL(trainData)]),as.matrix(t(SL_L2CV.fold.vec)), as.matrix(SAheart$Penalty.Vector))
      

    SL_L2CV.WholeError.Vec <- Find_Wmatrix_MeanL1Error(SAheart$TrainingData, SAheart$TrainingLabels,(SL_L2CV.List$weight.vec),SAheart$BinaryClassification)
    SL_L2CV.WholeError.Mat <- rbind(SL_L2CV.WholeError.Mat,SL_L2CV.WholeError.Vec)
      
      
    #SL_L2CV.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],t(SL_L2CV.W.Mat),SAheart$BinaryClassification)
    SL_L2CV.TestError.Vec <- SL_L2CV.List$mean.validation.loss
    SL_L2CV.TestError.Mat <- rbind(SL_L2CV.TestError.Mat,SL_L2CV.TestError.Vec)
    #print("SL_L2CV.TestError.Vec")
    #print(dim(as.matrix(SL_L2CV.TestError.Vec)))
      
    #SL_L2CV.TrainError.Vec <- Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],t(SL_L2CV.W.Mat),SAheart$BinaryClassification)
    SL_L2CV.TrainError.Vec <- SL_L2CV.List$mean.train.loss.vec
    SL_L2CV.TrainError.Mat <- rbind(SL_L2CV.TrainError.Mat,SL_L2CV.TrainError.Vec)
    #print("SL_L2CV.TrainError.Vec")
    #print(dim(as.matrix(SL_L2CV.TrainError.Vec)))
    #------------------------------------------------------------------------
  }

  #------------------------------------------------------------------------
```
### Matrix of loss values
```{r}
#-------------------Matrix of Cross Validation loss values-------------------------------------

#print(SLI.TestError.Mat)

  #Spam_LossMatrix <-rbind(,ES_CV_Error.TestError.Mat[2,2:5])
  Spam_LossMatrix <-rbind(as.double(SLI.TestError.Mat[30,2:5]),as.double(as.double(t(SL_L2.TestError.Mat[2:5]))))
  Spam_LossMatrix <-rbind(Spam_LossMatrix,as.double(colMeans(SL_L2_Pen.TestError.Mat)[2:5]))
  Spam_LossMatrix <-rbind(Spam_LossMatrix,as.double(SL_L2CV.TrainError.Mat[2:5]))
  print(Spam_LossMatrix)
  

  colnames(Spam_LossMatrix)<-c("Fold1","Fold2","Fold3","Fold4")
  rownames(Spam_LossMatrix)<-c("Linear Squared 30 Iterations","L2 Reg","L2 Reg with Penalty vector","L2 Reg with CV")
  barplot(Spam_LossMatrix, xlab = "Iterations", ylab = "Error",main = "SAheart_LossMatrix",legend = (rownames(Spam_LossMatrix)),beside = TRUE)
```

### Train/validation loss plot


```{r}
  #-------------------- Graph Data Set: 2 SAheart -----------------------------
    #-------------------------LMSquareLossIterations (SLI_Error)--------------
    plot(rowMeans(as.matrix(SLI.TrainError.Mat)),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "SAheart Loss of each matrix",ylim=c(.1,1))
    lines(rowMeans(as.matrix(SLI.TestError.Mat)),type="o", col = "light blue")
    #------------------------------------------------------------------------
    
    #--------------------LMSquareLossEarlyStoppingCV (ES_CV_Error)-----------
    lines(rowMeans(as.matrix(ES_CV_Error.TestError.Vec)),type="o", col = "green")
    lines(rowMeans(as.matrix(ES_CV_Error.TrainError.Vec)),type="o", col = "Light green")
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2--(SL_L2_Error)------------------
    lines(as.matrix((SL_L2.TestError.Mat[2:NROW(SL_L2.TestError.Mat)])),type="o", col = "black")
    lines(as.matrix(SL_L2.TrainError.Mat[2:NROW(SL_L2.TrainError.Mat)]),type="o", col = "gray")
    #------------------------------------------------------------------------
    
    #-----------------------LMSquareLossL2penalties--(SL_L2_Pen)-------------
    lines(as.matrix((SL_L2_Pen.TestError.Mat[2:NROW(SL_L2_Pen.TestError.Mat)])),type="o", col = "red")
    lines(as.matrix((SL_L2_Pen.TrainError.Mat[2:NROW(SL_L2_Pen.TrainError.Mat)])),type="o", col = "pink")
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
    lines(as.matrix((SL_L2CV.TestError.Mat[2:NROW(SL_L2CV.TestError.Mat)])),type="o", col = "orange")
    lines(as.matrix((SL_L2CV.TrainError.Mat[2:NROW(SL_L2CV.TrainError.Mat)])),type="o", col = "gold")
    #------------------------------------------------------------------------
  #------------------------------------------------------------------------
```
### Extra Credit attempt: compaire The LinearModels to the KNN
```{r}
  #--------------------NN1ToKmaxPredict------------------------------------
  SAheart<-Prep_SAheart()
  Fold.vec = Random_Folds(SAheart$n_Elements,4)
  Fold.n   = 4
  KNNLearnCV.List = KNNLearnCV(SAheart$TrainingData, SAheart$TrainingLabels, 30, Fold.vec, Fold.n)

  barplot(KNNLearnCV.List$TestMeanError.Means,main = "SAheart: KNNLearnCV L2 Mean Error",xlab = "KNN Compared",ylab = "Error",beside = TRUE)
  #------------------------------------------------------------------------

```
```{r}
  plot(colMeans(as.matrix(KNNLearnCV.List$TestMeanError.mat)),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "",ylim=c(0,1))
  lines(colMeans(as.matrix(KNNLearnCV.List$TrainMeanError.mat)),type="o", col = "gray")
  
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
    lines(as.matrix((SL_L2CV.TestError.Mat[2:NROW(SL_L2CV.TestError.Mat)])),type="o", col = "orange")
    lines(as.matrix((SL_L2CV.TrainError.Mat[2:NROW(SL_L2CV.TrainError.Mat)])),type="o", col = "gold")
    #------------------------------------------------------------------------
dot.x <- KNNLearnCV.List$selected.KNN
dot.y <- KNNLearnCV.List$TestMeanError.Means[dot.x]

matpoints(x = dot.x,
          y = dot.y,
          col = 2,
          pch = 19)
 # legend(25,30,c("decay","growth"),col=c("blue","red"),pch=c(15,19), y.intersp=1800)
```


## Data set 3: Zip.train
```{r}
  ZipTrain<-Prep_ZipTrain()
```
### Code 
```{r}
  #-------------------- Data set 3: Zip.train ----------------------------------
  LossMat.fold.n  <- 4
  LossMat.fold.vec<-Random_Folds((ZipTrain$n_Elements),LossMat.fold.n)
  Data = cbind(ZipTrain$TrainingData ,ZipTrain$TrainingLabels)
  #Randomly shuffle the data
  #Data<-Data[sample(nrow(Data)),]

  SLI.WholeError.Mat = 0
  SLI.TrainError.Mat = 0
  SLI.TestError.Mat  = 0
  
  ES_CV_Error.WholeError.Mat = 0
  ES_CV_Error.TrainError.Mat = 0
  ES_CV_Error.TestError.Mat  = 0
  
  SL_L2.WholeError.Mat = 0
  SL_L2.TrainError.Mat = 0
  SL_L2.TestError.Mat  = 0
  
  SL_L2_Pen.WholeError.Mat = 0
  SL_L2_Pen.TrainError.Mat = 0
  SL_L2_Pen.TestError.Mat  = 0
  
  SL_L2CV.WholeError.Mat = 0
  SL_L2CV.TrainError.Mat = 0
  SL_L2CV.TestError.Mat  = 0
  #Perform folds.n fold cross validation
  for(i in 1:(LossMat.fold.n))
  {
    testIndexes <- which(LossMat.fold.vec==i,arr.ind=TRUE)
    testData    <- as.matrix(Data[testIndexes, ])
    trainData   <- as.matrix(Data[-testIndexes, ])
  
    print("Please wait this might take some time, Iteration:")
    print(i)
    
    Scalar.Step = 0.01
    Iterations  = 30
    
    #-------------------------LMSquareLossIterations (SLI_Error)-------------
    LMSquareLossIterations.W.Vec <- LMSquareLossIterations(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],Iterations,Scalar.Step)
    SLI.WholeError.Vec <-Find_Wmatrix_MeanL2Error(ZipTrain$TrainingData, ZipTrain$TrainingLabels,as.matrix(LMSquareLossIterations.W.Vec),ZipTrain$BinaryClassification)
    print("dim SLI.WHoleError")
    print(dim(as.matrix(SLI.WholeError.Vec)))
    SLI.WholeError.Mat <- cbind(SLI.WholeError.Mat,SLI.WholeError.Vec)

    SLI.TrainError.Vec <-Find_Wmatrix_MeanL2Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],as.matrix(LMSquareLossIterations.W.Vec),ZipTrain$BinaryClassification)
    SLI.TrainError.Mat <- cbind(SLI.TrainError.Mat,SLI.TrainError.Vec)

    SLI.TestError.Vec  <-Find_Wmatrix_MeanL2Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],as.matrix(LMSquareLossIterations.W.Vec),ZipTrain$BinaryClassification)
   SLI.TestError.Mat<- cbind(SLI.TestError.Mat,SLI.TestError.Vec)
    #------------------------------------------------------------------------

    #--------------------LMSquareLossEarlyStoppingCV (ES_CV_Error)-----------
    ES_CV_Error.fold.n  <- 4
    ES_CV_Error.fold.vec<-Random_Folds(NROW(trainData),ES_CV_Error.fold.n)

    #ES_CV_Error.W.Vec <- LMSquareLossEarlyStoppingCV(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],ZipTrain$Folds.Vec,ZipTrain$Folds.n,ZipTrain$Iterations)
    ES_CV_Error.List <- LMSquareLossEarlyStoppingCV(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],ES_CV_Error.fold.vec,ES_CV_Error.fold.n,Iterations)
    
    ES_CV_Error.WholeError.Vec   <-Find_Wmatrix_MeanL2Error(ZipTrain$TrainingData, ZipTrain$TrainingLabels,as.matrix(ES_CV_Error.List$w.mat),ZipTrain$BinaryClassification)
    #print("dim ES_CV_Error.WholeError.Vec")
    #print(dim(as.matrix(ES_CV_Error.WholeError.Vec)))
    #ES_CV_Error.WholeError.Mat <- cbind(ES_CV_Error.WholeError.Mat,ES_CV_Error.WholeError.Vec)

    ES_CV_Error.TrainError.Vec <-Find_Wmatrix_MeanL2Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],as.matrix(ES_CV_Error.List$w.mat),ZipTrain$BinaryClassification)
    #ES_CV_Error.TrainError.Mat <- cbind(ES_CV_Error.TrainError.Mat,ES_CV_Error.TrainError.Vec)

    ES_CV_Error.TestError.Vec  <-Find_Wmatrix_MeanL2Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],as.matrix(ES_CV_Error.List$w.mat),ZipTrain$BinaryClassification)
    #ES_CV_Error.TestError.Mat  <- cbind(ES_CV_Error.TestError.Mat,ES_CV_Error.TestError.Vec)
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2--(SL_L2_Error)------------------
    Normalized_TrainingData_List <- NormalizeMatrix_List(trainData[,1:NCOL(trainData)-1])
    Penalty.Scalar = 1
    opt.thresh = .2

    SL_L2.W.Matrix <-LMSquareLossL2(Normalized_TrainingData_List$NormalizedMatrix, trainData[,NCOL(trainData)], Penalty.Scalar, opt.thresh,ZipTrain$Initial.Vector)
    DeNormalized.W.Matrix <-(t(SL_L2.W.Matrix))/Normalized_TrainingData_List$sd

    #DeNorm.Error <-Find_Wmatrix_MeanL2Error(ZipTrain$TrainingData, ZipTrain$TrainingLabels,t(DeNormalized.W.Matrix),ZipTrain$BinaryClassification)
    SL_L2.WholeError.Vec <-Find_Wmatrix_MeanL1Error(ZipTrain$TrainingData, ZipTrain$TrainingLabels,t(DeNormalized.W.Matrix),ZipTrain$BinaryClassification)
    SL_L2.WholeError.Mat <- rbind(SL_L2.WholeError.Mat,SL_L2.WholeError.Vec)
    
    SL_L2.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],t(DeNormalized.W.Matrix),ZipTrain$BinaryClassification)
    SL_L2.TestError.Mat <- rbind(SL_L2.TestError.Mat,SL_L2.TestError.Vec)
    
    SL_L2.TrainError.Vec <-Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],t(DeNormalized.W.Matrix),ZipTrain$BinaryClassification)
    SL_L2.TrainError.Mat <- rbind(SL_L2.TrainError.Mat,SL_L2.TrainError.Vec)
    #------------------------------------------------------------------------
    
    #-----------------------LMSquareLossL2penalties--(SL_L2_Pen)-------------
    Penalty.Scalar = 1
    opt.thresh = .2
    SL_L2_Pen.W.Mat <-LMSquareLossL2penalties(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)], ZipTrain$Penalty.Vector)

    #DeNorm.Error <-Find_Wmatrix_MeanL2Error(ZipTrain$TrainingData, ZipTrain$TrainingLabels,t(DeNormalized.W.Matrix),ZipTrain$BinaryClassification)
    #print("SL_L2_Pen.WholeError.Vec")
    SL_L2_Pen.WholeError.Vec <-Find_Wmatrix_MeanL1Error(ZipTrain$TrainingData, ZipTrain$TrainingLabels,(SL_L2_Pen.W.Mat),ZipTrain$BinaryClassification)
    SL_L2_Pen.WholeError.Mat <- rbind(SL_L2_Pen.WholeError.Mat,SL_L2_Pen.WholeError.Vec)
    
    #print("SL_L2_Pen.TestError.Vec")
    SL_L2_Pen.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],(SL_L2_Pen.W.Mat),ZipTrain$BinaryClassification)
    SL_L2_Pen.TestError.Mat <- rbind(SL_L2_Pen.TestError.Mat,SL_L2_Pen.TestError.Vec)
    
    #print("SL_L2_Pen.TrainError.Vec")
    SL_L2_Pen.TrainError.Vec <- Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],(SL_L2_Pen.W.Mat),ZipTrain$BinaryClassification)
    SL_L2_Pen.TrainError.Mat <- rbind(SL_L2_Pen.TrainError.Mat,SL_L2_Pen.TrainError.Vec)
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
      SL_L2CV.fold.n  <- 4
      SL_L2CV.fold.vec<-Random_Folds(NROW(trainData),SL_L2CV.fold.n)
      
      SL_L2CV.List <- LMSquareLossL2CV(as.matrix(trainData[,1:NCOL(trainData)-1]), as.matrix(trainData[,NCOL(trainData)]),as.matrix(t(SL_L2CV.fold.vec)), as.matrix(ZipTrain$Penalty.Vector))
      

      SL_L2CV.WholeError.Vec <- Find_Wmatrix_MeanL1Error(ZipTrain$TrainingData, ZipTrain$TrainingLabels,(SL_L2CV.List$weight.vec),ZipTrain$BinaryClassification)
      SL_L2CV.WholeError.Mat <- rbind(SL_L2CV.WholeError.Mat,SL_L2CV.WholeError.Vec)
      
      
      #SL_L2CV.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],t(SL_L2CV.W.Mat),ZipTrain$BinaryClassification)
      SL_L2CV.TestError.Vec <- SL_L2CV.List$mean.validation.loss
      SL_L2CV.TestError.Mat <- rbind(SL_L2CV.TestError.Mat,SL_L2CV.TestError.Vec)
      #print("SL_L2CV.TestError.Vec")
      #print(dim(as.matrix(SL_L2CV.TestError.Vec)))
      
      #SL_L2CV.TrainError.Vec <- Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],t(SL_L2CV.W.Mat),ZipTrain$BinaryClassification)
      SL_L2CV.TrainError.Vec <- SL_L2CV.List$mean.train.loss.vec
      SL_L2CV.TrainError.Mat <- rbind(SL_L2CV.TrainError.Mat,SL_L2CV.TrainError.Vec)
      #print("SL_L2CV.TrainError.Vec")
      #print(dim(as.matrix(SL_L2CV.TrainError.Vec)))
    #------------------------------------------------------------------------
  }


  #------------------------------------------------------------------------
```
### Matrix of loss values
```{r}
#-------------------Matrix of Cross Validation loss values-------------------------------------

#print(SLI.TestError.Mat)

  #ZipTrain_LossMatrix <-rbind(,ES_CV_Error.TestError.Mat[2,2:5])
  ZipTrain_LossMatrix <-rbind(as.double(SLI.TestError.Mat[30,2:5]),as.double(as.double(t(SL_L2.TestError.Mat[2:5]))))
  ZipTrain_LossMatrix <-rbind(ZipTrain_LossMatrix,as.double(colMeans(SL_L2_Pen.TestError.Mat)[2:5]))
  ZipTrain_LossMatrix <-rbind(ZipTrain_LossMatrix,as.double(SL_L2CV.TrainError.Mat[2:5]))

  

  colnames(ZipTrain_LossMatrix)<-c("Fold1","Fold2","Fold3","Fold4")
  rownames(ZipTrain_LossMatrix)<-c("Linear Squared 30 Iterations","L2 Reg","L2 Reg with Penalty vector","L2 Reg with CV")
  barplot(ZipTrain_LossMatrix, xlab = "Iterations", ylab = "Error",main = "_LossMatrix",legend = (rownames(Spam_LossMatrix)),beside = TRUE)
```

### Train/validation loss plot
```{r}
  #-------------------- Graph Data Set: 2 SAheart -----------------------------
    #-------------------------LMSquareLossIterations (SLI_Error)--------------
    plot(rowMeans(as.matrix(SLI.TrainError.Mat)),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "SAheart Loss of each matrix",ylim=c(0,200))
    lines(rowMeans(as.matrix(SLI.TestError.Mat)),type="o", col = "light blue")
    #------------------------------------------------------------------------
    
    #--------------------LMSquareLossEarlyStoppingCV (ES_CV_Error)-----------
    lines(rowMeans(as.matrix(ES_CV_Error.TestError.Vec)),type="o", col = "green")
    lines(rowMeans(as.matrix(ES_CV_Error.TrainError.Vec)),type="o", col = "Light green")
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2--(SL_L2_Error)------------------
    lines(as.matrix((SL_L2.TestError.Mat[2:NROW(SL_L2.TestError.Mat)])),type="o", col = "black")
    lines(as.matrix(SL_L2.TrainError.Mat[2:NROW(SL_L2.TrainError.Mat)]),type="o", col = "gray")
    #------------------------------------------------------------------------
    
    #-----------------------LMSquareLossL2penalties--(SL_L2_Pen)-------------
    
    lines(as.matrix((SL_L2_Pen.TrainError.Mat[2:NROW(SL_L2_Pen.TrainError.Mat)])),type="o", col = "pink")
    lines(as.matrix((SL_L2_Pen.TestError.Mat[2:NROW(SL_L2_Pen.TestError.Mat)])),type="o", col = "red")
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
    lines(as.matrix((SL_L2CV.TestError.Mat[2:NROW(SL_L2CV.TestError.Mat)])),type="o", col = "orange")
    lines(as.matrix((SL_L2CV.TrainError.Mat[2:NROW(SL_L2CV.TrainError.Mat)])),type="o", col = "gold")
    #------------------------------------------------------------------------
  #------------------------------------------------------------------------
```
### Extra Credit attempt: compaire The LinearModels to the KNN
```{r}
  #--------------------KNNLearnCV------------------------------------
  Ziptrain<-Prep_ZipTrain()
  Fold.vec = Random_Folds(Ziptrain$n_Elements,4)
  Fold.n   = 4
  KNNLearnCV.List = KNNLearnCV(Ziptrain$TrainingData, Ziptrain$TrainingLabels, 30, Fold.vec, Fold.n)

  barplot(KNNLearnCV.List$TestMeanError.Means,main = "Ziptrain: KNNLearnCV L2 Mean Error",xlab = "KNN Compared",ylab = "Error",beside = TRUE)
  #------------------------------------------------------------------------

```



## Data set 4: Prostate
```{r}
   Prostate<-Prep_Prostate()
```
### Code 
```{r}
  #-------------------- Data set 4: Prostate ----------------------------------
  LossMat.fold.n  <- 4
  LossMat.fold.vec<-Random_Folds((Prostate$n_Elements),LossMat.fold.n)
  Data = cbind(Prostate$TrainingData ,Prostate$TrainingLabels)
  #Randomly shuffle the data
  #Data<-Data[sample(nrow(Data)),]

  SLI.WholeError.Mat = 0
  SLI.TrainError.Mat = 0
  SLI.TestError.Mat  = 0
  
  ES_CV_Error.WholeError.Mat = 0
  ES_CV_Error.TrainError.Mat = 0
  ES_CV_Error.TestError.Mat  = 0
  
  SL_L2.WholeError.Mat = 0
  SL_L2.TrainError.Mat = 0
  SL_L2.TestError.Mat  = 0
  
  SL_L2_Pen.WholeError.Mat = 0
  SL_L2_Pen.TrainError.Mat = 0
  SL_L2_Pen.TestError.Mat  = 0
  
  SL_L2CV.WholeError.Mat = 0
  SL_L2CV.TrainError.Mat = 0
  SL_L2CV.TestError.Mat  = 0
  #Perform folds.n fold cross validation
  for(i in 1:(LossMat.fold.n))
  {
    testIndexes <- which(LossMat.fold.vec==i,arr.ind=TRUE)
    testData    <- as.matrix(Data[testIndexes, ])
    trainData   <- as.matrix(Data[-testIndexes, ])
  
    print("Please wait this might take some time, Iteration:")
    print(i)
    
    Scalar.Step = 0.1
    Iterations  = 30

    #-------------------------LMSquareLossIterations (SLI_Error)-------------

    LMSquareLossIterations.W.Vec <- LMSquareLossIterations(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],Iterations,Scalar.Step)
    SLI.WholeError.Vec <-Find_Wmatrix_MeanL2Error(Prostate$TrainingData, Prostate$TrainingLabels,as.matrix(LMSquareLossIterations.W.Vec),Prostate$BinaryClassification)
    print("dim SLI.WHoleError")
    print(dim(as.matrix(SLI.WholeError.Vec)))
    SLI.WholeError.Mat <- cbind(SLI.WholeError.Mat,SLI.WholeError.Vec)

    SLI.TrainError.Vec <-Find_Wmatrix_MeanL2Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],as.matrix(LMSquareLossIterations.W.Vec),Prostate$BinaryClassification)
    SLI.TrainError.Mat <- cbind(SLI.TrainError.Mat,SLI.TrainError.Vec)

    SLI.TestError.Vec  <-Find_Wmatrix_MeanL2Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],as.matrix(LMSquareLossIterations.W.Vec),Prostate$BinaryClassification)
    SLI.TestError.Mat<- cbind(SLI.TestError.Mat,SLI.TestError.Vec)
    #------------------------------------------------------------------------

    #--------------------LMSquareLossEarlyStoppingCV (ES_CV_Error)-----------
    ES_CV_Error.fold.n  <- 4
    ES_CV_Error.fold.vec<-Random_Folds(NROW(trainData),ES_CV_Error.fold.n)
    Scalar.Step = 0.1
    Iterations  = 30
    #ES_CV_Error.W.Vec <- LMSquareLossEarlyStoppingCV(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],Prostate$Folds.Vec,Prostate$Folds.n,Prostate$Iterations)
    ES_CV_Error.List <- LMSquareLossEarlyStoppingCV(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],ES_CV_Error.fold.vec,ES_CV_Error.fold.n,Iterations)
    
    ES_CV_Error.WholeError.Vec   <-Find_Wmatrix_MeanL2Error(Prostate$TrainingData, Prostate$TrainingLabels,as.matrix(ES_CV_Error.List$w.mat),Prostate$BinaryClassification)
    #print("dim ES_CV_Error.WholeError.Vec")
    #print(dim(as.matrix(ES_CV_Error.WholeError.Vec)))
    #ES_CV_Error.WholeError.Mat <- cbind(ES_CV_Error.WholeError.Mat,ES_CV_Error.WholeError.Vec)

   ES_CV_Error.TrainError.Vec <-Find_Wmatrix_MeanL2Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],as.matrix(ES_CV_Error.List$w.mat),Prostate$BinaryClassification)
   #ES_CV_Error.TrainError.Mat <- cbind(ES_CV_Error.TrainError.Mat,ES_CV_Error.TrainError.Vec)

   ES_CV_Error.TestError.Vec  <-Find_Wmatrix_MeanL2Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],as.matrix(ES_CV_Error.List$w.mat),Prostate$BinaryClassification)
   #ES_CV_Error.TestError.Mat  <- cbind(ES_CV_Error.TestError.Mat,ES_CV_Error.TestError.Vec)
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2--(SL_L2_Error)------------------
    Normalized_TrainingData_List <- NormalizeMatrix_List(trainData[,1:NCOL(trainData)-1])
    Penalty.Scalar = 1
    opt.thresh = .2

    SL_L2.W.Matrix <-LMSquareLossL2(Normalized_TrainingData_List$NormalizedMatrix, trainData[,NCOL(trainData)], Penalty.Scalar, opt.thresh,Prostate$Initial.Vector)
    DeNormalized.W.Matrix <-(t(SL_L2.W.Matrix))/Normalized_TrainingData_List$sd

    #DeNorm.Error <-Find_Wmatrix_MeanL2Error(Prostate$TrainingData, Prostate$TrainingLabels,t(DeNormalized.W.Matrix),Prostate$BinaryClassification)
    SL_L2.WholeError.Vec <-Find_Wmatrix_MeanL1Error(Prostate$TrainingData, Prostate$TrainingLabels,t(DeNormalized.W.Matrix),Prostate$BinaryClassification)
    SL_L2.WholeError.Mat <- rbind(SL_L2.WholeError.Mat,SL_L2.WholeError.Vec)
    
    SL_L2.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],t(DeNormalized.W.Matrix),Prostate$BinaryClassification)
    SL_L2.TestError.Mat <- rbind(SL_L2.TestError.Mat,SL_L2.TestError.Vec)
    
    SL_L2.TrainError.Vec <-Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],t(DeNormalized.W.Matrix),Prostate$BinaryClassification)
    SL_L2.TrainError.Mat <- rbind(SL_L2.TrainError.Mat,SL_L2.TrainError.Vec)
    #------------------------------------------------------------------------
    
    #-----------------------LMSquareLossL2penalties--(SL_L2_Pen)-------------
    Penalty.Scalar = 1
    opt.thresh = .2
    SL_L2_Pen.W.Mat <-LMSquareLossL2penalties(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)], Prostate$Penalty.Vector)

    #DeNorm.Error <-Find_Wmatrix_MeanL2Error(Prostate$TrainingData, Prostate$TrainingLabels,t(DeNormalized.W.Matrix),Prostate$BinaryClassification)
    #print("SL_L2_Pen.WholeError.Vec")
    SL_L2_Pen.WholeError.Vec <-Find_Wmatrix_MeanL1Error(Prostate$TrainingData, Prostate$TrainingLabels,(SL_L2_Pen.W.Mat),Prostate$BinaryClassification)
    SL_L2_Pen.WholeError.Mat <- rbind(SL_L2_Pen.WholeError.Mat,SL_L2_Pen.WholeError.Vec)
    
    #print("SL_L2_Pen.TestError.Vec")
    SL_L2_Pen.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],(SL_L2_Pen.W.Mat),Prostate$BinaryClassification)
    SL_L2_Pen.TestError.Mat <- rbind(SL_L2_Pen.TestError.Mat,SL_L2_Pen.TestError.Vec)
    
    #print("SL_L2_Pen.TrainError.Vec")
    SL_L2_Pen.TrainError.Vec <- Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],(SL_L2_Pen.W.Mat),Prostate$BinaryClassification)
    SL_L2_Pen.TrainError.Mat <- rbind(SL_L2_Pen.TrainError.Mat,SL_L2_Pen.TrainError.Vec)
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
      SL_L2CV.fold.n  <- 4
      SL_L2CV.fold.vec<-Random_Folds(NROW(trainData),SL_L2CV.fold.n)
      
      SL_L2CV.List <- LMSquareLossL2CV(as.matrix(trainData[,1:NCOL(trainData)-1]), as.matrix(trainData[,NCOL(trainData)]),as.matrix(t(SL_L2CV.fold.vec)), as.matrix(Prostate$Penalty.Vector))
      

      SL_L2CV.WholeError.Vec <- Find_Wmatrix_MeanL1Error(Prostate$TrainingData, Prostate$TrainingLabels,(SL_L2CV.List$weight.vec),Prostate$BinaryClassification)
      SL_L2CV.WholeError.Mat <- rbind(SL_L2CV.WholeError.Mat,SL_L2CV.WholeError.Vec)
      
      
      #SL_L2CV.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],t(SL_L2CV.W.Mat),Prostate$BinaryClassification)
      SL_L2CV.TestError.Vec <- SL_L2CV.List$mean.validation.loss
      SL_L2CV.TestError.Mat <- rbind(SL_L2CV.TestError.Mat,SL_L2CV.TestError.Vec)
      #print("SL_L2CV.TestError.Vec")
      #print(dim(as.matrix(SL_L2CV.TestError.Vec)))
      
      #SL_L2CV.TrainError.Vec <- Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],t(SL_L2CV.W.Mat),Prostate$BinaryClassification)
      SL_L2CV.TrainError.Vec <- SL_L2CV.List$mean.train.loss.vec
      SL_L2CV.TrainError.Mat <- rbind(SL_L2CV.TrainError.Mat,SL_L2CV.TrainError.Vec)
      #print("SL_L2CV.TrainError.Vec")
      #print(dim(as.matrix(SL_L2CV.TrainError.Vec)))
      
    #------------------------------------------------------------------------
  }

  #------------------------------------------------------------------------
```
### Matrix of loss values
```{r}
#-------------------Matrix of Cross Validation loss values-------------------------------------


```
### Train/validation loss plot
```{r}
  #-------------------- Data set 4: Prostate -----------------------------
    #-------------------------LMSquareLossIterations (SLI_Error)--------------
    plot(rowMeans(as.matrix(SLI.TrainError.Mat)),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "SAheart Loss of each matrix")
    lines(rowMeans(as.matrix(SLI.TestError.Mat)),type="o", col = "light blue")
    #------------------------------------------------------------------------
    
    #--------------------LMSquareLossEarlyStoppingCV (ES_CV_Error)-----------
    lines(rowMeans(as.matrix(ES_CV_Error.TestError.Vec)),type="o", col = "green")
    lines(rowMeans(as.matrix(ES_CV_Error.TrainError.Vec)),type="o", col = "Light green")
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2--(SL_L2_Error)------------------
    lines(as.matrix((SL_L2.TestError.Mat[2:NROW(SL_L2.TestError.Mat)])),type="o", col = "black")
    lines(as.matrix(SL_L2.TrainError.Mat[2:NROW(SL_L2.TrainError.Mat)]),type="o", col = "gray")
    #------------------------------------------------------------------------
    
    #-----------------------LMSquareLossL2penalties--(SL_L2_Pen)-------------
    lines(as.matrix((SL_L2_Pen.TestError.Mat[2:NROW(SL_L2_Pen.TestError.Mat)])),type="o", col = "red")
    lines(as.matrix((SL_L2_Pen.TrainError.Mat[2:NROW(SL_L2_Pen.TrainError.Mat)])),type="o", col = "pink")
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
    lines(as.matrix((SL_L2CV.TestError.Mat[2:NROW(SL_L2CV.TestError.Mat)])),type="o", col = "orange")
    lines(as.matrix((SL_L2CV.TrainError.Mat[2:NROW(SL_L2CV.TrainError.Mat)])),type="o", col = "gold")
    #------------------------------------------------------------------------
  #------------------------------------------------------------------------
```
### Extra Credit attempt: compaire The LinearModels to the KNN
```{r}
  #--------------------NN1ToKmaxPredict------------------------------------
  Prostate<-Prep_Prostate()
  Fold.vec = Random_Folds(Prostate$n_Elements,4)
  Fold.n   = 4
  KNNLearnCV.List = KNNLearnCV(Prostate$TrainingData, Prostate$TrainingLabels, 30, Fold.vec, Fold.n)

  barplot(KNNLearnCV.List$TestMeanError.Means,main = "Prostate: KNNLearnCV L2 Mean Error",xlab = "KNN Compared",ylab = "Error",beside = TRUE)
  #------------------------------------------------------------------------
```



## Data set 5: Ozone
```{r}
  OZone<-Prep_Ozone()
```
### Code 
```{r}
  #-------------------- Data set 5: Ozone ----------------------------------
  LossMat.fold.n  <- 4
  LossMat.fold.vec<-Random_Folds((OZone$n_Elements),LossMat.fold.n)
  Data = cbind(OZone$TrainingData ,OZone$TrainingLabels)
  #Randomly shuffle the data
  #Data<-Data[sample(nrow(Data)),]

  SLI.WholeError.Mat = 0
  SLI.TrainError.Mat = 0
  SLI.TestError.Mat  = 0
  
  ES_CV_Error.WholeError.Mat = 0
  ES_CV_Error.TrainError.Mat = 0
  ES_CV_Error.TestError.Mat  = 0
  
  SL_L2.WholeError.Mat = 0
  SL_L2.TrainError.Mat = 0
  SL_L2.TestError.Mat  = 0
  
  SL_L2_Pen.WholeError.Mat = 0
  SL_L2_Pen.TrainError.Mat = 0
  SL_L2_Pen.TestError.Mat  = 0
  
  SL_L2CV.WholeError.Mat = 0
  SL_L2CV.TrainError.Mat = 0
  SL_L2CV.TestError.Mat  = 0
  #Perform folds.n fold cross validation
  for(i in 1:(LossMat.fold.n))
  {
    testIndexes <- which(LossMat.fold.vec==i,arr.ind=TRUE)
    testData    <- as.matrix(Data[testIndexes, ])
    trainData   <- as.matrix(Data[-testIndexes, ])
  
    print("Please wait this might take some time, Iteration:")
    print(i)
    

    #-------------------------LMSquareLossIterations (SLI_Error)-------------
    Scalar.Step = 0.1
    Iterations  = 30
    LMSquareLossIterations.W.Vec <- LMSquareLossIterations(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],Iterations,Scalar.Step)
    SLI.WholeError.Vec <-Find_Wmatrix_MeanL2Error(OZone$TrainingData, OZone$TrainingLabels,as.matrix(LMSquareLossIterations.W.Vec),OZone$BinaryClassification)
    print("dim SLI.WHoleError")
    print(dim(as.matrix(SLI.WholeError.Vec)))
    SLI.WholeError.Mat <- cbind(SLI.WholeError.Mat,SLI.WholeError.Vec)

    SLI.TrainError.Vec <-Find_Wmatrix_MeanL2Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],as.matrix(LMSquareLossIterations.W.Vec),OZone$BinaryClassification)
    SLI.TrainError.Mat <- cbind(SLI.TrainError.Mat,SLI.TrainError.Vec)

    SLI.TestError.Vec  <-Find_Wmatrix_MeanL2Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],as.matrix(LMSquareLossIterations.W.Vec),OZone$BinaryClassification)
   SLI.TestError.Mat<- cbind(SLI.TestError.Mat,SLI.TestError.Vec)
    #------------------------------------------------------------------------

    #--------------------LMSquareLossEarlyStoppingCV (ES_CV_Error)-----------
    ES_CV_Error.fold.n  <- 4
    ES_CV_Error.fold.vec<-Random_Folds(NROW(trainData),ES_CV_Error.fold.n)
    Scalar.Step = 0.1
    Iterations  = 30
    #ES_CV_Error.W.Vec <- LMSquareLossEarlyStoppingCV(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],OZone$Folds.Vec,OZone$Folds.n,OZone$Iterations)
    ES_CV_Error.List <- LMSquareLossEarlyStoppingCV(trainData[,1:NCOL(trainData)-1], trainData[,NCOL(trainData)],ES_CV_Error.fold.vec,ES_CV_Error.fold.n,Iterations)
    
    ES_CV_Error.WholeError.Vec   <-Find_Wmatrix_MeanL2Error(OZone$TrainingData, OZone$TrainingLabels,as.matrix(ES_CV_Error.List$w.mat),OZone$BinaryClassification)
    #print("dim ES_CV_Error.WholeError.Vec")
    #print(dim(as.matrix(ES_CV_Error.WholeError.Vec)))
    #ES_CV_Error.WholeError.Mat <- cbind(ES_CV_Error.WholeError.Mat,ES_CV_Error.WholeError.Vec)

    ES_CV_Error.TrainError.Vec <-Find_Wmatrix_MeanL2Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],as.matrix(ES_CV_Error.List$w.mat),OZone$BinaryClassification)
    #ES_CV_Error.TrainError.Mat <- cbind(ES_CV_Error.TrainError.Mat,ES_CV_Error.TrainError.Vec)

    ES_CV_Error.TestError.Vec  <-Find_Wmatrix_MeanL2Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],as.matrix(ES_CV_Error.List$w.mat),OZone$BinaryClassification)
    #ES_CV_Error.TestError.Mat  <- cbind(ES_CV_Error.TestError.Mat,ES_CV_Error.TestError.Vec)
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2--(SL_L2_Error)------------------
    Normalized_TrainingData_List <- NormalizeMatrix_List(trainData[,1:NCOL(trainData)-1])
    Penalty.Scalar = 1
    opt.thresh = .2

    SL_L2.W.Matrix <-LMSquareLossL2(Normalized_TrainingData_List$NormalizedMatrix, trainData[,NCOL(trainData)], Penalty.Scalar, opt.thresh,OZone$Initial.Vector)
    DeNormalized.W.Matrix <-(t(SL_L2.W.Matrix))/Normalized_TrainingData_List$sd

    #DeNorm.Error <-Find_Wmatrix_MeanL2Error(OZone$TrainingData, OZone$TrainingLabels,t(DeNormalized.W.Matrix),OZone$BinaryClassification)
    SL_L2.WholeError.Vec <-Find_Wmatrix_MeanL1Error(OZone$TrainingData, OZone$TrainingLabels,t(DeNormalized.W.Matrix),OZone$BinaryClassification)
    SL_L2.WholeError.Mat <- rbind(SL_L2.WholeError.Mat,SL_L2.WholeError.Vec)
    
    SL_L2.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],t(DeNormalized.W.Matrix),OZone$BinaryClassification)
    SL_L2.TestError.Mat <- rbind(SL_L2.TestError.Mat,SL_L2.TestError.Vec)
    
    SL_L2.TrainError.Vec <-Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],t(DeNormalized.W.Matrix),OZone$BinaryClassification)
    SL_L2.TrainError.Mat <- rbind(SL_L2.TrainError.Mat,SL_L2.TrainError.Vec)
    #------------------------------------------------------------------------
    
    #-----------------------LMSquareLossL2penalties--(SL_L2_Pen)-------------
    Penalty.Scalar = 1
    opt.thresh = .2
    SL_L2_Pen.W.Mat <-LMSquareLossL2penalties(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)], OZone$Penalty.Vector)

    #DeNorm.Error <-Find_Wmatrix_MeanL2Error(OZone$TrainingData, OZone$TrainingLabels,t(DeNormalized.W.Matrix),OZone$BinaryClassification)
    print("SL_L2_Pen.WholeError.Vec")
    SL_L2_Pen.WholeError.Vec <-Find_Wmatrix_MeanL1Error(OZone$TrainingData, OZone$TrainingLabels,(SL_L2_Pen.W.Mat),OZone$BinaryClassification)
    SL_L2_Pen.WholeError.Mat <- rbind(SL_L2_Pen.WholeError.Mat,SL_L2_Pen.WholeError.Vec)
    
    print("SL_L2_Pen.TestError.Vec")
    SL_L2_Pen.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],(SL_L2_Pen.W.Mat),OZone$BinaryClassification)
    SL_L2_Pen.TestError.Mat <- rbind(SL_L2_Pen.TestError.Mat,SL_L2_Pen.TestError.Vec)
    
    print("SL_L2_Pen.TrainError.Vec")
    SL_L2_Pen.TrainError.Vec <- Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],(SL_L2_Pen.W.Mat),OZone$BinaryClassification)
    SL_L2_Pen.TrainError.Mat <- rbind(SL_L2_Pen.TrainError.Mat,SL_L2_Pen.TrainError.Vec)
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
      SL_L2CV.fold.n  <- 4
      SL_L2CV.fold.vec<-Random_Folds(NROW(trainData),SL_L2CV.fold.n)
      
      SL_L2CV.List <- LMSquareLossL2CV(as.matrix(trainData[,1:NCOL(trainData)-1]), as.matrix(trainData[,NCOL(trainData)]),as.matrix(t(SL_L2CV.fold.vec)), as.matrix(OZone$Penalty.Vector))
      

      SL_L2CV.WholeError.Vec <- Find_Wmatrix_MeanL1Error(OZone$TrainingData, OZone$TrainingLabels,(SL_L2CV.List$weight.vec),OZone$BinaryClassification)
      SL_L2CV.WholeError.Mat <- rbind(SL_L2CV.WholeError.Mat,SL_L2CV.WholeError.Vec)
      
      
      #SL_L2CV.TestError.Vec <-Find_Wmatrix_MeanL1Error(trainData[,1:NCOL(trainData)-1],trainData[,NCOL(trainData)],t(SL_L2CV.W.Mat),OZone$BinaryClassification)
      SL_L2CV.TestError.Vec <- SL_L2CV.List$mean.validation.loss
      SL_L2CV.TestError.Mat <- rbind(SL_L2CV.TestError.Mat,SL_L2CV.TestError.Vec)
      print("SL_L2CV.TestError.Vec")
      print(dim(as.matrix(SL_L2CV.TestError.Vec)))
      
      #SL_L2CV.TrainError.Vec <- Find_Wmatrix_MeanL1Error(testData[,1:NCOL(testData)-1],testData[,NCOL(testData)],t(SL_L2CV.W.Mat),OZone$BinaryClassification)
      SL_L2CV.TrainError.Vec <- SL_L2CV.List$mean.train.loss.vec
      SL_L2CV.TrainError.Mat <- rbind(SL_L2CV.TrainError.Mat,SL_L2CV.TrainError.Vec)
      print("SL_L2CV.TrainError.Vec")
      print(dim(as.matrix(SL_L2CV.TrainError.Vec)))
    #------------------------------------------------------------------------
  }

  #------------------------------------------------------------------------
```

### Matrix of loss values
### Train/validation loss plot
```{r}
  #-------------------- Data set 5: Ozone -----------------------------
    #-------------------------LMSquareLossIterations (SLI_Error)--------------
    plot(rowMeans(as.matrix(SLI.TrainError.Mat)),type="o", col = "blue", xlab = "Iterations", ylab = "Error",main = "SAheart Loss of each matrix",ylim=c(1900 ,5000))
    lines(rowMeans(as.matrix(SLI.TestError.Mat)),type="o", col = "light blue")
    #------------------------------------------------------------------------
    
    #--------------------LMSquareLossEarlyStoppingCV (ES_CV_Error)-----------
    lines(rowMeans(as.matrix(ES_CV_Error.TestError.Vec)),type="o", col = "green")
    lines(rowMeans(as.matrix(ES_CV_Error.TrainError.Vec)),type="o", col = "Light green")
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2--(SL_L2_Error)------------------
    lines(as.matrix((SL_L2.TestError.Mat[2:NROW(SL_L2.TestError.Mat)])),type="o", col = "black")
    lines(as.matrix(SL_L2.TrainError.Mat[2:NROW(SL_L2.TrainError.Mat)]),type="o", col = "gray")
    #------------------------------------------------------------------------
    
    #-----------------------LMSquareLossL2penalties--(SL_L2_Pen)-------------
    lines(as.matrix((SL_L2_Pen.TestError.Mat[2:NROW(SL_L2_Pen.TestError.Mat)])),type="o", col = "red")
    lines(as.matrix((SL_L2_Pen.TrainError.Mat[2:NROW(SL_L2_Pen.TrainError.Mat)])),type="o", col = "pink")
    #------------------------------------------------------------------------
    
    #-------------------------LMSquareLossL2CV_List-(SL_L2CV)----------------
    lines(as.matrix((SL_L2CV.TestError.Mat[2:NROW(SL_L2CV.TestError.Mat)])),type="o", col = "orange")
    lines(as.matrix((SL_L2CV.TrainError.Mat[2:NROW(SL_L2CV.TrainError.Mat)])),type="o", col = "gold")
    #------------------------------------------------------------------------
  #------------------------------------------------------------------------
```

### Extra Credit attempt: compaire The LinearModels to the KNN
```{r}
  #--------------------NN1ToKmaxPredict------------------------------------
  Ozone<-Prep_Ozone()
  print("NN1ToKmax_Ozone_Tests: ")
  Fold.vec = Random_Folds(Ozone$n_Elements,4)
  Fold.n   = 4
  KNNLearnCV.List = KNNLearnCV(Ozone$TrainingData, Ozone$TrainingLabels, 30, Fold.vec, Fold.n)

  barplot(KNNLearnCV.List$TestMeanError.Means,main = "Ozone: KNNLearnCV L2 Mean Error",xlab = "KNN Compared",ylab = "Error",beside = TRUE)
  #------------------------------------------------------------------------
```

